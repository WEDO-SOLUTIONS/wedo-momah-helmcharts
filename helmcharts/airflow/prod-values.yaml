# prod-values.yaml

# ===============================================================================
# AIRFLOW 3.1.5 CONFIGURATION
# ===============================================================================
# MAJOR ARCHITECTURE CHANGE: In Airflow 3.0+, the webserver has been split into:
#   1. API Server - Handles all API requests and task execution coordination
#   2. Webserver - Serves the UI only
# Both components run separately in Airflow 3.x deployments
# ===============================================================================

# -------------------------------
# Core Airflow & Helm configuration
# -------------------------------
# This should match the version in your custom image
airflowVersion: "3.1.5"
executor: KubernetesExecutor

# In Airflow 3.x, webserverSecretKey moved to [api] section
# This is now handled by config.api.secret_key below
# webserverSecretKey is DEPRECATED - don't use it

# -------------------------------
# Disable in-cluster Postgres & PgBouncer
# -------------------------------
postgresql:
  enabled: false

pgbouncer:
  enabled: false

# -------------------------------
# External metadata DB configuration
# -------------------------------
data:
  # FIX 1: This section MUST be a structured object like this
  metadataConnection:
    user:     airflow_admin_3
    pass:     VeryStrongPassword!123
    protocol: postgresql
    host:     10.246.3.65
    port:     5432
    db:       airflow-3
    sslmode:  "require"
  # resultBackendConnection:     # optional, not needed if you use XComObjectStorageBackend

# -------------------------------
# Core airflow.cfg overrides
# -------------------------------
airflow:
  config:
    # Base URL for API server (replaces AIRFLOW_WEBSERVER_BASE_URL in 3.x)
    AIRFLOW_API_BASE_URL: "https://cityview-airflow.momah.gov.sa"

config:
  core:
    max_active_tasks_per_dag: 200
    parallelism:              500
    default_pool_task_slot_count: 500
    # dag_concurrency was renamed to max_active_tasks_per_dag in Airflow 2.2+
    # execution_api_server_url is auto-generated by Helm for KubernetesExecutor
    # It will be: http://<release-name>-api-server:8080/execution/

  # Database connection pool settings to prevent memory bloat
  # With 200 DAGs and 30 concurrent runs, need higher pool limits
  database:
    sql_alchemy_pool_enabled: 'True'
    sql_alchemy_pool_size: 50
    sql_alchemy_pool_recycle: 1800
    sql_alchemy_max_overflow: 80
    sql_alchemy_pool_pre_ping: 'True'
    sql_alchemy_pool_timeout: 60

  # Increase API client timeout for task pods to handle load
  api:
    # Secret key for API authentication (moved from webserver)
    secret_key: "16f143ec4690e48710cbe27914d2a115dcb16c92e1441206815d09bc93bf5c60"
    # Base URL for the API server
    base_url: "https://cityview-airflow.momah.gov.sa"
    # Auth backend for API (you can keep FAB if needed)
    auth_backends: "airflow.providers.fab.auth_manager.api.auth.backend.fab_auth"
    # Increase client timeout for high-load scenarios
    client_request_timeout: 60

  # Webserver section (UI only in Airflow 3.x)
  webserver:
    # If using FAB for authentication (recommended for production)
    auth_manager: "airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager"
    # Or use SimpleAuthManager (default in Airflow 3.x but less secure)
    # auth_manager: "airflow.api_fastapi.auth.managers.simple.simple_auth_manager.SimpleAuthManager"

  # JWT Configuration for API authentication
  api_auth:
    jwt_secret: "16f143ec4690e48710cbe27914d2a115dcb16c92e1441206815d09bc93bf5c60"
    jwt_algorithm: "HS256"

# -------------------------------
# Logging / XCom (OCI-S3) validation
# -------------------------------
  logging:
    # Remote logging enabled - logs will be stored in OCI Object Storage (S3-compatible)
    remote_logging:         'True'
    logging_level:          INFO
    remote_base_log_folder: 's3://prod-airflow-logging-xcom/logs'
    remote_log_conn_id:     oci_s3_conn
    delete_worker_pods:     'False'
    encrypt_s3_logs:        'False'

  # AWS/S3 Configuration for OCI Object Storage compatibility
  aws:
    s3_use_sigv4: 'True'
    signature_version: 's3v4'

extraEnv: |
  - name: AIRFLOW_CONN_OCI_S3_CONN
    value: 'aws://8b27cd27701b5cacb854d684d00a4e407d0ee75a:%2Be8IrLxwqNdHR0s4SbHKTANaeos7KD22QK%2F1SMV7qzo%3D@?endpoint_url=https%3A%2F%2Faxownvq9lhmx.compat.objectstorage.me-jeddah-1.oraclecloud.com&region_name=me-jeddah-1'
  - name: AWS_ACCESS_KEY_ID
    value: '8b27cd27701b5cacb854d684d00a4e407d0ee75a'
  - name: AWS_SECRET_ACCESS_KEY
    value: '+e8IrLxwqNdHR0s4SbHKTANaeos7KD22QK/1SMV7qzo='
  - name: AWS_DEFAULT_REGION
    value: 'me-jeddah-1'
  - name: AWS_S3_ADDRESSING_STYLE
    value: 'path'
  - name: AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID
    value: 'oci_s3_conn'
  - name: AIRFLOW_CORE_XCOM_BACKEND
    value: 'airflow.providers.common.io.xcom.backend.XComObjectStorageBackend'
  - name: AIRFLOW_COMMON_IO_XCOM_OBJECTSTORAGE_PATH
    value: 's3://prod-airflow-logging-xcom/xcom'
  - name: AIRFLOW_COMMON_IO_XCOM_OBJECTSTORAGE_THRESHOLD
    value: '0'
  - name: AIRFLOW_COMMON_IO_XCOM_OBJECTSTORAGE_COMPRESSION
    value: 'zip'

# -------------------------------
# Custom airflow_local_settings.py (fixes S3RemoteLogIO import error in Airflow 3.x)
# -------------------------------
airflowLocalSettings: |-
  # Minimal logging config without AWS-specific imports
  # Remote S3 logging will work via airflow.cfg settings
  import os
  from pathlib import Path

  AIRFLOW_HOME = Path(os.environ.get("AIRFLOW_HOME", "~/airflow")).expanduser()
  BASE_LOG_FOLDER = os.environ.get("AIRFLOW__LOGGING__BASE_LOG_FOLDER", f"{AIRFLOW_HOME}/logs")

  LOGGING_CONFIG = {
      'version': 1,
      'disable_existing_loggers': False,
      'formatters': {
          'airflow': {
              'format': '[%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s'
          },
      },
      'handlers': {
          'console': {
              'class': 'logging.StreamHandler',
              'formatter': 'airflow',
              'stream': 'ext://sys.stdout'
          },
          'task': {
              'class': 'logging.StreamHandler',
              'formatter': 'airflow',
              'stream': 'ext://sys.stdout'
          },
      },
      'loggers': {
          'airflow': {
              'handlers': ['console'],
              'level': 'INFO',
              'propagate': False
          },
          'airflow.task': {
              'handlers': ['task'],
              'level': 'INFO',
              'propagate': False,
          },
      },
      'root': {
          'handlers': ['console'],
          'level': 'INFO',
      }
  }

# -------------------------------
# Images, Ingress, Webserver, Scheduler, Triggerer, DAGS, Cleanup
# -------------------------------
images:
  # IMPORTANT: Update your custom image to be based on Airflow 3.1.5
  # Python 3.9+ is required for Airflow 3.x (3.9, 3.10, 3.11, 3.12 supported)
  #
  # CRITICAL: Your image MUST include these providers for Airflow 3.x:
  #   - apache-airflow-providers-fab (for FAB authentication)
  #   - apache-airflow-providers-cncf-kubernetes (for KubernetesExecutor)
  #   - apache-airflow-providers-common-io (for XCom storage backend)
  #
  # Install with: pip install apache-airflow[fab,cncf.kubernetes,common.io]==3.1.5
  airflow:
    repository: registry.momrah.gov.sa/urbi-omar/momah-airflow
    tag:        "3.1.5-python3.12-02"  # UPDATE THIS to your actual 3.1.5-based image tag
    pullPolicy: Always
  useDefaultImageForMigration: false
  statsd:
    repository: quay.io/prometheus/statsd-exporter
    tag:        "v0.28.0"
    pullPolicy: IfNotPresent
  redis:
    repository: redis
    tag:        "7.2-bookworm"
    pullPolicy: IfNotPresent
  gitSync:
    repository: registry.k8s.io/git-sync/git-sync
    tag:        "v4.3.0"
    pullPolicy: IfNotPresent

ingress:
  enabled: true
  # AIRFLOW 3.x: Use apiServer instead of web
  apiServer:
    enabled:          true
    host:             cityview-airflow.momah.gov.sa
    ingressClassName: urbi
    pathType:         ImplementationSpecific
    path:             /
    tls:
      enabled:        false

workers:
  replicas: 3
  #autoscaling:
  #  enabled: true
  #  minReplicas: 3
  #  maxReplicas: 50
  #  targetCPUUtilizationPercentage: 70
  resources:
    requests:
      cpu:    "2000m"
      memory: "16Gi"
    limits:
      cpu:    "8000m"
      memory: "32Gi"
  serviceAccount:
    create:                       true
    automountServiceAccountToken: true


scheduler:
  enabled: true
  replicas: 8
  resources:
    requests:
      cpu: "1000m"
      memory: "4Gi"
    limits:
      cpu: "4000m"
      memory: "16Gi"
  waitForMigrations:
    enabled: true

# -------------------------------
# API Server Configuration (NEW in Airflow 3.0+)
# -------------------------------
# The API server handles all API requests and task execution coordination
# Note: API server is automatically enabled for Airflow 3.x - no 'enabled' flag needed
apiServer:
  replicas: 8
  resources:
    requests:
      cpu: "4000m"
      memory: "16Gi"
    limits:
      cpu: "8000m"
      memory: "32Gi"

# -------------------------------
# DAG Processor (Standalone in Airflow 3.x)
# -------------------------------
# In Airflow 3.x, DAG processing is separated from the scheduler
dagProcessor:
  enabled: true  # Required for Airflow 3.x
  replicas: 1
  resources:
    requests:
      cpu: "1000m"
      memory: "1Gi"
    limits:
      cpu: "2000m"
      memory: "2Gi"

# -------------------------------
# Webserver (UI Only in Airflow 3.x)
# -------------------------------
# In Airflow 3.x, webserver ONLY serves the UI
# All API calls go to the API server component
webserver:
  enabled: true
  replicas: 8
  resources:
    requests:
      cpu: "4000m"
      memory: "16Gi"
    limits:
      cpu: "8000m"
      memory: "32Gi"
  defaultUser:
    enabled:   true
    role:      Admin
    username:  af_admin
    email:     app.services@wedosolutions.sa
    firstName: Airflow
    lastName:  Admin
    password:  "OmarK@#$1998"

triggerer:
  enabled: true
  replicas: 2
  persistence:
    enabled: true
    size:    50Gi

dags:
  gitSync:
    enabled: true
    repo:    https://github.com/WEDO-SOLUTIONS/momah-cityview-airflow-dags.git
    branch:  main
    rev:     HEAD
    subPath: dags
    period:  5s
  persistence:
    enabled: false

cleanup:
  enabled:  true
  schedule: '*/5 * * * *'
  command:  null
  args:
    - bash
    - '-c'
    - 'exec airflow kubernetes cleanup-pods --namespace={{ .Release.Namespace }}'
