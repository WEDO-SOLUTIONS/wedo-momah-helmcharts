    1  ls
    2  curl https://map-urbi.momrah.gov.sa/api.js
    3  curl -k https://map-urbi.momrah.gov.sa/api.js
    4  clear
    5  ls
    6  curl -k https://map-urbi.momrah.gov.sa/api.js
    7  clear
    8  sshd -T
    9  sudo sshd -T
   10  sudo sshd -T | grep  password
   11  sudo -i
   12  history | grep raw
   13  ssh admin@10.247.0.158
   14  ssh -i ~/.ssh/keys/staging.key admin@10.247.0.158
   15  cd /opt/on-premise/momrah/
   16  helmfile -e oracle-staging -f helmfile/services/haproxy.yaml diff
   17  helmfile -e oracle-staging -f helmfile/services/haproxy.yaml apply
   18  helmfile -e oracle-staging -f helmfile/services/redis.yaml diff
   19  helmfile -e oracle-staging -f helmfile/services/pro-api.yaml diff
   20  helmfile -e oracle-staging -f helmfile/services/pro-api.yaml apply
   21  helmfile -e oracle-staging -f helmfile/services/pro-api.yaml delete
   22  helmfile -e oracle-staging -f helmfile/services/pro-api.yaml diff
   23  helmfile -e oracle-staging -f helmfile/services/pro-api.yaml apply
   24  curl http://map-urbi.momrah.gov.sa
   25  curl -v http://map-urbi.momrah.gov.sa
   26  curl -v http://map-urbi.momrah.gov.sa/api.js
   27  curl -v https://map-urbi.momrah.gov.sa/api.js
   28  curl -v http://map-urbi.momrah.gov.sa/api.js
   29  ip a
   30  curl -v http://map-urbi.momrah.gov.sa/api.js
   31  curl -v https://map-urbi.momrah.gov.sa/api.js
   32  curl -k -v https://map-urbi.momrah.gov.sa/api.js
   33  curl -k -v https://map-urbi.momrah.gov.sa/api.js | head -n 20
   34  curl -k -v https://map-urbi.momrah.gov.sa/api.js |& head -n 20
   35  curl -k -v https://map-urbi.momrah.gov.sa/api.js |& head -n 40
   36  curl -k -v https://map-urbi.momrah.gov.sa/api.js |& head -n 50
   37  helmfile -e oracle-staging --namespace urbi -f helmfile/services/ingress-controller.yaml diff
   38  helmfile -e oracle-staging --namespace urbi -f helmfile/services/ingress-controller.yaml apply
   39  kubectl get pods
   40  kubectl --context=staging get pods
   41  kubectl --context=staging describe pod urbi-ingress-nginx-controller-65b596b5d4-zq2j9
   42  kubectl --context=staging describe deployment urbi-ingress-nginx-controller
   43  kubectl --context=staging describe svc
   44  kubectl --context=staging get pods
   45  kubectl --context=staging delete deployment urbi-ingress-nginx-controller-65b596b5d4-zq2j9
   46  kubectl --context=staging delete deployment urbi-ingress-nginx-controller
   47  curl -k -v https://map-urbi.momrah.gov.sa/api.js |& head -n 50
   48  curl -k -v https://map-urbi.momrah.gov.sa/api.js 
   49  curl  -v http://map-urbi.momrah.gov.sa/api.js 
   50  curl -k -v https://map-urbi.momrah.gov.sa/api.js |& head -n 50
   51  helmfile -e oracle-staging -f helmfile/services/pro-api.yaml apply
   52  kubectl --context=staging get pods
   53  kubectl --context=staging get pods | grep starter
   54  kubectl --context=staging delete pod pro-api-asset-importer-starter-6fz78
   55  kubectl --context=staging delete pod pro-api-asset-importer-starter-kktz9
   56  helmfile -e oracle-staging -f helmfile/services/pro-ui.yaml apply
   57  helmfile -e oracle-staging -f helmfile/services/pro-api.yaml apply
   58  helmfile -e oracle-staging -f helmfile/services/citylens.yaml diff
   59  history | grep raw
   60  ansible -i ansible/inventory/oracle-staging -m setup stg-elasticsearch-3 | grep python
   61  ansible -i ansible/inventory/oracle-staging -m setup stg-elasticsearch-3 -e ansible_python_interpreter=/usr/bin/python3.6
   62  ansible -i ansible/inventory/oracle-staging -m shell -a 'ls' stg-elasticsearch-3 -e ansible_python_interpreter=/usr/bin/python3.6
   63  ssh -i ~/.ssh/keys/staging.key admin@10.247.0.158
   64  ssh -i ~/.ssh/keys/staging.key admin@10.247.1.255
   65  kubectl --context=staging get pods -o wide | grep haproxy
   66  kubectl --context=staging get svc
   67  kubectl --context=staging get pods -o wide | grep pro
   68  kubectl --context=staging logs pro-api-asset-importer-starter-gf5xf
   69  kubectl --context=staging get pods -o wide | grep pro
   70  kubectl --context=staging logs pro-api-asset-importer-starter-x7lf9
   71  kubectl --context=staging get pods -o wide | grep pro
   72  w
   73  kubectl --context=staging get pods -o wide | grep pro
   74  kubectl --context=staging logs pro-api-permissions-55f777dd4-8jnmp
   75  kubectl --context=staging get svc
   76  kubectl --context=staging logs 
   77  kubectl --context=staging get pods
   78  kubectl --context=staging logs urbi-ingress-ingress-nginx-controller-85f7d76c59-z4cpj
   79  kubectl --context=staging logs urbi-ingress-ingress-nginx-controller-85f7d76c59-z4cpj -f
   80  kubectl --context=staging get svc
   81  kubectl --context=stagingget svc -o yaml urbi-ingress-ingress-nginx-controller
   82  helm list --kube-context staging -A
   83  kubectl --context=staging get svc
   84  kubectl --context=staging describe svc urbi-ingress-ingress-nginx-controller
   85  kubectl --context=staging logs urbi-ingress-ingress-nginx-controller-85f7d76c59-z4cpj -f
   86  kubectl --context=staging get pods
   87  kubectl --context=staging logs pro-api-permissions-55f777dd4-8jnmp
   88  kubectl --context=staging get pods
   89  kubectl --context=staging logs pro-api-permissions-54677fc956-jtz8t
   90  kubectl --context=staging get pods
   91  kubectl --context=staging logs pro-api-asset-importer-starter-h865w
   92  kubectl --context=staging get pods
   93  kubectl --context=staging logs pro-api-permissions-54677fc956-jtz8t
   94  kubectl --context=staging get pods
   95  kubectl --context=staging logs pro-api-asset-importer-29011320-6vzfb
   96  kubectl --context=staging get pods
   97  kubectl --context=staging -n urbi exec -ti balady-frontend-85b6965f-rc4cm -- curl -v https://haproxy:9200
   98  kubectl --context=staging -n urbi exec -ti balady-frontend-85b6965f-rc4cm -- curl -k -v https://haproxy:9200
   99  kubectl --context=staging logs haproxy-c6d9f78c8-c79cx
  100  kubectl --context=staging -n urbi exec -ti balady-frontend-85b6965f-rc4cm -- curl -k -v https://haproxy:9200
  101  kubectl --context=staging -n urbi exec -ti balady-frontend-85b6965f-rc4cm -- curl -k -v https://haproxy:9300
  102  kubectl --context=staging -n urbi exec -ti balady-frontend-85b6965f-rc4cm -- curl -k -v https://haproxy:9200
  103  kubectl --context=staging -n urbi exec -ti balady-frontend-85b6965f-rc4cm -- curl -k -v http://haproxy:9200
  104  kubectl --context=staging -n urbi exec -ti balady-frontend-85b6965f-rc4cm -- curl -k -v http://10.247.3.1:9200
  105  kubectl --context=staging -n urbi exec -ti balady-frontend-85b6965f-rc4cm -- curl -k -v http://10.247.3.1:9300
  106  kubectl --context=staging -n urbi exec -ti balady-frontend-85b6965f-rc4cm -- curl -k -v https://10.247.3.1:9200
  107  kubectl --context=staging get pods
  108  kubectl --context=staging logs pro-api-asset-importer-worker-l2v8d-nwlwq
  109  kubectl --context=staging get pods
  110  kubectl --context=staging logs pro-api-asset-importer-worker-znx26-9zwjq
  111  kubectl --context=staging logs pro-api-asset-importer-worker-zs6xz-rltrl
  112  kubectl --context=staging logs pro-api-asset-importer-worker-2grhj-9px5d
  113  ssh -i ~/.ssh/keys/staging.key admin@10.247.1.255
  114  kubectl --context=staging get pods
  115  nano .bashrc
  116  tmux attach -t r9odt
  117  echo $BASTION_ID 
  118  tmux attach -t r9odt
  119  echo $BASTION_ID 
  120  tmux attach -t r9odt
  121  echo $BASTION_ID 
  122  tmux attach -t r9odt
  123  cat /opt/on-premise/momrah/helmfile/values/ingress-controller/common.gotmpl 
  124  nano /opt/on-premise/momrah/helmfile/values/ingress-controller/common.gotmpl 
  125  curl -k https://map-urbi.momrah.gov.sa
  126  clear
  127  curl -k https://map-urbi.momrah.gov.sa
  128  clear
  129  curl https://map-urbi.momrah.gov.sa
  130  curl -k https://map-urbi.momrah.gov.sa
  131  clear
  132  ls
  133  curl -k http://map-urbi.momrah.gov.sa
  134  clear
  135  curl -k http://map-urbi.momrah.gov.sa
  136  curl -k https://map-urbi.momrah.gov.sa
  137  nslookup map-urbi.momrah.gov.sa
  138  curl https://map-urbi.momrah.gov.sa
  139  curl -k https://map-urbi.momrah.gov.sa
  140  clear
  141  kubectl --context=staging -n urbi get all
  142  tmux attach -t r9odt
  143  ssh -i ~/.ssh/keys/staging.key admin@10.247.0.4
  144  helmfile -e oracle-staging -f helmfile/services/ingress-controller.yaml diff --context 2
  145  reset
  146  helmfile -e oracle-staging -f helmfile/services/ingress-controller.yaml diff --context 2
  147  helmfile -e oracle-staging -f helmfile/services/ingress-controller.yaml apply
  148  cat ansible/inventory/oracle-staging/oracle.yml 
  149  echo $BASTION_ID 
  150  cd dgctl/
  151  tmux attach -t r9odt
  152   history 
  153   w
  154  echo $BASTION_ID 
  155  tmux attach -t r9odt
  156  ls
  157  echo $BASTION_ID 
  158  ssh admin@10.246.0.122
  159  ssh admin@10.247.2.92
  160  clear
  161  ssh admin@10.246.0.122
  162  ping 10.246.0.122
  163  tracert 10.246.0.122
  164  traceroutt 10.246.0.122
  165  traceroute 10.246.0.122
  166  tracepath 10.246.0.122
  167  ssh admin@10.246.0.122
  168  ping 10.246.0.122
  169  ssh admin@10.246.0.122
  170  curl -k https://map-urbi.momrah.gov.sa
  171  curl -k https://map-urbi.momrah.gov.sa/api.js
  172  nslookup map-urbi.momrah.gov.sa
  173   history 
  174  ssh admin@10.246.0.122
  175  ansible-playbook  -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/bootstrap_oraclelinux.yml -l prod-kafka-1
  176  w
  177  echo $BASTION_ID 
  178  tmux attach -t r9odt
  179  cd /opt/on-premise/momrah/
  180  cat ansible/inventory/oracle-staging/oracle.yml 
  181  ssh -i ~/.ssh/keys/staging.key admin@10.247.0.158
  182  cd ../ansible-on-premise/
  183  ansible-playbook -m raw -a hostname -i /opt/on-premise/momrah/ansible/inventory/oracle-production all
  184  ansible -m raw -a hostname -i /opt/on-premise/momrah/ansible/inventory/oracle-production all
  185  ansible  -i /opt/on-premise/momrah/ansible/inventory/oracle-production all -m ping
  186  #ansible-playbook  -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/
  187   playbooks/bootstrap_oraclelinux.yml
  188  #ansible-playbook  -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/bootstrap_oraclelinux.yml -l prod-postgresql-1
  189  ansible-playbook  -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/bootstrap_oraclelinux.yml -l prod-postgresql-1
  190  ansible  -i /opt/on-premise/momrah/ansible/inventory/oracle-production prod-postgresql-1 -m raw -a 'sudo dnf install python3-dnf'
  191  ansible  -i /opt/on-premise/momrah/ansible/inventory/oracle-production prod-postgresql-1 -m raw -a 'sudo dnf install -y python3-dnf'
  192  ansible-playbook  -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/bootstrap_oraclelinux.yml -l prod-postgresql-1
  193  nano /opt/on-premise/momrah/helmfile/values/ingress-controller/common.gotmpl 
  194  nano playbooks/bootstrap_oraclelinux.yml 
  195  ansible  -i /opt/on-premise/momrah/ansible/inventory/oracle-production prod-postgresql-1 -m ping
  196  ansible-playbook  -i /opt/on-premise/momrah/ansible/inventory/oracle-production -l prod-postgresql-1  playbooks/install-packages.yml 
  197  ssh -i ~/.ssh/keys/production.key -l admin 10.246.0.122
  198  ls -la
  199  ls /
  200  cat setup_venv.sh 
  201   ansible -V
  202   ansible -v
  203   ansible --version
  204  . venv/bin/activate
  205   ansible --version
  206  ansible-playbook  -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/bootstrap_oraclelinux.yml -l prod-postgresql-1
  207  ansible-playbook  -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/bootstrap_oraclelinux.yml -l prod-postgresql-2
  208  deactivate
  209  ansible-playbook  -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/bootstrap_oraclelinux.yml -l prod-postgresql-3
  210  ansible-playbook  -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/bootstrap_oraclelinux.yml -l oracle_production
  211  nano playbooks/bootstrap_oraclelinux.yml 
  212  ansible-playbook  -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/bootstrap_oraclelinux.yml -l prod-postgresql-2
  213  . venv/bin/activate
  214  ansible-playbook  -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/bootstrap_oraclelinux.yml -l prod-postgresql-2
  215  deactivate
  216  ansible-playbook  -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/bootstrap_oraclelinux.yml -l oracle_production
  217  ssh -i ~/.ssh/keys/production.key -l admin 10.246.0.122
  218  ansible-playbook  -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/bootstrap_oraclelinux.yml -l prod-postgresql-1
  219  ssh -i ~/.ssh/keys/production.key -l admin 10.246.0.122
  220  ansible  -i /opt/on-premise/momrah/ansible/inventory/oracle-production oracle_production -m raw -b -a 'dnf install -y  zip unzip dnf-plugins-core dnf-utils'
  221  ansible  -i /opt/on-premise/momrah/ansible/inventory/oracle-production oracle_production -m raw -b -a 'dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo'
  222  ansible  -i /opt/on-premise/momrah/ansible/inventory/oracle-production oracle_production -m raw -b -a 'dnf install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin'
  223  ansible  -i /opt/on-premise/momrah/ansible/inventory/oracle-production oracle_production -m raw -b -a 'docker ps -a'
  224  ansible  -i /opt/on-premise/momrah/ansible/inventory/oracle-production oracle_production -m raw -b -a 'sudo docker ps -a'
  225  ansible  -i /opt/on-premise/momrah/ansible/inventory/oracle-production oracle_production -m raw -b -a 'systemctl status docker '
  226  ansible  -i /opt/on-premise/momrah/ansible/inventory/oracle-production oracle_production -m raw -b -a 'systemctl status docker'
  227  ssh -i ~/.ssh/keys/production.key -l admin 10.246.0.122
  228  nano play.yaml
  229  ansible-playbook  -i /opt/on-premise/momrah/ansible/inventory/oracle-production play.yaml -l oracle_production
  230  ansible-playbook  -i /opt/on-premise/momrah/ansible/inventory/oracle-production play.yaml -l oracle_production -D
  231  rm play.yaml 
  232  ansible  -i /opt/on-premise/momrah/ansible/inventory/oracle-production oracle_production -m raw -b -a 'systemctl enable --now docker'
  233  ansible  -i /opt/on-premise/momrah/ansible/inventory/oracle-production oracle_production -m raw -b -a 'sudo docker ps -a'
  234  cat /opt/on-premise/momrah/ansible/inventory/oracle-staging/oracle.yml 
  235  ansible  -i /opt/on-premise/momrah/ansible/inventory/oracle-production oracle_production -m raw -b -a 'lsblk'
  236  ssh -i ~/.ssh/keys/staging.key admin@10.247.0.158
  237  ansible  -i /opt/on-premise/momrah/ansible/inventory/oracle-production oracle_production -m raw -b -a 'mkfs.ext4 -L data /dev/sdb'
  238  ssh -i ~/.ssh/keys/staging.key admin@10.247.0.158 mount
  239  ansible  -i /opt/on-premise/momrah/ansible/inventory/oracle-production oracle_production -m raw -b -a 'mkdir /mnt/data'
  240  cd
  241  cd -
  242  cd /opt/on-premise/ansible-on-premise
  243  ssh -i ~/.ssh/keys/production.key -l admin 10.246.0.90
  244  ansible  -i /opt/on-premise/momrah/ansible/inventory/oracle-production oracle_production -m raw -b -a 'whoami'
  245  ssh -i ~/.ssh/keys/production.key -l admin 10.246.0.244
  246  ansible  -i /opt/on-premise/momrah/ansible/inventory/oracle-production oracle_production -m raw -b -a 'whoami'
  247  ssh -i ~/.ssh/keys/production.key -l admin 10.246.1.78
  248  ansible  -i /opt/on-premise/momrah/ansible/inventory/oracle-production oracle_production -m raw -b -a 'whoami'
  249  ssh -i ~/.ssh/keys/production.key -l admin 10.246.3.76
  250  cd
  251  cd -
  252  cd /opt/on-premise/ansible-on-premise
  253  ssh -i ~/.ssh/keys/production.key -l admin 10.246.0.255
  254  ssh -i ~/.ssh/keys/production.key -l admin 10.246.3.17
  255  ssh -i ~/.ssh/keys/production.key -l admin 10.246.1.119
  256  ssh -i ~/.ssh/keys/production.key -l admin 10.246.3.219
  257  kubectl --context=staging get svc
  258  pkill -9 helmfile
  259  kubectl --context=staging get svc
  260  kubectl --context=staging get ingress
  261  kubectl --context=staging get svc
  262  kubectl --context=staging get ingress
  263  kubectl --context=staging get svc
  264  kubectl --context=staging get ingress
  265  kubectl --context=staging get svc
  266  kubectl --context=staging get pods -o wide | grep balady
  267  kubectl --context=staging get svc
  268  kubectl --context=staging get pods -o wide | grep balady
  269  kubectl --context=staging get ValidatingWebhookConfiguration
  270  kubectl --context=staging delete ValidatingWebhookConfiguration urbi-ingress-nginx-admission
  271  kubectl --context=staging get pods -o wide | grep balady
  272  kubectl --context=staging describe pod balady-admin-594fcff964-q6vnf
  273  kubectl --context=staging get pods -o wide | grep balady
  274  kubectl --context=staging describe pod balady-admin-594fcff964-q6vnf
  275  kubectl --context=staging get pods -o wide | grep balady
  276  kubectl --context=staging get pods -o wide 
  277  kubectl --context=staging get pods -o wide  | grep disc
  278  kubectl --context=staging get pods -o wide  | grep balady
  279  ssh -i ~/.ssh/keys/staging.key admin@10.247.1.255
  280  w
  281  uname -a
  282  kubectl --context=staging get pods -o wide -A
  283  kubectl --context=staging get svc -o wide -A
  284  kubectl --context=staging get pods -o wide | grep balady
  285  kubectl --context=staging -n urbi exec -ti balady-frontend-85b6965f-rc4cm -- curl https://ssoappdev.momra.gov.sa 
  286  kubectl --context=staging -n urbi exec -ti balady-frontend-85b6965f-rc4cm -- curl -v https://ssoappdev.momra.gov.sa 
  287  kubectl --context=staging get svc -o wide -A
  288  cd /opt/on-premise/
  289  mc
  290  cd
  291  cd /opt/on-premise/hel
  292  kubectl --context=staging get ingress
  293  history | grep raw
  294  tmux ls
  295  tmux a -t is
  296  tmux resize-window -Z
  297  tmux set-window-size -x $(tput cols) -y $(tput lines)
  298  tmux a -t is
  299  tmux new -s is
  300  tmux a -t is
  301  clear
  302  cd /opt/on-premise/momrah/helmfile/values/ingress-controller/
  303  ls
  304  cat common.gotmpl 
  305  tmux a -t is
  306  ll
  307  ls -la
  308  tmux attach -t r9odt
  309  exho $BASTION_ID 
  310  echo $BASTION_ID 
  311  tmux attach -t r9odt
  312  tmux attach -t is
  313  clear
  314  ls
  315  curl https://10.247.9.92
  316  tmux attach -t r9odt
  317  ssh -l admin -i ~/.ssh/keys/staging.key 10.247.0.155
  318  ssh -l admin -i ~/.ssh/keys/staging.key 10.247.0.158
  319  history | gerp raw
  320  history | grep raw
  321  echo $BASTION_ID 
  322  tmux attach -t r9odt
  323  exho $BASTION_ID 
  324  echo $BASTION_ID 
  325  tmux attach -t r9odt
  326  clear
  327  kubectl --context=staging -n urbi get ingress
  328  clear
  329  cd omar/
  330  ls
  331  nslookup map-urbi.momrah.gov.sa
  332  curl map-urbi.momrah.gov.sa
  333  curl map-urbi.momrah.gov.sa/api.js
  334  wget map-urbi.momrah.gov.sa/api.js
  335  wget https://map-urbi.momrah.gov.sa/api.js
  336  wget --no-check-certificate https://map-urbi.momrah.gov.sa/api.js
  337  wget https://map-urbi.momrah.gov.sa/api.js
  338  curl https://map-urbi.momrah.gov.sa/api.js
  339  curl http://map-urbi.momrah.gov.sa/api.js
  340  wget http://map-urbi.momrah.gov.sa/api.js
  341  kubectl --context=staging get deployment
  342  kubectl --context=staging scale deployment --replicas 0 keys-admin
  343  kubectl --context=staging scale deployment --replicas 0 keys-api
  344  kubectl --context=staging scale deployment --replicas 0 keys-taskr
  345  kubectl --context=staging scale deployment --replicas 0 keys-tasker
  346  kubectl --context=staging get deployment
  347  kubectl --context=staging get pods | grep keys
  348  cd ..
  349  cd he
  350  cd momrah/
  351  ls
  352  helmfile -f helmfile/services/keys.yaml diff
  353  helmfile -f helmfile/services/keys.yaml -e oracle-staging diff
  354  helmfile -f helmfile/services/keys.yaml -e oracle-staging sync
  355  helmfile -f helmfile/services/navi/router.yaml -e oracle-staging apply
  356  helmfile -f helmfile/services/catalog.yaml -e oracle-staging apply
  357  helmfile -f helmfile/services/tiles.yaml -e oracle-staging apply
  358  ssh -i ~/.ssh/keys/staging.key admin@10.247.1.255
  359  kubectl --context=staging get pods | grep ingress
  360  kubectl --context=staging logs urbi-ingress-ingress-nginx-controller-85f7d76c59-z4cpj
  361  kubectl --context=staging get pods | grep ingress
  362  curl http://map-urbi.momrah.gov.sa/api.js
  363  cldaer
  364  clear
  365  curl http://map-urbi.momrah.gov.sa/api.js
  366  kubectl --context=staging -n urbi get deployments
  367  kubectl --context=staging -n urbi get pods
  368  clear
  369  kubectl --context=staging -n urbi get pods
  370  clear
  371  ls
  372  curl http://map-urbi.momrah.gov.sa/api.js
  373  curl https://map-urbi.momrah.gov.sa/api.js
  374  wget --no-check-certificate https://map-urbi.momrah.gov.sa/api.js
  375  tmux attach -t r9odt
  376  echo $BASTION_ID 
  377  tmux attach -t r9odt
  378  clear
  379  kubectl --context=staging get secrets -n urbi
  380  kubectl --context=staging get secrets
  381  clear
  382  ls
  383  cd /opt/
  384  ls
  385  ls containerd/
  386  sudo ls containerd/
  387  cd on-premise/
  388  ls
  389  cd on-premise-helm-charts/
  390  ls
  391  cd charts/
  392  ls
  393  cd ..
  394  clear
  395  ls
  396  cd ..
  397  ls
  398  momrah/
  399  ls
  400  cd momrah/
  401  ls
  402  cd ansible/
  403  ls
  404  cat nginx-perf.yaml 
  405  clear
  406  ls
  407  cd ..
  408  ls
  409  cd helmfile/
  410  ls
  411  cd tests/
  412  ls
  413  cd ..
  414  ls
  415  cd services/
  416  ls
  417  cat ingress-controller.yaml 
  418  cd ../values/
  419  ls
  420  cd ingress-controller/
  421  ls
  422  cat oracle-staging.yaml 
  423  ls
  424  cat common.gotmpl 
  425  history | grep helmfile
  426  ls
  427  cat ../../services/ingress-controller.yaml 
  428  ls
  429  clear
  430  kubectl --context=staging get deployments
  431  kubectl --context=staging get deployment urbi-ingress-ingress-nginx-controller -o yaml
  432  kubectl --context=staging get secret
  433  kubectl --context=staging get deployment urbi-ingress-ingress-nginx-controller -o yaml
  434  kubectl --context=staging get secret
  435  kubectl --context=staging get secret urbi-ingress-ingress-nginx-admission -o yaml
  436  echo "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJkVENDQVJ1Z0F3SUJBZ0lRQ0k2VjY3VnlOVHA2VStxOXhFMlNpekFLQmdncWhrak9QUVFEQWpBUE1RMHcKQ3dZRFZRUUtFd1J1YVd3eE1DQVhEVEkxTURJeE56RXpNamsxT0ZvWUR6SXhNalV3TVRJME1UTXlPVFU0V2pBUApNUTB3Q3dZRFZRUUtFd1J1YVd3eE1Ga3dFd1lIS29aSXpqMENBUVlJS29aSXpqMERBUWNEUWdBRWo3YmcrNmRMCmpxbEJjdS95UUZMWVE2WHRVdGJhdnV4bVdTakdHdk45OHhjZ1laUldKVWwxTkZXRG9seWpSQXpSd1p2RGI0dDUKVTBOc3dZYXpjVEx6WWFOWE1GVXdEZ1lEVlIwUEFRSC9CQVFEQWdJRU1CTUdBMVVkSlFRTU1Bb0dDQ3NHQVFVRgpCd01CTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkpqMHN6cE9GWVh2NmorclA4bXNTbkY3Cmh1NjVNQW9HQ0NxR1NNNDlCQU1DQTBnQU1FVUNJUUNBRVVkN2piOGF0WG5PWjN1OGg0TGsvcjF5WEdBMDE5MmMKYXVrOG9ydTdJUUlnVEpxdjkxbzBoUHZ3emNYSzNUK3ZTcHB6N0JZbkZ6RUloZ1BBdDRhNG13az0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=" | base64 --decode
  437  kubectl --context=staging get deployment urbi-ingress-ingress-nginx-controller -o yaml
  438  kubectl --context=staging get serviceaccount
  439  kubectl --context=staging get serviceaccount urbi-ingress-ingress-nginx -o yaml
  440  history | grep create
  441  clear
  442  kubectl --context=staging get secret urbi-ingress-ingress-nginx-admission -o yaml
  443  echo $BASTION_ID 
  444  tmux attach -t r9odt
  445  echo $BASTION_ID 
  446  tmux attach -t r9odt
  447   man update-alternatives 
  448  tmux attach -t r9odt
  449  ll
  450  rm api.js 
  451  tmux attach -t r9odt
  452  clear
  453  ls
  454  history | grep curl
  455  ls
  456  curl https://map-urbi.momrah.gov.sa/api.js
  457  curl -k https://map-urbi.momrah.gov.sa/api.js
  458  clear
  459  ls
  460  curl -k https://map-urbi.momrah.gov.sa/api.js
  461  clear
  462  sshd -T
  463  sudo sshd -T
  464  sudo sshd -T | grep  password
  465  sudo -i
  466  history | grep raw
  467  ssh admin@10.247.0.158
  468  ssh -i ~/.ssh/keys/staging.key admin@10.247.0.158
  469  cd /opt/on-premise/momrah/
  470  helmfile -e oracle-staging -f helmfile/services/haproxy.yaml diff
  471  helmfile -e oracle-staging -f helmfile/services/haproxy.yaml apply
  472  helmfile -e oracle-staging -f helmfile/services/redis.yaml diff
  473  helmfile -e oracle-staging -f helmfile/services/pro-api.yaml diff
  474  helmfile -e oracle-staging -f helmfile/services/pro-api.yaml apply
  475  helmfile -e oracle-staging -f helmfile/services/pro-api.yaml delete
  476  helmfile -e oracle-staging -f helmfile/services/pro-api.yaml diff
  477  helmfile -e oracle-staging -f helmfile/services/pro-api.yaml apply
  478  curl http://map-urbi.momrah.gov.sa
  479  curl -v http://map-urbi.momrah.gov.sa
  480  curl -v http://map-urbi.momrah.gov.sa/api.js
  481  curl -v https://map-urbi.momrah.gov.sa/api.js
  482  curl -v http://map-urbi.momrah.gov.sa/api.js
  483  ip a
  484  curl -v http://map-urbi.momrah.gov.sa/api.js
  485  curl -v https://map-urbi.momrah.gov.sa/api.js
  486  curl -k -v https://map-urbi.momrah.gov.sa/api.js
  487  curl -k -v https://map-urbi.momrah.gov.sa/api.js | head -n 20
  488  curl -k -v https://map-urbi.momrah.gov.sa/api.js |& head -n 20
  489  curl -k -v https://map-urbi.momrah.gov.sa/api.js |& head -n 40
  490  curl -k -v https://map-urbi.momrah.gov.sa/api.js |& head -n 50
  491  helmfile -e oracle-staging --namespace urbi -f helmfile/services/ingress-controller.yaml diff
  492  helmfile -e oracle-staging --namespace urbi -f helmfile/services/ingress-controller.yaml apply
  493  kubectl get pods
  494  kubectl --context=staging get pods
  495  kubectl --context=staging describe pod urbi-ingress-nginx-controller-65b596b5d4-zq2j9
  496  kubectl --context=staging describe deployment urbi-ingress-nginx-controller
  497  kubectl --context=staging describe svc
  498  kubectl --context=staging get pods
  499  kubectl --context=staging delete deployment urbi-ingress-nginx-controller-65b596b5d4-zq2j9
  500  kubectl --context=staging delete deployment urbi-ingress-nginx-controller
  501  curl -k -v https://map-urbi.momrah.gov.sa/api.js |& head -n 50
  502  curl -k -v https://map-urbi.momrah.gov.sa/api.js 
  503  curl  -v http://map-urbi.momrah.gov.sa/api.js 
  504  curl -k -v https://map-urbi.momrah.gov.sa/api.js |& head -n 50
  505  helmfile -e oracle-staging -f helmfile/services/pro-api.yaml apply
  506  kubectl --context=staging get pods
  507  kubectl --context=staging get pods | grep starter
  508  kubectl --context=staging delete pod pro-api-asset-importer-starter-6fz78
  509  kubectl --context=staging delete pod pro-api-asset-importer-starter-kktz9
  510  helmfile -e oracle-staging -f helmfile/services/pro-ui.yaml apply
  511  helmfile -e oracle-staging -f helmfile/services/pro-api.yaml apply
  512  helmfile -e oracle-staging -f helmfile/services/citylens.yaml diff
  513  history | grep raw
  514  ansible -i ansible/inventory/oracle-staging -m setup stg-elasticsearch-3 | grep python
  515  ansible -i ansible/inventory/oracle-staging -m setup stg-elasticsearch-3 -e ansible_python_interpreter=/usr/bin/python3.6
  516  ansible -i ansible/inventory/oracle-staging -m shell -a 'ls' stg-elasticsearch-3 -e ansible_python_interpreter=/usr/bin/python3.6
  517  ssh -i ~/.ssh/keys/staging.key admin@10.247.0.158
  518  ssh -i ~/.ssh/keys/staging.key admin@10.247.1.255
  519  kubectl --context=staging get pods -o wide | grep haproxy
  520  kubectl --context=staging get svc
  521  kubectl --context=staging get pods -o wide | grep pro
  522  kubectl --context=staging logs pro-api-asset-importer-starter-gf5xf
  523  kubectl --context=staging get pods -o wide | grep pro
  524  kubectl --context=staging logs pro-api-asset-importer-starter-x7lf9
  525  kubectl --context=staging get pods -o wide | grep pro
  526  w
  527  kubectl --context=staging get pods -o wide | grep pro
  528  kubectl --context=staging logs pro-api-permissions-55f777dd4-8jnmp
  529  kubectl --context=staging get svc
  530  kubectl --context=staging logs 
  531  kubectl --context=staging get pods
  532  kubectl --context=staging logs urbi-ingress-ingress-nginx-controller-85f7d76c59-z4cpj
  533  kubectl --context=staging logs urbi-ingress-ingress-nginx-controller-85f7d76c59-z4cpj -f
  534  kubectl --context=staging get svc
  535  kubectl --context=stagingget svc -o yaml urbi-ingress-ingress-nginx-controller
  536  helm list --kube-context staging -A
  537  kubectl --context=staging get svc
  538  kubectl --context=staging describe svc urbi-ingress-ingress-nginx-controller
  539  kubectl --context=staging logs urbi-ingress-ingress-nginx-controller-85f7d76c59-z4cpj -f
  540  kubectl --context=staging get pods
  541  kubectl --context=staging logs pro-api-permissions-55f777dd4-8jnmp
  542  kubectl --context=staging get pods
  543  kubectl --context=staging logs pro-api-permissions-54677fc956-jtz8t
  544  kubectl --context=staging get pods
  545  kubectl --context=staging logs pro-api-asset-importer-starter-h865w
  546  kubectl --context=staging get pods
  547  kubectl --context=staging logs pro-api-permissions-54677fc956-jtz8t
  548  kubectl --context=staging get pods
  549  kubectl --context=staging logs pro-api-asset-importer-29011320-6vzfb
  550  kubectl --context=staging get pods
  551  kubectl --context=staging -n urbi exec -ti balady-frontend-85b6965f-rc4cm -- curl -v https://haproxy:9200
  552  kubectl --context=staging -n urbi exec -ti balady-frontend-85b6965f-rc4cm -- curl -k -v https://haproxy:9200
  553  kubectl --context=staging logs haproxy-c6d9f78c8-c79cx
  554  kubectl --context=staging -n urbi exec -ti balady-frontend-85b6965f-rc4cm -- curl -k -v https://haproxy:9200
  555  kubectl --context=staging -n urbi exec -ti balady-frontend-85b6965f-rc4cm -- curl -k -v https://haproxy:9300
  556  kubectl --context=staging -n urbi exec -ti balady-frontend-85b6965f-rc4cm -- curl -k -v https://haproxy:9200
  557  kubectl --context=staging -n urbi exec -ti balady-frontend-85b6965f-rc4cm -- curl -k -v http://haproxy:9200
  558  kubectl --context=staging -n urbi exec -ti balady-frontend-85b6965f-rc4cm -- curl -k -v http://10.247.3.1:9200
  559  kubectl --context=staging -n urbi exec -ti balady-frontend-85b6965f-rc4cm -- curl -k -v http://10.247.3.1:9300
  560  kubectl --context=staging -n urbi exec -ti balady-frontend-85b6965f-rc4cm -- curl -k -v https://10.247.3.1:9200
  561  kubectl --context=staging get pods
  562  kubectl --context=staging logs pro-api-asset-importer-worker-l2v8d-nwlwq
  563  kubectl --context=staging get pods
  564  kubectl --context=staging logs pro-api-asset-importer-worker-znx26-9zwjq
  565  kubectl --context=staging logs pro-api-asset-importer-worker-zs6xz-rltrl
  566  kubectl --context=staging logs pro-api-asset-importer-worker-2grhj-9px5d
  567  ssh -i ~/.ssh/keys/staging.key admin@10.247.1.255
  568  kubectl --context=staging get pods
  569  nano .bashrc
  570  tmux attach -t r9odt
  571  echo $BASTION_ID 
  572  tmux attach -t r9odt
  573  echo $BASTION_ID 
  574  tmux attach -t r9odt
  575  kubectl --context=staging get pods 
  576  kubectl --context=staging logs -f elasticsearch-dump-parking-gfk4g
  577  kubectl --context=staging delete -f 1.yaml
  578  nano .bashrc
  579  >1.yaml 
  580  nano 1.yaml 
  581  kubectl --context=staging apply  -f 1.yaml
  582  kubectl --context=staging get pods  | grep ela
  583  kubectl --context=staging logs -f elasticsearch-rest-parking-2vqfm
  584  kubectl --context=staging desc -f elasticsearch-rest-parking-2vqfm
  585  kubectl --context=staging describe pod -f elasticsearch-rest-parking-2vqfm
  586  kubectl --context=staging describe pod elasticsearch-rest-parking-2vqfm
  587  kubectl --context=staging describe pod -f elasticsearch-rest-parking-2vqfm
  588  kubectl --context=staging describe pod elasticsearch-rest-parking-2vqfm
  589  kubectl --context=staging logs -f elasticsearch-rest-parking-2vqfm
  590  kubectl --context=staging delete -f 1.yaml
  591  cat 1.yaml 
  592  ssh -i ~/.ssh/keys/staging.key admin@10.247.1.255
  593  cd
  594  cd -
  595  cd /opt/on-premise/ansible-on-premise
  596  ssh -i ~/.ssh/keys/production.key -l admin 10.246.0.122
  597  ssh -i ~/.ssh/keys/production.key -l admin 10.246.2.7
  598  ssh -i ~/.ssh/keys/production.key -l admin 10.246.1.251
  599  ssh -i ~/.ssh/keys/production.key -l admin 10.246.2.22
  600  tmux attach -t is
  601  tmux attach -t r9odt
  602     ls -la
  603   cat 1.yaml 
  604   cd /opt/on-premise/
  605  ls
  606  mc
  607  tmux attach -t is
  608  ssh prod-postgres1
  609  ssh prod-postgres-1
  610  ssh stg-postgres-1
  611  less /etc/hosts 
  612  sudo vim /etc/hosts
  613  ssh prod-postgres-1
  614  ssh prod-postgresql-1
  615  ssh 10.246.0.122
  616  ssh 10.246.0.90
  617  ssh stg-postgresql-1
  618  ssh 
  619  ssh admin@prod-postgresql-1
  620  ssh admin@stg-elasticsearch-1
  621  ssh admin@10.247.3.1
  622  ssh admin@stg-elasticsearch-1 -i ~/.ssh/keys/staging.key
  623  ssh admin@prod-elasticsearch-1 -i ~/.ssh/keys/production.key
  624  ssh -i ~/.ssh/keys/production.key prod-postgresql-1
  625  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
  626  ssh -i ~/.ssh/keys/production.key admin@prod-cassandra-1
  627  telnet prod-cassandra-1 7199
  628  nc prod-cassandra-1 7199
  629  ping prod-cassandra-1
  630  ssh -i ~/.ssh/keys/production.key admin@prod-cassandra-1
  631  ping prod-cassandra-1
  632  telnet prod-cassandra-1
  633  ssh -i ~/.ssh/keys/production.key admin@prod-cassandra-1
  634  ssh -i ~/.ssh/keys/staging.key admin@stg-cassandra-1
  635  ssh -i ~/.ssh/keys/production.key admin@prod-cassandra-1
  636  ssh -i ~/.ssh/keys/production.key admin@prod-cassandra-2
  637  ssh -i ~/.ssh/keys/production.key admin@prod-cassandra-3
  638  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
  639  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-2
  640  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-3
  641  ssh -i ~/.ssh/keys/production.key admin@prod-kafka-1
  642  ssh -i ~/.ssh/keys/production.key admin@prod-kafka-2
  643  ssh -i ~/.ssh/keys/production.key admin@prod-kafka-3
  644  ssh -i ~/.ssh/keys/production.key admin@prod-elasticsearch-1
  645  ssh -i ~/.ssh/keys/production.key admin@prod-elasticsearch-2
  646  ssh -i ~/.ssh/keys/production.key admin@prod-elasticsearch-3
  647  ssh -i ~/.ssh/keys/production.key admin@prod-cassandra-1
  648  ssh -i ~/.ssh/keys/staging.key admin@stg-cassandra-1
  649  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
  650  tmux attach -t r9odt
  651  tmux attach -t шы
  652  tmux attach -t is
  653  tmux attach -t r9odt
  654  kubectl --context=staging get ns
  655  kubectl --context=staging -n airflow get all
  656  tmux attach -t r9odt
  657  kubectl get pods
  658  more /etc/os-release 
  659  ip a
  660  sudo -i
  661  kubectl get pods
  662  ll .kube/
  663  cat .kube/config 
  664  kubectl get pods
  665  w
  666  kubectl version
  667  kubectl get ns
  668  kubectl config get-contexts
  669  kubectl --context staging
  670  kubectl --context staging get pods
  671  kubectl config get-contexts
  672  env
  673  kubectl 
  674  kubectl kubectl config use-context staging
  675  kubectl config use-context staging
  676  kubectl get pods
  677  kubectl config get-contexts
  678  env KUBECONFIG
  679  more ~/.kube/config 
  680  ll /opt/kube
  681  ll /opt/
  682  ll /opt/on-premise/
  683  ll /etc/profile.d/99-helm.sh
  684  ll /etc/profile.d/
  685  cat /etc/profile.d/99-on-premise-env.sh 
  686  env | grep KUBECONFIG
  687  echo $KUBECONFIG
  688  ll .ssh/
  689  cat .ssh/authorized_keys
  690  vim .ssh/authorized_keys
  691  cat .ssh/authorized_keys
  692  more /etc/motd
  693  sudo vim /etc/motd
  694  more /etc/motd
  695  more /etc/motd.d/cockpit 
  696  sudo > /etc/motd
  697  sudo vim /etc/motd.d/on-premise
  698   sudo vim /etc/motd
  699  kubectl get pods
  700  k9s
  701  curl -sS https://webinstall.dev/k9s | bash
  702  k9s
  703  kubectl get top
  704  kubectl top pods
  705  df
  706  more /etc/os-release 
  707  sudo dnf 
  708  sudo dnf search python3.12-dnf
  709  sudo dnf search python3.12
  710  sudo dnf search python3.12 | grep dnf
  711  yum
  712  yum search python3.12
  713  yum search python3.12-dnf
  714  rpm saerch python3.12
  715  kubectl get pods
  716  w
  717  ll /home/
  718  ll /home/opc/
  719  sudo ll /home/opc/
  720  sudo ls /home/opc/
  721  sudo yum install bash-completion
  722  kubectl completion bash | sudo tee /etc/bash_completion.d/kubectl > /dev/null
  723  sudo chmod a+r /etc/bash_completion.d/kubectl
  724  source /usr/share/bash-completion/bash_completion
  725  more .bashrc
  726  vim .bashrc
  727  kubectl version
  728  kubectl get nodes
  729  kubectl get deployments.apps 
  730  kubectl get pods | grep haproxy
  731  kubectl get pods | grep pro-
  732  kubectl get ns
  733  kubectl -n kube-system get pods
  734  kubectl get ns
  735  kubectl get pods -A
  736  kubectl get ns
  737  vim .bashrc
  738  tmux ls
  739  tmux a -t is
  740  tmux attach -t r9odt
  741  tmux a -t is
  742  ssh -i ~/.ssh/keys/production.key prod-cassandra-1
  743  ssh -i ~/.ssh/keys/production.key prod-cassandra-2
  744  ssh -i ~/.ssh/keys/production.key admin@prod-cassandra-1
  745  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
  746  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-1
  747  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
  748  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-1
  749  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
  750  tmux ls
  751  tmux a -t is
  752  tmux attach -t r9odt
  753  kubectl get pods -A
  754  kubectl get pod
  755  mc
  756  tmux attach -t r9odt
  757  tmux a -t is
  758  echo $KUBECONFIG
  759  echo $BASTION_ID 
  760  tmux attach -t r9odt
  761  clear
  762  kubectl --context=staging get secrets 
  763  kubectl --context=staging get secret catalog-api-deploys
  764  kubectl --context=staging get secret catalog-api-deploys -o yaml
  765  kubectl --context=staging get secret pro-api-secret -o yaml
  766  kubectl --context=staging get secret pro-api-secret -o yaml | grep -E '^\s+[a-zA-Z0-9_-]+:' | while read -r line; do     key=$(echo "$line" | cut -d: -f1 | tr -d ' ');     value=$(echo "$line" | cut -d: -f2 | tr -d ' ');     decoded_value=$(echo "$value" | base64 --decode);     echo "$key: $decoded_value"; done
  767  clear
  768  ls
  769  kubectl --context=staging get jobs
  770  kubectl --context=production get jobs
  771  kubectl --context=staging get jobs
  772  kubectl --context=production get jobs
  773  history
  774  kubectl --context=production get all
  775  clear
  776  tmux a -t is
  777  ssh -i ~/.ssh/keys/staging.key admin@10.247.3.1
  778  vim main.yml 
  779  vim install_oraclelinux.yml 
  780  cd ../..
  781  cd ..
  782  vim ~/.bashrc
  783  env | grep HIST
  784  vim ~/.bashrc
  785  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-staging playbooks/elastic.yml 
  786  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-staging playbooks/elastic.yml -vv
  787  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-staging playbooks/elastic.yml -vvvv
  788  vim roles/docker/tasks/install_oraclelinux.yml 
  789  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-staging playbooks/elastic.yml -vvvv
  790  vim roles/docker/tasks/install_oraclelinux.yml 
  791  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-staging playbooks/elastic.yml
  792  ll
  793  la
  794  ls -la
  795  less .gitignore 
  796  ./setup_venv.sh 
  797  source ./venv/bin/activate
  798  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-staging playbooks/elastic.yml
  799  deactivate
  800  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-staging playbooks/elastic.yml
  801  source ./venv/bin/activate
  802  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-staging playbooks/elastic.yml
  803  ansible-galaxy collection install ansible.posix
  804  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-staging playbooks/elastic.yml
  805  ansible-galaxy collection install community.general
  806  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-staging playbooks/elastic.yml
  807  deactivate 
  808  cd
  809  kubectl config show
  810  kubectl config
  811  kubectl config view
  812  less /etc/hosts
  813  ping 169.254.169.254
  814  ping registry.momrah.gov.sa
  815  ssh -i ~/.ssh/keys/staging.key admin@10.247.3.1
  816  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-1
  817  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
  818  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-2
  819  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-3
  820  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-1
  821  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-3
  822  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-2
  823  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
  824  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-1
  825  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-1
  826  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
  827  ll
  828  cd tmp/
  829  ll
  830  cd /opt/on-premise/
  831  ll
  832  cd misc-k8s/
  833  ll
  834  less network-toolbox.yaml 
  835  kubectl -n urbi get pods
  836  kubectl --context=staging -n urbi get pods
  837  kubectl --context=staging -n urbi get pods network-toolbox 
  838  kubectl --context=production get ns
  839  kubectl --context=production -n urbi get po
  840  less network-toolbox.yaml 
  841  kubectl --context=production apply -f network-toolbox.yaml 
  842  kubectl --context=production -n urbi get po
  843  kubectl --context=production -n urbi describe pod network-toolbox 
  844  kubectl --context=production -n urbi get po
  845  kubectl --context=production -n urbi exec -it network-toolbox -- bash
  846  kubectl --context=staging -n urbi exec -it network-toolbox -- bash
  847  kubectl --context=production -n urbi exec -it network-toolbox -- bash
  848  kubectl --context=staging -n urbi exec -it network-toolbox -- bash
  849  kubectl --context=staging -n urbi logs catalog-api-6d4bfc97f8-jth5x 
  850  kubectl --context=staging -n urbi logs mapgl-js-api-695dd6bbd4-phxq5 
  851  kubectl --context=staging -n urbi logs tiles-api-6c7b9449b7-44vct 
  852  kubectl --context=staging -n urbi logs tiles-api-6c7b9449b7-44vct -c tiles-api
  853  k9s --context=staging -n urbi
  854  kubectl --context=staging -n urbi exec -it network-toolbox -- bash
  855  kubectl --context=production -n urbi exec -it network-toolbox -- bash
  856  tmux a -t is
  857  clear
  858  ls
  859  cd omar/
  860  ls
  861  nano tst.yaml
  862  kubectl --context=production apply -f tst.yaml 
  863  rm tst.yaml 
  864  nano test.yaml
  865  kubectl --context=production apply -f test.yaml 
  866  kubectl --context=production get pods
  867  kubectl --context=production logs test-reachability
  868  history
  869  kubectl --context=production -n urbi exec -it network-toolbox -- bash
  870  ssh -i ~/.ssh/keys/staging.key -l admin 10.247.1.255
  871  kubectl --context=staging -n urbi get deployment
  872  for d in balady-admin balady-admin balady-discrepancy-relay balady-frontend balady-message-relay balady-public ; do kubectl --context=staging -n urbi scale deployment  ${d} --replicas 0;done
  873  kubectl --context=staging -n urbi get deployment
  874  for d in balady-admin balady-discrepancy balady-admin balady-discrepancy-relay balady-frontend balady-message-relay balady-public ; do kubectl --context=staging -n urbi scale deployment  ${d} --replicas 0;done
  875  kubectl --context=staging -n urbi get deployment
  876  cd ..
  877  ssh -i ~/.ssh/keys/production.key -l admin 10.247.0.4
  878  ssh -i ~/.ssh/keys/production.key -l admin 10.246.0.122
  879  cd
  880  oci ce cluster create-kubeconfig --cluster-id ocid1.cluster.oc1.me-jeddah-1.aaaaaaaazgf36asfq237gysutz3y57cm5qnwtj6g7hmkldgqgcw3tcc6tsda --file ccc --region me-jeddah-1 --token-version 2.0.0  --kube-endpoint PRIVATE_ENDPOINT
  881  ls
  882  ls -la
  883  nano kc
  884  kubectl --kubeconfig kc get pods
  885  nano /home/jbadmin/.oci/config
  886  mkdir .oci
  887  nano /home/jbadmin/.oci/config
  888  kubectl --kubeconfig kc get pods
  889  nano /home/jbadmin/.oci/config
  890  nano .oci/key.pem
  891  kubectl --kubeconfig kc get pods
  892  oci setup repair-file-permissions --file /home/jbadmin/.oci/config
  893  kubectl --kubeconfig kc get pods
  894  oci setup repair-file-permissions --file /home/jbadmin/.oci/key.pem
  895  kubectl --kubeconfig kc get pods
  896  kubectl --kubeconfig kc apply -f /opt/on-premise/admin-sa.yaml 
  897  kubectl get secret -n kube-system admin-sa-token --template {{.data.token}}
  898  cat kc 
  899  kubectl get secret -n kube-system admin-sa-token --template {{.data.token}} | base64 -d
  900  kubectl --kubeconfig kc get secret -n kube-system admin-sa-token --template {{.data.token}} | base64 -d
  901  rm .oci/config 
  902  rm .oci/key.pem 
  903  rm kc
  904  kubectl get pods
  905  kubectl get pods --context production
  906  kubectl get ns --context production
  907  kubectl  --context production create ns urbi
  908  ll
  909  cd tmp/
  910  ll
  911  scp -i ~/.ssh/keys/staging.key admin@stg-postgresql-1:/tmp/spilo.tar.gz .
  912  ll
  913  scp -i ~/.ssh/keys/production.key spilo.tar.gz  admin@prod-postgresql-1:/tmp/
  914  scp -i ~/.ssh/keys/production.key spilo.tar.gz  admin@prod-postgresql-2:/tmp/
  915  scp -i ~/.ssh/keys/production.key spilo.tar.gz  admin@prod-postgresql-3:/tmp/
  916  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
  917  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-2
  918  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-3
  919  cd /opt/on-premise/ansible-on-premise/
  920  ansible-playbook playbooks/kafka.yml -i /opt/on-premise/momrah/ansible/inventory/oracle-staging --list-tags
  921  ansible-playbook playbooks/kafka.yml -i /opt/on-premise/momrah/ansible/inventory/oracle-staging 
  922  ansible-playbook playbooks/kafka.yml -i /opt/on-premise/momrah/ansible/inventory/oracle-staging -t kafka
  923  cd /opt/on-premise/momrah/ansible/
  924  grep -ril ansible_python_interpreter
  925  grep -ri ansible_python_interpreter
  926  ssh -i ~/.ssh/keys/staging.key admin@10.247.1.255
  927  ssh -i ~/.ssh/keys/staging.key admin@10.247.0.158
  928  less ~/.ssh/keys/
  929  ls ~/.ssh/keys/
  930  ssh -i ~/.ssh/keys/staging.key admin@10.247.2.92
  931  ssh -i ~/.ssh/keys/staging.key admin@10.247.1.255
  932  ssh -i ~/.ssh/keys/staging.key admin@10.247.2.92
  933  ssh -i ~/.ssh/keys/staging.key admin@10.247.3.1
  934  ssh -i ~/.ssh/keys/staging.key admin@10.247.2.92
  935  ssh -i ~/.ssh/keys/staging.key admin@10.247.1.255
  936  ssh -i ~/.ssh/keys/staging.key admin@10.247.3.1
  937  ssh -i ~/.ssh/keys/staging.key admin@10.247.2.92
  938  ssh -i ~/.ssh/keys/staging.key admin@10.247.1.255
  939  ssh -i ~/.ssh/keys/staging.key admin@10.247.3.1
  940  ssh -i ~/.ssh/keys/staging.key admin@10.247.2.92
  941  vim inventory/oracle-staging/group_vars/elasticsearch/elasticsearch_vars.yaml 
  942  ssh -i ~/.ssh/keys/staging.key admin@10.247.3.1
  943  vim inventory/oracle-staging/group_vars/elasticsearch/elasticsearch_vars.yaml 
  944  ls inventory/
  945  ls inventory/oracle-production/
  946  vim inventory/oracle-production/oracle.yml 
  947  ls inventory/oracle-production/group_vars/
  948  ls inventory/oracle-production/group_vars/elasticsearch/
  949  sudo yum install bat
  950  bat inventory/oracle-production/group_vars/elasticsearch/elasticsearch_vars.yaml 
  951  bat inventory/oracle-production/group_vars/kafka/vars.yml 
  952  yum search yazi
  953  yum search да
  954  yum search lf
  955  yum install lf
  956  sudo yum install lf
  957  sudo yum install lfm
  958  sudo yum install nnn
  959  nnn
  960  vim .
  961  ll
  962  pwd
  963  ls group_vars/
  964  ls inventory/oracle-production/
  965  ls inventory/oracle-production/group_vars/
  966  mc -b
  967  pwd
  968  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/patroni.yml 
  969  source venv/bin/activate
  970  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/patroni.yml 
  971  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/cassandra.yml 
  972  telnet 10.246.1.251 7000
  973  telnet 10.247.3.91 7000
  974  telnet 10.247.3.91
  975  ping 10.247.3.91
  976  ssh 10.247.3.91
  977  ping 10.246.1.78
  978  telnet 10.246.1.78 7000
  979  less /etc/hosts
  980  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/cassandra.yml 
  981  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/patroni.yml 
  982  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/elastic.yml 
  983  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/kafka.yml 
  984  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/patroni.yml 
  985  ping registry.momrah.gov.sa
  986  ping 169.254.169.254:53
  987  ping 169.254.169.254
  988  telnet registry.momrah.gov.sa
  989  telnet registry.momrah.gov.sa 53
  990  ping registry.momrah.gov.sa
  991  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/patroni.yml 
  992  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/patroni.yml -vv
  993  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/patroni.yml 
  994  ping registry.momrah.gov.sa
  995  telnet 10.246.43.127 443
  996  telnet 10.246.43.127 54
  997  telnet 10.246.43.127 53
  998  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/patroni.yml 
  999  kubectl --context=staging logs pro-api-asset-importer-starter-6fz78
 1000  tmux -v
 1001  tmux -V
 1002  sudo -i
 1003  cd /opt/on-premise/momrah/
 1004  ls
 1005  helmfile -f helmfile/services/catalog.yaml -e oracle-staging diff
 1006  helmfile -f helmfile/services/catalog.yaml -e oracle-staging apply
 1007  helmfile -f helmfile/services/keys.yaml -e oracle-staging diff --context 2
 1008  helmfile -f helmfile/services/keys.yaml -e oracle-staging apply
 1009  helm list --kube-context staging -A
 1010  helmfile -f helmfile/services/license.yaml -e oracle-staging diff
 1011  helmfile -f helmfile/services/license.yaml -e oracle-staging apply
 1012  helm list --kube-context staging 
 1013  helmfile -f helmfile/services/mapgl.yaml.yaml -e oracle-staging diff --context 2
 1014  helmfile -f helmfile/services/mapgl.yaml -e oracle-staging diff --context 2
 1015  helmfile -f helmfile/services/mapgl.yaml -e oracle-staging apply
 1016  helm list --kube-context staging 
 1017  helmfile -f helmfile/services/navi/front.yaml -e oracle-staging apply
 1018  helmfile -f helmfile/services/navi/кщгеук.yaml -e oracle-staging apply
 1019  helmfile -f helmfile/services/navi/router.yaml -e oracle-staging apply
 1020  helmfile -f helmfile/services/navi/castle.yaml -e oracle-staging apply
 1021  helm list --kube-context staging 
 1022  helmfile -f helmfile/services/navi/back-custom.yaml -e oracle-staging apply
 1023  helm list --kube-context staging 
 1024  helmfile -f helmfile/services/search.yaml -e oracle-staging apply
 1025  kubectl --context=staging -n urbi get pods
 1026  kubectl --context=staging -n urbi exec -ti keys-api-59cfd874f9-2l85j -- keysctl services
 1027  helm list --kube-context staging 
 1028  helmfile -f helmfile/services/tiles.yaml -e oracle-staging apply
 1029  kubectl --context=staging -n urbi get pods
 1030  kubectl --context=staging -n urbi get pods | grep wins
 1031  kubectl --context=staging -n urbi logs twins-api-importer-htgnc
 1032  helmfile -f helmfile/services/navi/router.yaml -e oracle-staging apply
 1033  kubectl --context=staging -n urbi get pods | grep wins
 1034  kubectl --context=staging -n urbi get pods | grep twins
 1035  kubectl --context=staging -n urbi logs twins-api-importer-cfhkp
 1036  kubectl --context=staging -n urbi get pods | grep twins
 1037  kubectl --context=staging -n urbi logs twins-api-importer-cfhkp
 1038  sudo dgctl/manual/dgctl-balady.sh 
 1039  sudo ./dgctl/manual/dgctl-balady.sh staging
 1040  cd dgctl/manual/
 1041  sudo ./dgctl/manual/dgctl-balady.sh staging
 1042  sudo ./dgctl-balady.sh staging
 1043  ssh -i ~/.ssh/keys/staging.key admin@10.247.1.255
 1044  cd ..
 1045  ls
 1046  cd ..
 1047  ls
 1048  helmfile -f helmfile/services/balady.yaml -e oracle-staging diff
 1049  helmfile -f helmfile/services/balady.yaml -e oracle-staging diff --context 2
 1050  helmfile -f helmfile/services/balady.yaml -e oracle-staging apply
 1051  helmfile -f helmfile/services/balady-discrepancy.yaml -e oracle-staging diff --context 2
 1052  helmfile -f helmfile/services/balady-discrepancy.yaml -e oracle-staging apply
 1053  kubectl --context=staging -n urbi logs twins-api-importer-cfhkp
 1054  kubectl --context=staging -n urbi logs twins-api-importer-7tm67
 1055  helmfile -f helmfile/services/haproxy.yaml -e oracle-staging apply
 1056  helmfile -f helmfile/services/twins-api.yaml -e oracle-staging diff --context 2
 1057  helmfile -f helmfile/services/twins-api.yaml -e oracle-staging apply
 1058  helmfile -f helmfile/services/redis.yaml -e oracle-staging diff
 1059  helmfile -f helmfile/services/navi/router.yaml -e oracle-staging apply
 1060  helmfile -f helmfile/services/navi/back-custom.yaml -e oracle-staging apply
 1061  helmfile -f helmfile/services/navi/front.yaml -e oracle-staging apply
 1062  helmfile -f helmfile/services/navi/router.yaml -e oracle-staging apply
 1063  helmfile -f helmfile/services/tilegen-api.yaml -e oracle-staging apply
 1064  helmfile -f helmfile/services/tiles.yaml -e oracle-staging apply
 1065  helmfile -f helmfile/services/citylens.yaml -e oracle-staging apply
 1066  cd ../momrah/
 1067  cd dgctl/manual/
 1068  sudo ./dgctl-balady.sh staging
 1069  nano dgctl-balady.sh 
 1070  sudo ./dgctl-balady.sh staging
 1071  cd ..
 1072  ssh -i ~/.ssh/keys/staging.key -l admin 10.247.3.1
 1073  ssh -i ~/.ssh/keys/staging.key -l admin 10.247.0.155
 1074  ssh -i ~/.ssh/keys/staging.key -l admin 10.247.0.4
 1075  ssh -i ~/.ssh/keys/production.key -l admin 10.246.3.219
 1076  ssh -i ~/.ssh/keys/staging.key -l admin 10.247.0.155
 1077  ssh -i ~/.ssh/keys/staging.key -l admin 10.247.0.158
 1078  tmux attach -t r9odt
 1079  reset
 1080  tmux attach -t r9odt
 1081  tmux 
 1082  tmux new -s r9odt
 1083  :q
 1084  tmux a -t is
 1085  curl -k https://map-urbi.momrah.gov.sa/api.js
 1086  curl -v -k https://map-urbi.momrah.gov.sa/api.js
 1087  cat /etc/hosts
 1088  curl -v -k https://map-urbi.momrah.gov.sa/api.js
 1089  cat /etc/hosts
 1090  history
 1091  curl -v -k https://map-urbi.momrah.gov.sa/api.js
 1092  kubectl --context=staging get nodes -v 
 1093  kubectl --context=staging get nodes
 1094  ssh -i ~/.ssh/keys/staging.key -l admin 10.247.0.158
 1095  kubectl --context=staging -n urbi scale 
 1096  kubectl --context=staging get sts
 1097  kubectl --context=staging get pods | grep citylens
 1098  sudo yum install s3cmd
 1099  vim ~/.s3cfg/me-jeddah-1-oci
 1100  sudo vim ~/.s3cfg/me-jeddah-1-oci
 1101  mkdir ~/.s3cfg
 1102  vim ~/.s3cfg/me-jeddah-1-oci
 1103  s3cmd -c ~/.s3cfg/me-jeddah-1-oci ls
 1104  s3cmd -c ~/.s3cfg/me-jeddah-1-oci ls s3://urbi-pro-test
 1105  s3cmd -c ~/.s3cfg/me-jeddah-1-oci ls s3://test-imports
 1106  s3cmd -c ~/.s3cfg/me-jeddah-1-oci ls s3://test-imports/imports/
 1107  s3cmd -c ~/.s3cfg/me-jeddah-1-oci ls
 1108  s3cmd -c ~/.s3cfg/me-jeddah-1-oci ls s3://elastic-stg-snapshot/
 1109  s3cmd -c ~/.s3cfg/me-jeddah-1-oci ls s3://momrah-data/
 1110  ll
 1111  cd tmp/
 1112  ll
 1113  s3cmd -c ~/.s3cfg/me-jeddah-1-oci get s3://momrah-data/keys.dump.25.2.2025.tar.gz
 1114  s3cmd -c ~/.s3cfg/me-jeddah-1-oci del s3://momrah-data/keys.dump.25.2.2025.tar.gz
 1115  s3cmd -c ~/.s3cfg/me-jeddah-1-oci ls s3://momrah-data/
 1116  ll
 1117  tmux a -t is
 1118  pwd
 1119  tmux a -t is
 1120   curl https://axownvq9lhmx.compat.objectstorage.me-jeddah-1.oci.customer-oci.com
 1121  curl https://axownvq9lhmx.compat.objectstorage.me-jeddah-1.oci.customer-oci.com
 1122  mkdir .s3cfg
 1123  nano .s3cfg/me-jeddah-1-oci 
 1124  s3cmd
 1125  ssh -i ~/.ssh/keys/staging.key -l admin 10.247.0.158
 1126  s3cmd -c ~/.s3cfg/me-jeddah-1-oci get s3://urbi-momrah-staging-dgctl/db/citylens.dump.2025-03-17.stg.tar.gz ./
 1127  scp -i ~/.ssh/keys/staging.key ./citylens.dump.2025-03-17.stg.tar.gz admin@10.247.0.158:
 1128  ssh -i ~/.ssh/keys/staging.key -l admin 10.247.0.158
 1129  tmux attach -t r9odt
 1130  less ../momrah/dgctl/manual/prod-manifest 
 1131  cd
 1132  cd /tmp/
 1133  ll
 1134  cd /opt/on-premise/
 1135  cd
 1136  pwd
 1137  cd tmp/
 1138  ls
 1139  scp -i ~/.ssh/keys/production.key keys.dump.25.2.2025.tar.gz   admin@prod-postgresql-1:/tmp/
 1140  ssh -i ~/.ssh/keys/production.key -l admin admin@prod-postgres-1
 1141  ssh -i ~/.ssh/keys/production.key admin admin@prod-postgres-1
 1142  ssh -i ~/.ssh/keys/production.key admin@prod-postgres-1
 1143  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
 1144  tmux a -t is
 1145  ll
 1146  cd tmp/
 1147  ll
 1148  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
 1149  tmux a -t is
 1150  s3cmd -c ~/.s3cfg/me-jeddah-1-oci get s3://urbi-momrah-staging-dgctl/db/citylens_dbs.dump.2025-03-18.stg.tar.gz ./
 1151  scp -i ~/.ssh/keys/staging.key citylens_dbs.dump.2025-03-18.stg.tar.gz admin@10.247.0.158:
 1152  ssh -i ~/.ssh/keys/staging.key -l admin 10.247.0.158
 1153  cd /opt/on-premise/momrah/
 1154  tmux attach -t r9odt
 1155  ll
 1156  cd /opt/on-premise/
 1157  ls
 1158  nano ingress-cert.yaml
 1159  k
 1160  kubectl --context=staging get pods | grep navi
 1161  tmux a -t is
 1162  tmux attach -t r9odt
 1163  tmux a -t is
 1164  kubectl --context=staging -n urbi get pods | grep citylens
 1165  ssh -i ~/.ssh/keys/staging.key -l admin 10.247.0.158
 1166  kubectl --context=staging -n urbi get pods | grep citylens
 1167  kubectl --context=staging -n urbi logs citylens-db-migration-job-xztnm
 1168  kubectl --context=staging -n urbi get pods | grep citylens
 1169  kubectl --context=staging -n urbi logs citylens-db-migration-job-97xt2
 1170  kubectl --context=staging -n urbi get pods | grep citylens
 1171  kubectl --context=staging -n urbi logs citylens-data-migration-job-h546z
 1172  kubectl --context=staging -n urbi get pods | grep citylens
 1173  kubectl --context=staging -n urbi logs citylens-routes-migration-job-8bgpm
 1174  kubectl --context=staging -n urbi get pods | grep citylens
 1175  kubectl --context=staging -n urbi logs citylens-dashboard-batch-events-5878c589d4-mqqtm
 1176  kubectl --context=staging -n urbi get pods | grep citylens
 1177  kubectl --context=staging -n urbi logs citylens-camcom-sender-fddb745b4-zhlrb
 1178  kubectl --context=staging -n urbi get pods | grep balady
 1179  kubectl --context=staging -n urbi logs balady-discrepancy-7978c9774d-j7rlz
 1180  kubectl --context=staging -n urbi logs balady-admin-f788bc4-jq999
 1181  kubectl --context=staging -n urbi get pods | grep citylens
 1182  helmfile -f helmfile/services/citylens.yaml -e oracle-staging apply
 1183  nano helmfile/values/citylens/oracle-staging.yaml 
 1184  helmfile -f helmfile/services/citylens.yaml -e oracle-staging apply
 1185  nano helmfile/values/citylens/oracle-staging.yaml 
 1186  helmfile -f helmfile/services/citylens.yaml -e oracle-staging apply
 1187  tmux attach -t r9odt
 1188  cd /opt/on-premise/ansible-on-premise/
 1189  ll
 1190  cd ../momrah/ansible/
 1191  ll
 1192  cd inventory/oracle-production/group_vars/patroni/
 1193  ll
 1194  vim vars.yml 
 1195  cd ../../../oracle-staging/group_vars/patroni/
 1196  vim vars.yml 
 1197  ll
 1198  cd ..
 1199  ll
 1200  cd database_postgres/
 1201  ll
 1202  vim vars.yml 
 1203  cd ..
 1204  ll
 1205  grep -ril citylens
 1206  grep -ri citylens
 1207  grep -ri citylens -A 2
 1208  grep -ri citylens -A 3
 1209  cd ..
 1210  grep -ri citylens -A 3
 1211  cd ..
 1212  ll
 1213  grep -ri citylens
 1214  grep -ril citylens
 1215  cd ..
 1216  grep -ril citylens
 1217  grep -ril citylens_hangfire
 1218  cd ..
 1219  grep -ril citylens_hangfire
 1220  pwd
 1221  cd
 1222  ll
 1223  grep -ril citylens_hangfire
 1224  tmux a -t is
 1225  nslookup registry.momrah.gov.sa
 1226  wget registry.momrah.gov.sa
 1227  wget https://registry.momrah.gov.sa
 1228  ls
 1229  rm index.html 
 1230  clear
 1231  kubectl --context=staging get deployments
 1232  curl -k -v https://map-urbi.momrah.gov.sa
 1233  clear
 1234  curl --location 'https://catalog-urbi.momrah.gov.sa/3.0/items?key=ab52daf5-0c3c-4b0e-8ea6-97b7fc80fff8&locale=en_SA&locale=ar_SA&region_id=216&q=cafe'
 1235  nslookup catalog-urbi.momrah.gov.sa
 1236  ping 8.8.8.8
 1237  tmux a -t is
 1238  tmux attach -t r9odt
 1239  clear
 1240  ls
 1241  kubectl --context=staging get ingress 
 1242  kubectl --context=staging describe ingress balady-discrepancy
 1243  kubectl --context=staging get ingress balady-discrepancy -o yaml
 1244  cd omar/
 1245  ls
 1246  mkdir oci-stg-ingress
 1247  cd oci-stg-ingress/
 1248  nano discrepancy.yaml
 1249  kubectl --context=staging -n urbi apply -f discrepancy.yaml 
 1250  clear
 1251  rm discrepancy.yaml 
 1252  nano be-balady-discrepancy.yaml
 1253  kubectl --context=staging -n urbi apply -f be-balady-discrepancy.yaml 
 1254  kubectl --context=staging -n urbi get ingress 
 1255  kubectl --context=staging -n urbi get ingress catalog-api -o yaml
 1256  nano be-catalog-api
 1257  nano be-catalog-api.yaml
 1258  kubectl --context=staging -n urbi apply -f be-catalog-api.yaml 
 1259  clear
 1260  kubectl --context=staging -n urbi get ingress be-catalog-api -o yaml
 1261  rm be-catalog-api.yaml 
 1262  nano be-catalog-api.yaml
 1263  kubectl --context=staging -n urbi apply -f be-catalog-api.yaml 
 1264  clear
 1265  ls
 1266  rm be-balady-discrepancy.yaml 
 1267  nano be-balady-discrepancy.yaml 
 1268  kubectl --context=staging -n urbi apply -f be-balady-discrepancy.yaml 
 1269  kubectl --context=staging -n urbi get ingress
 1270  kubectl --context=staging -n urbi get ingress tiles-api -o yaml
 1271  nano be-tiles-api.yaml
 1272  kubectl --context=staging -n urbi apply -f be-tiles-api.yaml 
 1273  \
 1274  kubectl --context=staging -n urbi get ingress navi-back-directions-car -o yaml
 1275  clear
 1276  nano be-navi-api.yaml
 1277  ls
 1278  kubectl --context=staging -n urbi apply -f be-navi-api.yaml 
 1279  rm be-navi-api.yaml 
 1280  nano be-navi-api.yaml
 1281  kubectl --context=staging -n urbi apply -f be-navi-api.yaml
 1282  kubectl --context=staging -n urbi get ingress mapgl
 1283  kubectl --context=staging -n urbi get ingress 
 1284  kubectl --context=staging -n urbi get ingress  mapgl-js-api
 1285  kubectl --context=staging -n urbi get ingress  mapgl-js-api -o yaml
 1286  nano be-mapgl-js-api.yaml
 1287  kubectl --context=staging -n urbi apply -f be-mapgl-js-api.yaml 
 1288  ls
 1289  nano be-mapgl-js-api.yaml 
 1290  kubectl --context=staging -n urbi apply -f be-mapgl-js-api.yaml 
 1291  kubectl --context=staging -n urbi get ingress balady-frontend -o yaml
 1292  kubectl --context=staging -n urbi get ingress balady-public -o yaml
 1293  clear
 1294  nano be-balady-public.yaml
 1295  kubectl --context=staging -n urbi apply -f be-balady-public.yaml 
 1296  cat be-balady-public.yaml 
 1297  tmux attach -t r9odt
 1298  clear
 1299  kubectl --context=staging -n urbi get ingerss
 1300  kubectl --context=staging -n urbi get ingress
 1301  kubectl --context=staging -n urbi get ingerss
 1302  kubectl --context=staging -n urbi get ingress
 1303  cd omar/oci-stg-ingress/
 1304  ls
 1305  tmux attach -t r9odt
 1306  kubectl --context=staging get deployments -n urbi
 1307  kubectl --context=staging get storageclass
 1308  kubectl --context=staging get pvc
 1309  kubectl --context=staging get pvc redis-data-redis-redis-cluster-0 -o yaml
 1310  kubectl --context=staging get storageclass
 1311  tmux ls
 1312  tmux a -t is
 1313  tmux attach -t r9odt
 1314   cat .bashrc
 1315   grep -r 'Kubeconfig placed as /opt/on-premise/admin-sa-kubeconfig.yml config file and already exported as KUBECONFIG env/etc 
 1316   grep -r 'Kubeconfig placed as /opt/on-premise/admin-sa-kubeconfig.yml config file and already exported as KUBECONFIG env' /etc 
 1317  kubectl --context=staging apply -f be-mapgl-js-api.yaml 
 1318  kubectl --context=staging -n urbi get ingress 
 1319  kubectl --context=staging -n urbi get ingress be-mapgl-js-api -o yaml
 1320  kubectl --context=staging -n urbi get ingress 
 1321  kubectl --context=staging apply -f .
 1322  kubectl --context=staging -n urbi get ingress 
 1323  echo $BASTION_ID 
 1324  tmux attach -t r9odt
 1325  tmux attach -t is
 1326  clear
 1327  ls
 1328  cd omar/
 1329  ls
 1330  cd ..
 1331  kubectl --context=production get pods 
 1332  clear
 1333  tmux attach -t is
 1334  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-3
 1335  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-2
 1336  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
 1337  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-3
 1338  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-2
 1339  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
 1340  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-2
 1341  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
 1342  tmux attach -t is
 1343  clear
 1344  ls
 1345  history
 1346  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
 1347  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-1
 1348  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-3
 1349  clear
 1350  ls
 1351  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-3
 1352  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
 1353  clear
 1354  ls
 1355  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
 1356  tmux attach -t r9odt
 1357  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
 1358  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-1
 1359  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
 1360  clear
 1361  tmux attach -t is
 1362  clear
 1363  ls
 1364  cd omar/
 1365  mkdir wedo
 1366  cd wedo/
 1367  ls
 1368  mkdir dqp
 1369  cd dqp/
 1370  mkdir dqp-closures-service
 1371  cd dqp-closures-service/
 1372  nano Dockerfile
 1373  ls
 1374  git clone https://github.com/omarshiha/UGCService/tree/sso/v1.1
 1375  ls
 1376  git clone https://github.com/omarshiha/UGCService.git
 1377  cd ..
 1378  ls
 1379  cd ..
 1380  ls
 1381  cd ..
 1382  ls
 1383  cd wedo/
 1384  ls
 1385  nano gt
 1386  pwd
 1387  nano /home/jbadmin/
 1388  nano /home/jbadmin/.bashrc
 1389  echo $GITHUB_TOKEN
 1390  source bashrc
 1391  source .bashrc
 1392  source /home/jbadmin/.bashrc
 1393  source .bashrc
 1394  echo $GITHUB_TOKEN
 1395  cd 
 1396  ls -la
 1397  source .bashrc
 1398  source bashrc
 1399  source ~/.bashrc
 1400  nano ~/.bashrc
 1401  source ~/.bashrc
 1402  cd omar/wedo/
 1403  ls
 1404  pwd
 1405  nano ~/.bashrc
 1406  source ~/.bashrc
 1407  echo $GITHUB_TOKEN
 1408  clear
 1409  cd dqp/dqp-closures-service/
 1410  ls
 1411  git pull https://$GITHUB_TOKEN@github.com/omarshiha/UGCService.git sso/v1.1
 1412  git clone https://$GITHUB_TOKEN@github.com/omarshiha/UGCService.git sso/v1.1
 1413  ls
 1414  cd sso/
 1415  ls
 1416  cd v1.1/
 1417  ls
 1418  cd ..
 1419  ls
 1420  rm -r sso/
 1421  clear
 1422  ls
 1423  git clone https://$GITHUB_TOKEN@github.com/omarshiha/UGCService.git
 1424  git status
 1425  ls
 1426  cd UGCService/
 1427  ls
 1428  git status
 1429  git checkout sso/v1.1
 1430  ls
 1431  cp UGC-0.0.2-SNAPSHOT.jar ..
 1432  ls
 1433  cd ..
 1434  ls
 1435  rm UGC-0.0.2-SNAPSHOT.jar 
 1436  mv Dockerfile UGCService/
 1437  ls
 1438  clear
 1439  cd UGCService/
 1440  ls
 1441  nano Dockerfile 
 1442  ls
 1443  docker build -t registry.momrah.gov.sa/urbi-omar/dqp-tasks-api:v0.1 .
 1444  ls
 1445  docker images
 1446  docker rmi registry.momrah.gov.sa/urbi-omar/dqp-tasks-api
 1447  docker rmi registry.momrah.gov.sa/urbi-omar/dqp-tasks-api:v0.1
 1448  clear
 1449  docker images
 1450  clear
 1451  ;s
 1452  ls
 1453  clear
 1454  ls
 1455  ls -la
 1456  mv .env.local .env
 1457  cat .env 
 1458  clear
 1459  ls
 1460  mkdir chart
 1461  cd chart/
 1462  ls
 1463  nano deployment.yaml
 1464  nano service.yaml
 1465  ls
 1466  nano ingress.yaml
 1467  cp /opt/on-premise/oci-stg-ingress/be-catalog-api.yaml .
 1468  clear
 1469  ls
 1470  cat be-catalog-api.yaml 
 1471  nano be-catalog-api.yaml 
 1472  clear
 1473  ls
 1474  cat be-catalog-api.yaml 
 1475  rm ingress.yaml 
 1476  mv be-catalog-api.yaml ingress.yaml
 1477  clear
 1478  cat ingress.yaml 
 1479  ls
 1480  clear
 1481  ls
 1482  cd ..
 1483  ls
 1484  docker images
 1485  history | grep pull
 1486  git pull https://$GITHUB_TOKEN@github.com/omarshiha/UGCService.git
 1487  git checkout sso/v1.1
 1488  git pull https://$GITHUB_TOKEN@github.com/omarshiha/UGCService.git
 1489  git pull
 1490  ls
 1491  ll
 1492  ls -la
 1493  cat .env
 1494  clear
 1495  history | grep build
 1496  docker build -t registry.momrah.gov.sa/urbi-omar/dqp-tasks-api:v0.1 .
 1497  ls
 1498  cd chart/
 1499  nano deployment.yaml 
 1500  nano service.yaml 
 1501  nano ingress.yaml 
 1502  clear
 1503  docker login https://registry.momrah.gov.sa
 1504  cat /home/jbadmin/.docker/config.json
 1505  docker push registry.momrah.gov.sa/urbi-omar/dqp-tasks-api:v0.1
 1506  docker images
 1507  clear
 1508  nano ingress.yaml 
 1509  tmux a -t is
 1510  clear
 1511  ls
 1512  cd omar/
 1513  ls
 1514  cd wedo
 1515  ls
 1516  cd dqp/dqp-closures-service
 1517  ls
 1518  cd UGCService/
 1519  ls
 1520  cat Dockerfile 
 1521  tmux attach -t r9odt
 1522  tmux attach -t is
 1523  clear
 1524  cd omar/
 1525  ls
 1526  cd wedo
 1527  ls
 1528  cd dqp
 1529  ls
 1530  cd dqp-closures-service/
 1531  ls
 1532  cd UGCService/
 1533  ls
 1534  cd chart/
 1535  clear
 1536  ls
 1537  cat deployment.yaml 
 1538  nano deployment.yaml 
 1539  clear
 1540  ls
 1541  cat deployment.yaml 
 1542  cat service.yaml 
 1543  clear
 1544  cat ingress.yaml 
 1545  clear
 1546  kubectl --context=staging -n urbi apply -f deployment.yaml 
 1547  kubectl --context=staging -n urbi apply -f service.yaml 
 1548  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1549  clear
 1550  kubectl --context=staging -n urbi get pods
 1551  kubectl --context=staging -n urbi get pods | grep dqp
 1552  clear
 1553  kubectl --context=staging -n urbi get pods | grep dqp
 1554  kubectl --context=staging -n urbi logs -f dqp-tasks-api-7d9ffb758-65xfb
 1555  kubectl --context=staging -n urbi logs dqp-tasks-api-7d9ffb758-65xfb
 1556  clear
 1557  ls
 1558  cd ..
 1559  ls
 1560  cd chart
 1561  cat Dock
 1562  cat Dockerfile
 1563  ls
 1564  cd ..
 1565  cat Dockerfile 
 1566  cat .env 
 1567  clear
 1568  ls
 1569  cat Dockerfile 
 1570  clear
 1571  cd ..
 1572  ls
 1573  cd ..
 1574  ls
 1575  cd ..
 1576  ls
 1577  cd dqp/
 1578  ls
 1579  cd dqp-closures-service/
 1580  ls
 1581  cd ..
 1582  ls
 1583  cd dqp-closures-service/
 1584  ls
 1585  cd ..
 1586  ls
 1587  mv dqp-closures-service/
 1588  mv dqp-closures-service/ dqp-tasks-service/
 1589  clears
 1590  cleasr
 1591  clear
 1592  ls dqp-tasks-service/
 1593  mkdir dqp-closures-service
 1594  ls
 1595  cd dqp-closures-service/
 1596  ls
 1597   1502  docker pull registry.momrah.gov.sa/urbi-omar/citylens-bbox-coordinates-estimator-image-processor:v1.0
 1598  git remote set-url https://$GITHUB_TOKEN@github.com/WEDO-SOLUTIONS/dqp-rc-api.git
 1599  git init
 1600  git remote set-url https://$GITHUB_TOKEN@github.com/WEDO-SOLUTIONS/dqp-rc-api.git
 1601  git remote add origin https://$GITHUB_TOKEN@github.com/WEDO-SOLUTIONS/dqp-rc-api.git
 1602  git remote -v
 1603  clear
 1604  cd omar/
 1605  ls
 1606  cd wedo
 1607  ls
 1608  cd dqp/dqp-tasks-service/
 1609  ls
 1610  cd UGCService/
 1611  ls
 1612  cp -r chart/ ../
 1613  ls
 1614  cd ..
 1615  ls
 1616  rm -r UGCService/
 1617  rm -r UGCService/ -y
 1618  rm -rf UGCService/
 1619  clear
 1620  ls
 1621  ls ../dqp-closures-service/
 1622  ls
 1623  cp chart/ ../dqp-closures-service/
 1624  cp -r chart/ ../dqp-closures-service/
 1625  kubectl --context=staging -n urbi apply -f dep
 1626  kubectl --context=staging -n urbi apply -f chart/deployment.yaml 
 1627  kubectl --context=staging -n urbi get pods | grep dqp
 1628  kubectl --context=staging -n urbi logs -f dqp-tasks-api-7d9ffb758-65xfb
 1629  ls
 1630  clear
 1631  ls
 1632  cd chart/
 1633  ;s
 1634  ls
 1635  cat deployment.yaml 
 1636  nano omar
 1637  clear
 1638  cd ..
 1639  ls
 1640  cd ..
 1641  ls
 1642  cd dqp-closures-service/
 1643  ls
 1644  cd chart/
 1645  ls
 1646  cat deployment.yaml 
 1647  ls
 1648  pwd
 1649  cat service.yaml 
 1650  rm deployment.yaml 
 1651  nano deployment.yaml
 1652  rm service.yaml 
 1653  nano service.yaml
 1654  cat ingress.yaml 
 1655  clear
 1656  cd omar/wedo/dqp/dqp-closures-service/chart/
 1657  ls
 1658  rm ingress.yaml 
 1659  nano ingress.yaml
 1660  clear
 1661  rm ../../../gt
 1662  clear
 1663  ls
 1664  cat deployment.yaml 
 1665  kubectl --context=staging -n urbi apply -f deployment.yaml 
 1666  kubectl --context=staging -n urbi apply -f service.yaml 
 1667  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1668  clear
 1669  ls
 1670  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1671  kubectl --context=staging -n urbi get pods | grep dqp
 1672  kubectl --context=staging -n urbi logs dqp-closures-api-5745f7d86d-d25v6
 1673  kubectl --context=staging -n urbi apply -f deployment.yaml 
 1674  kubectl --context=staging -n urbi apply -f service.yaml 
 1675  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1676  clear
 1677  kubectl --context=staging -n urbi get pods | grep dqp
 1678  kubectl --context=staging -n urbi delete pod dqp-closures-api-5745f7d86d-d25v6
 1679  kubectl --context=staging -n urbi get pods | grep dqp
 1680  kubectl --context=staging -n urbi logs -f 
 1681  kubectl --context=staging -n urbi logs -f dqp-closures-api-5745f7d86d-ndvzn
 1682  cat ingress.yaml 
 1683  nano ingress.yaml 
 1684  kubectl --context=staging -n urbi apply -f ingress.yaml
 1685  nano service.yaml 
 1686  kubectl --context=staging -n urbi apply -f service.yaml 
 1687  clear
 1688  ls
 1689  kubectl --context=staging -n urbi get pods | grep dqp
 1690  kubectl --context=staging -n urbi logs -f dqp-closures-api-5745f7d86d-ndvzn
 1691  clear
 1692  ls
 1693  nano ingress.yaml 
 1694  kubectl --context=staging -n urbi get ingress
 1695  kubectl --context=staging -n urbi get ingress be-catalog-api -o yaml
 1696  clear
 1697  nano ingress.yaml 
 1698  cat service.yaml 
 1699  kubectl --context=staging -n urbi get service 
 1700  kubectl --context=staging -n urbi get service catalog-api -o yaml
 1701  nano service.yaml 
 1702  nano ingress.yaml 
 1703  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1704  kubectl --context=staging -n urbi apply -f service.yaml 
 1705  kubectl --context=staging -n urbi get svc
 1706  kubectl --context=staging -n urbi delete svc dqp-closures-api-service
 1707  kubectl --context=staging -n urbi get svc
 1708  clear
 1709  kubectl --context=staging -n urbi get pods
 1710  kubectl --context=staging -n urbi get pods | grep dqp
 1711  kubectl --context=staging -n urbi logs -f dqp-closures-api-5745f7d86d-ndvzn
 1712  nano ingress.yaml 
 1713  cat ingress.yaml 
 1714  kubectl --context=staging -n urbi logs -f dqp-closures-api-5745f7d86d-ndvzn
 1715  rm ingress.yaml 
 1716  nano ingress.yaml
 1717  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1718  rm ingress.yaml 
 1719  nano ingress.yaml
 1720  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1721  kubectl --context=staging -n urbi get pods | grep dqp
 1722  kubectl --context=staging -n urbi logs -f dqp-closures-api-5745f7d86d-ndvzn
 1723  rm ingress.yaml 
 1724  nano ingress.yaml
 1725  cd omar/wedo/dqp/dqp-closures-service/chart/
 1726  ls
 1727  nano ingress.yaml
 1728  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1729  kubectl --context=staging -n urbi logs -f dqp-closures-api-5745f7d86d-ndvzn
 1730  clear
 1731  ls
 1732  cd omar/wedo/dqp/dqp-closures-service/
 1733  ls
 1734  cd chart/
 1735  cd ..
 1736  ls
 1737  cd dqp-tasks-service/
 1738  ls
 1739  cd chart/
 1740  ls
 1741  kubectl --context=staging -n urbi apply -f deployment.yaml 
 1742  cat ingress.yaml 
 1743  ls
 1744  rm ingress.yaml 
 1745  nano ingress.yaml
 1746  nano service.yaml 
 1747  ls
 1748  cat service.yaml 
 1749  cat ingress.yaml 
 1750  cat deployment.yaml 
 1751  kubectl --context=staging -n urbi delete svc dqp-tasks-api-service
 1752  kubectl --context=staging -n urbi apply -f deployment.yaml 
 1753  kubectl --context=staging -n urbi apply -f service.yaml 
 1754  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1755  clear
 1756  ls
 1757  kubectl --context=staging -n urbi get pods | grep dqp
 1758  kubectl --context=staging -n urbi logs -f dqp-tasks-api-7d9ffb758-65xfb
 1759  nano deployment.yaml 
 1760  kubectl --context=staging -n urbi apply -f deployment.yaml 
 1761  kubectl --context=staging -n urbi get pods | grep dqp
 1762  clear
 1763  kubectl --context=staging -n urbi get pods | grep dqp
 1764  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1765  cat ingress.yaml 
 1766  ls omar/wedo/gt
 1767  tmux attach -t r9odt
 1768  clear
 1769  ls
 1770  cd omar/wedo/dqp/
 1771  ls
 1772  mkdir dqp-ui
 1773  cp -r dqp-closures-service/chart dqp-ui/
 1774  ls
 1775  cd dqp-ui/
 1776  ls
 1777  cd chart/
 1778  ls
 1779  nano deployment.yaml 
 1780  cat deployment.yaml 
 1781  pwd
 1782  rm deployment.yaml 
 1783  nano deployment.yaml
 1784  cat service.yaml 
 1785  rm service.yaml 
 1786  nano service.yaml
 1787  clear
 1788  ls
 1789  cat deployment.yaml 
 1790  clear
 1791  cat service.yaml 
 1792  cat ingress.yaml 
 1793  ls
 1794  cd omar/wedo/dqp/
 1795  ls
 1796  cd dqp-ui
 1797  ls
 1798  cd chart/
 1799  ls
 1800  cat deployment.yaml 
 1801  cat service.yaml 
 1802  cat ingress.yaml 
 1803  rm ingress.yaml 
 1804  nano ingress.yaml
 1805  clear
 1806  nano deployment.yaml 
 1807  kubectl --context=staging -n urbi apply -f deployment.yaml 
 1808  kubectl --context=staging -n urbi apply -f service.yaml 
 1809  nano service.yaml 
 1810  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1811  nano ingress.yaml 
 1812  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1813  kubectl --context=staging -n urbi apply -f ../../dqp-closures-service/chart/ingress.yaml 
 1814  kubectl --context=staging -n urbi apply -f ingress.yaml
 1815  kubectl --context=staging -n urbi get pods | grep dqp
 1816  kubectl --context=staging -n urbi get ingress dqp-ui-67d5bfd9bf-677m8
 1817  kubectl --context=staging -n urbi logs dqp-ui-67d5bfd9bf-677m8
 1818  cat ingress.yaml 
 1819  nano ingress.yaml 
 1820  kubectl --context=staging -n urbi apply -f ingress.yaml
 1821  kubectl --context=staging -n urbi logs dqp-ui-67d5bfd9bf-677m8
 1822  cat service.yaml 
 1823  cat deployment.yaml 
 1824  cat ingress.yaml 
 1825  kubectl --context=staging -n urbi logs dqp-ui-67d5bfd9bf-677m8
 1826  kubectl --context=staging -n urbi logs -f dqp-ui-67d5bfd9bf-677m8
 1827  nano ingress.yaml 
 1828  kubectl --context=staging -n urbi apply -f ingress.yaml
 1829  kubectl --context=staging -n urbi logs dqp-ui-67d5bfd9bf-677m8
 1830  cat ingress.yaml 
 1831  clear
 1832  ls
 1833  rm ingress.yaml 
 1834  nano ingress.yaml
 1835  ls
 1836  kubectl --context=staging -n urbi apply -f ingress.yaml
 1837  clear
 1838  cd omar/wedo/dqp/dqp-closures-service/
 1839  ls
 1840  cd chart/
 1841  ls
 1842  cat ingress.yaml 
 1843  clear
 1844  ls
 1845  cd ..
 1846  ls
 1847  cd ..
 1848  cd dqp-ui/chart/
 1849  cat ingress.yaml 
 1850  kubectl --context=staging -n urbi apply -f ingress.yaml
 1851  rm ingress.yaml 
 1852  nano ingress.yaml
 1853  nano deployment.yaml 
 1854  cat deployment.yaml 
 1855  clear
 1856  kubectl --context=staging -n urbi apply -f deployment.yaml 
 1857  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1858  kubectl --context=staging -n urbi get ingress
 1859  kubectl --context=staging -n urbi delete ingress be-dqp-ui
 1860  kubectl --context=staging -n urbi delete ingress dqp-ui
 1861  clear
 1862  nano ingress.yaml 
 1863  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1864  ls
 1865  kubectl --context=staging -n urbi get pods | grep dqp
 1866  kubectl --context=staging -n urbi logs -f dqp-ui-85d6cb49ff-fh4gn
 1867  clear
 1868  kubectl --context=staging -n urbi logs -f dqp-ui-85d6cb49ff-fh4gn
 1869  clear
 1870  kubectl --context=staging -n urbi logs -f dqp-ui-85d6cb49ff-fh4gn
 1871  rm ingress.yaml 
 1872  nano ingress.yaml
 1873  nano deployment.yaml 
 1874  cat ingress.yaml 
 1875  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1876  clear
 1877  nano deployment.yaml 
 1878  rm ingress.yaml 
 1879  nano ingress.yaml
 1880  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1881  kubectl --context=staging -n urbi apply -f deployment.yaml 
 1882  ls
 1883  cat service.yaml 
 1884  rm ingress.yaml 
 1885  nano ingress.yaml
 1886  cd omar/wedo/dqp/dqp-ui
 1887  ls
 1888  clear
 1889  ls
 1890  cd chart/
 1891  ls
 1892  rm ingress.yaml 
 1893  nano ingress.yaml
 1894  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1895  nano deployment.yaml 
 1896  kubectl --context=staging -n urbi apply -f deployment.yaml 
 1897  clear
 1898  ls
 1899  cat ingress.yam
 1900  kubectl --context=staging -n urbi get pods | grep dqp
 1901  kubectl --context=staging -n urbi logs -f dqp-ui-6b589658bf-sj8b6
 1902  rm ingress.yaml 
 1903  nano ingress.yaml
 1904  nano deployment.yaml 
 1905  kubectl --context=staging -n urbi apply -f deployment.yaml 
 1906  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1907  kubectl --context=staging -n urbi get pods | grep dqp
 1908  ls
 1909  kubectl --context=staging -n urbi logs -f dqp-ui-7c5f4b549d-nk2jm
 1910  kubectl --context=staging -n urbi get pods | grep dqp
 1911  kubectl --context=staging -n urbi exec -it dqp-ui-7c5f4b549d-nk2jm
 1912  kubectl --context=staging  exec -it dqp-ui-7c5f4b549d-nk2jm
 1913  kubectl --context=staging -n urbi exec -it dqp-ui-7c5f4b549d-nk2jm -- sh
 1914  tmux attach -t r9odt
 1915  clear
 1916  ls
 1917  cd omar/wedo/
 1918  ls
 1919  cd dqp
 1920  ls
 1921  cd dqp-ui
 1922  ls
 1923  cd chart/
 1924  ls
 1925  cat ingress.yaml 
 1926  rm ingress.yaml 
 1927  nano ingress.yaml
 1928  kubectl --context=staging apply -f ingress.yaml 
 1929  nano deployment.yaml 
 1930  kubectl --context=staging apply -f deployment.yaml 
 1931  kubectl --context=staging get pods | grep dqp
 1932  nano deployment.yaml 
 1933  ls
 1934  cat ingress.yaml 
 1935  kubectl --context=staging apply -f deployment.yaml 
 1936  kubectl --context=staging apply -f ingress.yaml 
 1937  kubectl --context=staging get pods | grep dqp
 1938  kubectl --context=staging logs -f dqp-ui-6bfc8ccf6d-x9z4m
 1939  kubectl --context=staging exec -n urbi dqp-ui-6bfc8ccf6d-x9z4m -- ls -la /usr/share/nginx/html/dqp-ui/
 1940  kubectl exec -n urbi dqp-ui-6bfc8ccf6d-x9z4m -- ls -la /etc/nginx/conf.d/
 1941  clear
 1942  kubectl --context=staging exec -n urbi dqp-ui-6bfc8ccf6d-x9z4m -- ls -la /etc/nginx/conf.d/
 1943  kubectl exec -n urbi dqp-ui-6bfc8ccf6d-x9z4m -- ls -la /etc/nginx/conf.d/
 1944  clear
 1945  kubectl --context=staging exec -n urbi dqp-ui-6bfc8ccf6d-x9z4m -- ls -la /etc/nginx/conf.d/
 1946  kubectl --context=staging exec -n urbi dqp-ui-6bfc8ccf6d-x9z4m -- ls -la /usr/share/nginx/html/dqp-ui/
 1947  clear
 1948  cd omar/wedo/dqp/dqp-ui
 1949  cd chart/
 1950  ls
 1951  cat deployment.yaml 
 1952  clear
 1953  nano deployment.yaml 
 1954  nano ingress.yaml 
 1955  kubectl --context=staging apply -f deployment.yaml 
 1956  kubectl --context=staging apply -f ingress.yaml 
 1957  ls
 1958  kubectl --context=staging get pods | grep dqp
 1959  kubectl --context=staging logs -f dqp-ui-795486464f-ntj9r
 1960  kubectl --context=staging exec dqp-ui-6bfc8ccf6d-x9z4m -- ls -la /usr/share/nginx/html/dqp-ui/
 1961  kubectl --context=staging exec dqp-ui-795486464f-ntj9r -- ls -la /usr/share/nginx/html/dqp-ui/
 1962  clear
 1963  nano deployment.yaml 
 1964  ls
 1965  clear
 1966  cd omar/wedo/dqp/dqp-ui/chart/
 1967  ls
 1968  cat ingress.yaml 
 1969  rm ingress.yaml 
 1970  nano ingress.yaml
 1971  clear
 1972  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1973  cat ingress.yaml 
 1974  clear
 1975  nano deployment.yaml 
 1976  kubectl --context=staging -n urbi apply -f deployment.yaml 
 1977  kubectl --context=staging -n urbi get pods | grep dqp
 1978  rm ingress.yaml 
 1979  nano ingress.yaml
 1980  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1981  clear
 1982  cat ingress.yaml 
 1983  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1984  clear
 1985  kubectl --context=staging -n urbi get pods | grep dqp
 1986  ls
 1987  kubectl --context=staging -n urbi logs dqp-ui-6555d4b698-hhslr
 1988  kubectl --context=staging -n urbi get service catalog-api -o yaml
 1989  tmux attach -t r9odt
 1990  cd omar/wedo/dqp/dqp-ui/
 1991  ls
 1992  cd chart/
 1993  clear
 1994  ls
 1995  rm ingress.yaml 
 1996  nano ingress.yaml
 1997  kubectl --context=staging -n urbi apply -f ingress.yaml 
 1998  kubectl --context=staging -n urbi get pods | grep dqp
 1999  clear
 2000  kubectl --context=staging get pods | grep dqp
 2001  kubectl --context=staging logs -f dqp-ui-6555d4b698-hhslr
 2002  cat ingress.yaml 
 2003  rm ingress.yaml 
 2004  nano ingress.yaml
 2005  nano deployment.yaml 
 2006  kubectl --context=staging -n urbi apply -f ingress.yaml 
 2007  cat ingress.yaml
 2008  kubectl --context=staging -n urbi get ingress
 2009  clear
 2010  ls
 2011  cat deployment.yaml 
 2012  clear
 2013  kubectl --context=staging -n urbi apply -f deployment.yaml 
 2014  kubectl --context=staging get pods | grep dqp
 2015  clear
 2016  kubectl --context=staging get pods | grep dqp
 2017  kubectl --context=staging delete pod dqp-ui-6555d4b698-hhslr
 2018  kubectl --context=staging get pods | grep dqp
 2019  clear
 2020  cat deployment.yaml 
 2021  kubectl --context=staging -n urbi apply -f deployment.yaml 
 2022  kubectl --context=staging get pods | grep dqp
 2023  clear
 2024  kubectl --context=staging get pods | grep dqp
 2025  kubectl --context=staging logs -f dqp-ui-6555d4b698-4ph9t
 2026  clear
 2027  kubectl --context=staging get pods | grep dqp
 2028  kubectl --context=staging delete pod dqp-ui-54c5d496c7-fbsct
 2029  kubectl --context=staging get pods | grep dqp
 2030  clear
 2031  kubectl --context=staging delete deployment dqp-ui
 2032  kubectl --context=staging get pods | grep dqp
 2033  kubectl --context=staging -n urbi apply -f deployment.yaml 
 2034  kubectl --context=staging get pods | grep dqp
 2035  kubectl --context=staging logs -f dqp-ui-54c5d496c7-mk7hv
 2036  nano deployment.yaml 
 2037  kubectl --context=staging -n urbi apply -f deployment.yaml
 2038  kubectl --context=staging get pods | grep dqp
 2039  clear
 2040  kubectl --context=staging get pods | grep dqp
 2041  kubectl --context=staging logs -f dqp-ui-55894d4f56-mfx5g
 2042  rm ingress.yaml 
 2043  nano ingress.yaml
 2044  clear
 2045  ls
 2046  kubectl --context=staging -n urbi apply -f ingress.yaml 
 2047  kubectl --context=staging -n urbi apply -f i
 2048  nano deployment.yaml 
 2049  kubectl --context=staging -n urbi apply -f deployment.yaml 
 2050  kubectl --context=staging get pods | grep dqp
 2051  kubectl --context=staging delete deployment dqp-ui
 2052  clear
 2053  cat deployment.yaml 
 2054  cleasr
 2055  clear
 2056  kubectl --context=staging -n urbi apply -f deployment.yaml 
 2057  kubectl --context=staging get pods | grep dqp
 2058  kubectl --context=staging -n urbi get ingress
 2059  kubectl --context=staging -n urbi get ingress be-dqp-tasks-api -o yaml
 2060  kubectl --context=staging -n urbi get ingress be-dqp-ui -o yaml
 2061  s3cmd -c ~/.s3cfg/me-jeddah-1-oci get s3://urbi-momrah-staging-dgctl/db/prosnapshot.dump.2025-04-10.stg.tar.gz ./
 2062  ls -la
 2063  scp -i ~/.ssh/keys/staging.key prosnapshot.dump.2025-04-10.stg.tar.gz admin@10.247.0.158:
 2064  tmux attach -t r9odt
 2065  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-3
 2066  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-2
 2067  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
 2068  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-2
 2069  ssh -i ~/.ssh/keys/production.key admin@prod-cassandra-2
 2070  ssh -i ~/.ssh/keys/production.key admin@prod-kafka-2
 2071  ssh -i ~/.ssh/keys/production.key admin@prod-elastic-2
 2072  ssh -i ~/.ssh/keys/production.key admin@prod-elasticsearch-2
 2073  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-2
 2074  ssh -i ~/.ssh/keys/production.key admin@prod-kafka-2
 2075  df -h
 2076  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-2
 2077  ll
 2078  ll -h
 2079  df -h
 2080  s3cmd -c ~/.s3cfg/me-jeddah-1-oci get s3://momrah-data/citylens_prod_dbs.tgz ./
 2081  ll
 2082  scp -i ~/.ssh/keys/production.key citylens_prod_dbs.tgz  admin@prod-postgresql-2:/mnt/data/
 2083  scp -i ~/.ssh/keys/production.key citylens_prod_dbs.tgz  admin@prod-postgresql-2:/mnt/data
 2084  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-2
 2085  scp -i ~/.ssh/keys/production.key citylens_prod_dbs.tgz  admin@prod-postgresql-2:
 2086  tmux attach -t is
 2087  ssh -i ~/.ssh/keys/production.key admin@prod-elasticsearch-3
 2088  tmux attach -t is
 2089   ssh -i ~/.ssh/keys/staging.key -l admin 10.247.0.158
 2090  ll
 2091  tmux attach -t is
 2092  tmux attach -t r9odt
 2093  tmux attach -t is
 2094  tmux attach -t r9odt
 2095  tmux attach -t is
 2096  clear
 2097  cd omar/wedo/dqp/dqp-ui/
 2098  cd chart/
 2099  ls
 2100  nano ingress.yaml 
 2101  ls
 2102  rm ingress.yaml 
 2103  nano ingress.yaml
 2104  kubectl --context=staging apply -f ingress.yaml 
 2105  nano deployment.yaml 
 2106  kubectl --context=staging apply -f deployment.yaml 
 2107  kubectl --context=staging get pods | grep ddqp
 2108  kubectl --context=staging get pods | grep dqp
 2109  kubectl --context=staging get pods | grep ddqp
 2110  kubectl --context=staging get pods | grep dqp
 2111  kubectl --context=staging get ingress dqp-ui -o yaml
 2112  kubectl --context=staging get ingress be-dqp-ui -o yaml
 2113  history | grep exec
 2114  kubectl --context=staging exec -ti dqp-ui-54d8fbb6f4-tpvbl --sh
 2115  kubectl --context=staging exec -ti dqp-ui-54d8fbb6f4-tpvbl --nash
 2116  kubectl --context=staging exec -ti dqp-ui-54d8fbb6f4-tpvbl --bash
 2117  kubectl --context=staging -n urbi exec dqp-ui-54d8fbb6f4-tpvbl --bash
 2118  kubectl --context=staging -n urbi exec -it dqp-ui-54d8fbb6f4-tpvbl --sh
 2119  kubectl --context=staging -n urbi logs -f  dqp-ui-54d8fbb6f4-tpvbl
 2120  wget dqp-stg.momah.gov.sa
 2121  cat index.html
 2122  clear
 2123  rm index.html 
 2124  \
 2125  tmux a -t is
 2126  clear
 2127  exit
 2128  clear
 2129  exit
 2130  clear
 2131  ls
 2132  psql -h primary.yi4afkltew5hqcwawlqwrfipgjwp7a.postgresql.me-jeddah-1.oci.oraclecloud.com -p 5432 -U spgadmin -d postgres
 2133  sudo yum install postgresql
 2134  clear
 2135  psql -h primary.yi4afkltew5hqcwawlqwrfipgjwp7a.postgresql.me-jeddah-1.oci.oraclecloud.com -p 5432 -U spgadmin -d postgres
 2136  psql -h 10.247.3.9 -p 5432 -U spgadmin -d postgres
 2137  clear
 2138  psql -h 10.247.3.9 -p 5432 -U dqp_admin -d dqp
 2139  clear
 2140  ls
 2141  cd omar/wedo/dqp/dqp-closures-service/
 2142  ls
 2143  cd chart/
 2144  ls
 2145  cd omar/wedo/dqp/dqp-closures-service/chart/
 2146  nano deployment.yaml 
 2147  kubectl --context=staging -n urbi apply -f deployment.yaml 
 2148  kubectl --context=staging -n urbi get pods | grep dqp
 2149  kubectl --context=staging -n urbi logs -f dqp-closures-api-65f8cb79f5-7pf4s
 2150  clear
 2151  nano deployment.yaml 
 2152  cat deployment.yaml 
 2153  kubectl --context=staging -n urbi apply -f deployment.yaml 
 2154  kubectl --context=staging -n urbi get pods | grep dqp
 2155  kubectl --context=staging -n urbi apply -f deployment.yaml 
 2156  kubectl --context=staging -n urbi get pods | grep dqp
 2157  kubectl --context=staging -n urbi logs -f dqp-closures-api-76d57cd6fd-4hlkm
 2158  history | grep psql
 2159  psql -h 10.247.3.9 -p 5432 -U dqp_admin -d dqp
 2160  clear
 2161  cd omar/wedo/dqp/dqp-closures-service/chart/
 2162  ls
 2163  clear
 2164  nano deployment.yaml 
 2165  nano ingress.yaml 
 2166  kubectl --context=staging -n urbi apply -f ingress.yaml 
 2167  kubectl --context=staging -n urbi apply -f deployment.yaml 
 2168  kubectl --context=staging -n urbi get pods | grep dqp
 2169  nano ingress.yaml 
 2170  kubectl --context=staging -n urbi apply -f ingress.yaml 
 2171  cd ..
 2172  ls
 2173  cd ..
 2174  ls
 2175  cd dqp-tasks-service/
 2176  cd chart/
 2177  nano ingress.yaml 
 2178  kubectl --context=staging -n urbi apply -f ingress.yaml 
 2179  ls
 2180  cd ..
 2181  cd ../dqp-ui/chart/
 2182  ls
 2183  clear
 2184  ls
 2185  cd omar/wedo/dqp/dqp-ui/chart/
 2186  nano deployment.yaml 
 2187  kubectl --context=staging -n urbi apply -f deployment.yaml 
 2188  kubectl --context=staging -n urbi get pods | grep dqp
 2189  nano ingress.yaml 
 2190  kubectl --context=staging -n urbi logs -f dqp-ui-86479f7b48-tgtbd
 2191  ls
 2192  cd omar/wedo/dqp/dqp-ui/chart/
 2193  nano deployment.yaml 
 2194  kubectl --context=staging -n urbi apply -f deployment.yaml 
 2195  nano deployment.yaml 
 2196  kubectl --context=staging -n urbi get pods | grep dqp
 2197  cd omar/wedo/dqp/dqp-ui/chart/
 2198  nano deployment.yaml 
 2199  kubectl --context=staging -n urbi apply -f deployment.yaml 
 2200  cd omar/wedo/dqp/dqp-ui/chart/
 2201  nano ingress.yaml 
 2202  cd ..
 2203  cd cd ..
 2204  cd ..
 2205  cd dqp-closures-service/
 2206  ls
 2207  cd chart/
 2208  ls
 2209  nano ingress.yaml 
 2210  kubectl --context=staging -n urbi apply -f ingress.yaml 
 2211  nano ingress.yaml 
 2212  kubectl --context=staging -n urbi apply -f ingress.yaml 
 2213  clear
 2214  ls
 2215  cd omar/
 2216  ls
 2217  cd wedo
 2218  ls
 2219  mkdir upliftment
 2220  cd upliftment/
 2221  mkdir upliftment-ui
 2222  cd upliftment-ui/
 2223  mkdir chart
 2224  cd chart/
 2225  nano deployment.yaml
 2226  nano service.yaml
 2227  ls
 2228  cd ..
 2229  ls
 2230  cd ..
 2231  ls
 2232  mkdir bbox_coordinates_estimation
 2233  ls
 2234  cd bbox_coordinates_estimation/
 2235  mkdir listener_service
 2236  cd listener_service/
 2237  mkdir chart
 2238  cd chart/
 2239  nano deployment.yaml
 2240  nano configmap.yaml
 2241  nano secret.yaml
 2242  ls
 2243  cat service.yaml
 2244  nano service.yaml
 2245  cat ingress.yaml
 2246  nano ingress.yaml
 2247  cat ingress.yaml 
 2248  clear
 2249  ls
 2250  cd ..
 2251  ls
 2252  cd ..
 2253  cd /opt/on-premise/on-premise-helm-charts/
 2254  git checkout 1.34.0
 2255  git status
 2256  git branches
 2257  git branch
 2258  git checkout add_tolerations 
 2259  git status
 2260  cd ..
 2261  ll
 2262  cd ansible-on-premise/
 2263  cd ../momrah/ansible/inventory/oracle-production/group_vars/patroni/
 2264  ll
 2265  vim vars.yml 
 2266  cd ../../..
 2267  cd ..
 2268  cd ansible-on-premise/
 2269  source venv/bin/activate
 2270  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/patroni.yml 
 2271  cd ../momrah/helmfile/values/
 2272  cd tiles/
 2273  ll
 2274  cp oracle-staging.yaml oracle-production.yaml 
 2275  vim oracle-production.yaml 
 2276  cd multi/
 2277  ll
 2278  cp oracle-staging.yaml oracle-production.yaml 
 2279  vim oracle-production.yaml 
 2280  cd ../../mapgl/
 2281  ll
 2282  vim oracle-production.yaml 
 2283  cd ../
 2284  ll
 2285  cd navi/
 2286  ll
 2287  cd castle/
 2288  ll
 2289  cp oracle-staging.yaml oracle-production.yaml 
 2290  vim oracle-production.yaml 
 2291  cd ..
 2292  ll
 2293  cd back/
 2294  ll
 2295  cp oracle-staging-ctx.yaml oracle-production-ctx.yaml 
 2296  cp oracle-staging.yaml oracle-production.yaml 
 2297  cp oracle-staging-custom-resources.yaml oracle-production-custom-resources.yaml 
 2298  vim oracle-production.yaml 
 2299  cp production-custom-resources.yaml oracle-production-custom-resources.yaml 
 2300  vim oracle-production-custom-resources.yaml 
 2301  vim oracle-production-ctx.yaml 
 2302  vim oracle-production-custom-resources.yaml 
 2303  vim oracle-production.yaml 
 2304  vim _common.gotmpl 
 2305  grep -ril custom
 2306  grep -ri custom
 2307  grep -ri navicustom
 2308  cd ..
 2309  grep -ri navicustom
 2310  grep -ri navicustom -A 2 -B 2
 2311  vim common.yaml 
 2312  cd values/navi/router/
 2313  ll
 2314  cp oracle-staging.yaml oracle-production.yaml 
 2315  vim oracle-production.yaml 
 2316  cp production.yaml oracle-production.yaml 
 2317  vim oracle-production.yaml 
 2318  ll
 2319  vim _common.gotmpl 
 2320  cd ../front/
 2321  ll
 2322  cp oracle-staging.yaml oracle-production.yaml 
 2323  vim oracle-production.yaml 
 2324  cd ../../platform/
 2325  ll
 2326  cd ../redis/
 2327  ll
 2328  vim oracle-production.yaml 
 2329  ll
 2330  vim common.yaml 
 2331  vim oracle-production.yaml 
 2332  vim ../../services/redis.yaml 
 2333  vim common.yaml 
 2334  vim oracle-production.yaml 
 2335  cd ../pro-api/
 2336  ll
 2337  ls ../keycloak/
 2338  ll
 2339  touch oracle-production.yaml 
 2340  vim oracle-production.yaml 
 2341  s3cmd -c ~/.s3cfg/me-jeddah-1-oci ls | grep pro
 2342  vim oracle-production.yaml 
 2343  cd ../pro-ui/
 2344  ll
 2345  vim oracle-production.yaml 
 2346  cd ../navi/back/
 2347  ll
 2348  less oracle-production-ctx.yaml 
 2349  less oracle-production.yaml 
 2350  cd ../front/
 2351  less oracle-production.yaml 
 2352  cd ../router/
 2353  less oracle-production.yaml 
 2354  vim oracle-production.yaml 
 2355  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
 2356  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-1
 2357  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-2
 2358  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-2
 2359  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
 2360  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-2
 2361  pwd
 2362  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-2
 2363  pwd
 2364  cd ..
 2365  cd pro-api/
 2366  ll
 2367  vim oracle-production.yaml 
 2368  deactivate 
 2369  cd
 2370  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
 2371  pwd
 2372  tmux a -t is
 2373  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
 2374  ssh -i ~/.ssh/keys/production.key admin@prod-cassandra-1
 2375  ssh -i ~/.ssh/keys/production.key admin@prod-kafka-1
 2376  ssh -i ~/.ssh/keys/production.key admin@prod-elsticsearch-1
 2377  ssh -i ~/.ssh/keys/production.key admin@prod-elasticsearch-1
 2378  cd /opt/on-premise/
 2379  cd ansible-on-premise/
 2380  ll
 2381  св
 2382  cd
 2383  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-2
 2384  cd /opt/on-premise/momrah/
 2385  vim helmfile/values/keys/oracle-production.yaml 
 2386  helmfile -f helmfile/services/keys.yaml -e oracle-production apply -i
 2387  vim helmfile/values/keys/oracle-production.yaml 
 2388  helmfile -f helmfile/services/keys.yaml -e oracle-production apply -i
 2389  helmfile -f helmfile/services/keys.yaml -e oracle-production --context==1 apply -i
 2390  helmfile -f helmfile/services/keys.yaml -e oracle-production --context=1 apply -i
 2391  vim helmfile/values/keys/oracle-production.yaml 
 2392  helmfile -f helmfile/services/keys.yaml -e oracle-production --context=1 apply -i --set.chartVersion=1.32.0
 2393  vim helmfile/common.yaml 
 2394  helmfile -f helmfile/services/keys.yaml -e oracle-production --context=1 apply -i
 2395  helmfile -f helmfile/services/keys.yaml -e oracle-production --context=1 destroy -i
 2396  helmfile -f helmfile/services/keys.yaml -e oracle-production destroy -i
 2397  helmfile -f helmfile/services/keys.yaml -e oracle-production --context=1 apply -i
 2398  vim helmfile/common.yaml 
 2399  vim helmfile/values/keys/oracle-production.yaml 
 2400  helmfile -f helmfile/services/keys.yaml -e oracle-production --context=1 apply -i
 2401  vim helmfile/values/keys/oracle-production.yaml 
 2402  vim helmfile/common.yaml 
 2403  helmfile -f helmfile/services/keys.yaml -e oracle-production --context=1 apply -i
 2404  vim helmfile/common.yaml 
 2405  vim helmfile/values/oracle-production-common.yaml 
 2406  cp helmfile/values/catalog/oracle-staging.yaml helmfile/values/catalog/oracle-production.yaml 
 2407  vim helmfile/values/catalog/oracle-production.yaml 
 2408  vim helmfile/values/catalog/common.yaml 
 2409  vim helmfile/values/catalog/oracle-production.yaml 
 2410  helmfile -f helmfile/services/catalog.yaml -e oracle-production --context=1 apply -i
 2411  cp helmfile/values/search/oracle-staging.yaml helmfile/values/search/oracle-production.yaml 
 2412  vim helmfile/values/search/oracle-production.yaml 
 2413  helmfile -f helmfile/services/search.yaml -e oracle-production --context=1 apply -i
 2414  helmfile -f helmfile/services/tiles.yaml -e oracle-production --context=1 apply -i
 2415  helmfile -f helmfile/services/mapgl.yaml -e oracle-production --context=1 apply -i
 2416  helmfile -f helmfile/services/navi/castle.yaml -e oracle-production --context=1 apply -i
 2417  helmfile -f helmfile/services/navi/back-custom.yaml -e oracle-production -l service=direction-car --context=1 apply -i
 2418  helmfile -f helmfile/services/navi/back-custom.yaml -e oracle-production -l service=navi-back-direction-car --context=1 apply -i
 2419  helmfile -f helmfile/services/navi/back-custom.yaml -e oracle-production -l service=directions-car --context=1 apply -i
 2420  helmfile -f helmfile/services/navi/back-custom.yaml -e oracle-production -l service=directions-pedestrian --context=1 apply -i
 2421  helmfile -f helmfile/services/navi/back-custom.yaml -e oracle-production -l service=freeroam --context=1 apply -i
 2422  helmfile -f helmfile/services/navi/back-custom.yaml -e oracle-production -l service=map-matching --context=1 apply -i
 2423  helmfile -f helmfile/services/navi/back-custom.yaml -e oracle-production -l service=ctx --context=1 apply -i
 2424  helmfile -f helmfile/services/navi/router.yaml -e oracle-production --context=1 apply -i
 2425  helmfile -f helmfile/services/navi/front.yaml -e oracle-production --context=1 apply -i
 2426  helmfile -f helmfile/services/redis.yaml -e oracle-production --context=1 apply -i
 2427  helmfile -f helmfile/services/redis.yaml -e oracle-production --context=1 diff | grep persistance
 2428  helmfile -f helmfile/services/redis.yaml -e oracle-production --context=1 diff | grep pvc
 2429  helmfile -f helmfile/services/redis.yaml -e oracle-production --context=1 diff | grep storageClass
 2430  helmfile -f helmfile/services/redis.yaml -e oracle-production --context=1 diff
 2431  helmfile -f helmfile/services/redis.yaml -e oracle-production --context=1 diff | grep Persist
 2432  helmfile -f helmfile/services/redis.yaml -e oracle-production --context=1 diff | grep Persist -A 3 -B 3
 2433  helmfile -f helmfile/services/redis.yaml -e oracle-production --context=1 apply -i
 2434  helmfile -f helmfile/services/pro-api.yaml -e oracle-production --context=1 apply -i
 2435  helmfile -f helmfile/services/pro-ui.yaml -e oracle-production --context=1 apply -i
 2436  helmfile -f helmfile/services/navi/router.yaml -e oracle-production --context=1 apply -i
 2437  pwd
 2438  grep -ril stg_
 2439  pwd
 2440  vim helmfile/values/citylens/oracle-production.yaml
 2441  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-1
 2442  kubectl --context=staging -n urbi get pvc
 2443  kubectl --context=production -n urbi get pvc
 2444  kubectl --context=production -n urbi describe pvc urbi-prom-prometheus-server
 2445  kubectl --context=production -n urbi get pvc
 2446  kubectl --context=production -n urbi get pods
 2447  kubectl --context=production -n urbi describe pod urbi-gf-grafana-5d794b69d8-dc9cv
 2448  kubectl --context=production -n urbi get pods
 2449  kubectl --context=production -n urbi describe pod urbi-prom-kube-state-metrics-549d99d9d7-npvph
 2450  kubectl --context=production -n urbi get pods
 2451  kubectl --context=production -n urbi describe pod urbi-gf-grafana-d779d4b9b-kgslq
 2452  kubectl --context=production -n urbi get pods
 2453  kubectl --context=production -n urbi get pvc
 2454  kubectl --context=production -n urbi get pods
 2455  kubectl --context=staging -n urbi get pods
 2456    helm show values registry-1.docker.io/bitnamicharts/redis-cluster:11.4.1
 2457   helm show values oci://registry-1.docker.io/bitnamicharts/redis-cluster:11.4.1
 2458  ps axu --forest | grep helm
 2459  kubectl --context=staging -n urbi get pods
 2460  kubectl --context=staging -n urbi describe pod redis-redis-cluster-4
 2461  kubectl --context=staging -n urbi get pvc
 2462  kubectl --context=staging -n urbi describe pod redis-redis-cluster-4
 2463  kubectl --context=staging -n urbi get pods
 2464  kubectl --context=staging -n urbi get pvc
 2465  kubectl --context=staging -n urbi get pods
 2466  kubectl --context=staging -n urbi delete pod redis-redis-cluster-4
 2467  kubectl --context=staging -n urbi get pods
 2468  kubectl --context=staging -n urbi describe pod redis-redis-cluster-4
 2469  kubectl --context=staging -n urbi get pods
 2470  kubectl --context=staging -n urbi describe pod redis-redis-cluster-4
 2471  kubectl --context=staging -n urbi get pods
 2472  kubectl --context=staging -n urbi describe pod redis-redis-cluster-4
 2473  kubectl --context=staging -n urbi get pods
 2474  kubectl --context=staging -n urbi describe pod redis-redis-cluster-3
 2475  kubectl --context=staging -n urbi delete pod redis-redis-cluster-3
 2476  kubectl --context=staging -n urbi describe pod redis-redis-cluster-3
 2477  kubectl --context=staging -n urbi delete pod redis-redis-cluster-3
 2478  kubectl --context=staging -n urbi get pods
 2479  kubectl --context=staging -n urbi delete pod redis-redis-cluster-3
 2480  kubectl --context=staging -n urbi describe pod redis-redis-cluster-3
 2481  kubectl --context=staging -n urbi delete pod redis-redis-cluster-3
 2482  kubectl --context=staging -n urbi describe pod redis-redis-cluster-3
 2483  kubectl --context=staging -n urbi delete pod redis-redis-cluster-3
 2484  kubectl --context=staging -n urbi describe pod redis-redis-cluster-3
 2485  kubectl --context=staging -n urbi delete pod redis-redis-cluster-3
 2486  kubectl --context=staging -n urbi describe pod redis-redis-cluster-3
 2487  kubectl --context=staging -n urbi get nodes
 2488  kubectl --context=staging -n urbi get pods -o wide
 2489  kubectl --context=staging -n urbi describe pod redis-redis-cluster-5
 2490  kubectl --context=staging -n urbi get pods 
 2491  kubectl --context=staging -n urbi logs redis-redis-cluster-5
 2492  kubectl --context=staging -n urbi get pods 
 2493  kubectl --context=staging -n urbi describe pod redis-redis-cluster-5
 2494  kubectl --context=staging -n urbi describe pod redis-redis-cluster-3
 2495  kubectl --context=staging -n urbi delete pod redis-redis-cluster-3
 2496  kubectl --context=staging -n urbi describe pod redis-redis-cluster-3
 2497  ps axu --forest | grep helm
 2498  kill 859499
 2499  ps axu --forest | grep helm
 2500  kill 859499
 2501  ps axu --forest | grep helm
 2502  kill -9 859307 859499
 2503  kubectl --context=staging -n urbi get pvc
 2504  kubectl --context=staging -n urbi delete pvc redis-data-redis-redis-cluster-{0,1,2,3,4,5}
 2505  kubectl --context=staging -n urbi get pods 
 2506  kubectl --context=staging -n urbi get pvc
 2507  kubectl --context=staging -n urbi get pods 
 2508  kubectl --context=staging -n urbi get pvc
 2509  kubectl --context=staging -n urbi get pods 
 2510  kubectl --context=staging -n urbi get pvc
 2511  kubectl --context=staging -n urbi get pods 
 2512  kubectl --context=staging -n urbi get pvc
 2513  kubectl --context=staging -n urbi get pods 
 2514   helm show values ingress-nginx/ingress-nginx
 2515  kubectl --context=staging -n urbi get pods 
 2516  kubectl --context=staging -n urbi describe pod haproxy-c6d9f78c8-vdv4c
 2517  kubectl --context=staging -n urbi get pods 
 2518  kubectl --context=staging -n urbi describe pod urbi-ingress-ingress-nginx-admission-create-479rg
 2519  kubectl --context=staging -n urbi get pods 
 2520  kubectl --context=staging -n urbi describe pod urbi-ingress-ingress-nginx-admission-create-479rg
 2521  kubectl --context=staging -n urbi get pods 
 2522  kubectl --context=staging -n urbi describe pod urbi-ingress-ingress-nginx-admission-create-479rg
 2523  kubectl --context=staging -n urbi get pods 
 2524  kubectl --context=staging -n urbi describe pod urbi-ingress-ingress-nginx-admission-create-r4dhw
 2525  kubectl --context=staging -n urbi get pods 
 2526  kubectl --context=staging -n urbi describe pod urbi-ingress-ingress-nginx-admission-create-r4dhw
 2527  kubectl --context=staging -n urbi get pods 
 2528  kubectl --context=staging -n urbi describe pod urbi-ingress-ingress-nginx-admission-create-j9sjn
 2529   helm show values ingress-nginx/ingress-nginx
 2530   helm show values ingress-nginx/ingress-nginx | head
 2531   helm show values ingress-nginx/ingress-nginx less
 2532   helm show values ingress-nginx/ingress-nginx | less
 2533   helm show values ingress-nginx/ingress-nginx:4.12.0 | less
 2534   helm show values ingress-nginx/ingress-nginx 4.12.0 | less
 2535   helm show values ingress-nginx/ingress-nginx --version 4.12.0 | less
 2536   helm show values oci://registry-1.docker.io/bitnamicharts/haproxy:2.2.8
 2537   helm show values oci://registry-1.docker.io/bitnamicharts/haproxy:2.2.8 | les
 2538   helm show values oci://registry-1.docker.io/bitnamicharts/haproxy:2.2.8 | less
 2539   helm show values oci://registry-1.docker.io/bitnamicharts/redis-cluster:11.4.1| less
 2540  kubectl --context=staging -n urbi get pods 
 2541  kubectl --context=staging -n urbi get pods  -A
 2542  kubectl --context=staging -n urbi delete pod search-api-74f7bfcf59-56ns4
 2543  kubectl --context=staging -n urbi get pods  -A
 2544  kubectl --context=staging -n urbi get pods
 2545  kubectl --context=producton -n urbi get pods
 2546  kubectl --context=production -n urbi get pods
 2547  nano helmfile/values/haproxy/_common.gotmpl 
 2548  kubectl --context=production -n urbi get pods
 2549  kubectl --context=production -n urbi get pods | gerp haproxy
 2550  kubectl --context=production -n urbi get pods | grep haproxy
 2551  kubectl --context=production -n urbi describe pod haproxy-69654bc647-l72nd
 2552  kubectl --context=production -n urbi get pods | grep haproxy
 2553  kubectl --context=production -n urbi describe pod haproxy-69654bc647-l72nd
 2554  helm search repo haproxy
 2555  kubectl --context=production -n urbi get pods | grep haproxy
 2556  kubectl --context=production -n urbi describe pod haproxy-78bd98b89f-jlcvt
 2557  kubectl --context=production -n urbi get pods | grep haproxy
 2558  kubectl --context=production -n urbi describe pod haproxy-78bd98b89f-jlcvt
 2559  kubectl --context=production -n urbi get pods | grep haproxy
 2560  kubectl --context=production -n urbi describe pod haproxy-78bd98b89f-jlcvt
 2561  kubectl --context=production -n urbi get pods | grep haproxy
 2562  kubectl --context=production -n urbi describe pod haproxy-78bd98b89f-jlcvt
 2563  kubectl --context=production -n urbi get pods | grep haproxy
 2564  kubectl --context=production -n urbi describe pod haproxy-78bd98b89f-jlcvt
 2565  kubectl --context=production -n urbi get pods | grep haproxy
 2566  kubectl --context=production -n urbi describe pod haproxy-78bd98b89f-47fgp
 2567  kubectl --context=production -n urbi get pods | grep haproxy
 2568  kubectl --context=production -n urbi describe pod 
 2569  kubectl --context=production -n urbi get pods | grep haproxy
 2570  kubectl --context=production -n urbi get pods | grep snapshot
 2571  kubectl --context=production -n urbi get pods | grep snap
 2572  kubectl --context=snapshot -n urbi get pods | grep snap
 2573  kubectl --context=staging -n urbi get pods | grep snap
 2574  kubectl --context=staging -n urbi logs snapshot-pro-api-asset-importer-starter-6ffzg
 2575  kubectl --context=staging -n urbi get pods | grep snap
 2576  kubectl --context=staging -n urbi logs snapshot-pro-api-asset-importer-worker-zrwlt-9k54n
 2577  kubectl --context=staging -n urbi logs snapshot-pro-api-asset-importer-worker-z74dz-hkql2
 2578  kubectl --context=staging -n urbi get pods | grep snap | awk '{print $1}' | xargs kubectl --context=staging -n urbi delete pod
 2579  kubectl --context=staging -n urbi get pods | grep snap
 2580  kubectl --context=staging -n urbi logs snapshot-pro-api-asset-importer-starter-vjvkn
 2581  kubectl --context=staging -n urbi get pods | grep snap
 2582  kubectl --context=staging -n urbi logs snapshot-pro-api-asset-importer-starter-vjvkn
 2583  kubectl --context=staging -n urbi get pods | grep snap
 2584  kubectl --context=staging -n urbi logs snapshot-pro-api-asset-importer-worker-6nvtk-cfl9n
 2585  kubectl --context=staging -n urbi get pods | grep snap
 2586  kubectl --context=staging -n urbi delete snapshot-pro-api-asset-importer-starter-vjvkn
 2587  kubectl --context=staging -n urbi delete pod snapshot-pro-api-asset-importer-starter-vjvkn
 2588  kubectl --context=staging -n urbi delete snapshot-pro-api-asset-importer-starter-vjvkn
 2589  kubectl --context=staging -n urbi get pods | grep snap
 2590  kubectl --context=staging -n urbi get pods | grep snap | awk '{print $1}' | xargs kubectl --context=staging -n urbi delete pod
 2591  kubectl --context=staging -n urbi get pods | grep snap
 2592  kubectl --context=staging -n urbi logs snapshot-pro-api-asset-importer-worker-lbjlm-jstpz
 2593  kubectl --context=staging -n urbi get pods | grep snap
 2594   s3cmd
 2595  kubectl --context=staging -n urbi get pods | grep snap
 2596  ва -р
 2597  df -h
 2598   lsblk
 2599  kubectl --context=staging -n urbi get pods | grep snap
 2600  k9s --context=production -n urbi
 2601  k9s --context=staging -n urbi
 2602  k9s --context=production -n urbi
 2603  k9s --context=staging -n urbi
 2604  k9s --context=production -n urbi
 2605  cd /opt/on-premise/
 2606  ll
 2607  cd momrah/
 2608  cd dgctl/manual/
 2609  ll
 2610  less dgctl-prod.yaml 
 2611  less dgctl.sh 
 2612  cp dgctl.sh dgctl_1_32.sh
 2613  vim dgctl_1_32.sh 
 2614  ./dgctl_1_32.sh prod
 2615  sudo ./dgctl_1_32.sh prod
 2616  mv dgctl_1_32.sh dgctl_1_34.sh 
 2617  vim dgctl_1_34.sh 
 2618  sudo ./dgctl_1_34.sh prod
 2619  mv dgctl_1_34.sh dgctl_1_35.sh 
 2620  vim dgctl_1_35.sh 
 2621  sudo ./dgctl_1_35.sh prod
 2622  sudo ./dgctl.sh prod
 2623  touch packages.yml
 2624  kubectl --context=production get storageclass
 2625  #kubectl --context=production 
 2626  s
 2627  kubectl --context=production apply -f sc-prod-rwm.yaml 
 2628  kubectl --context=production get storageclass
 2629  kubectl --context=production get pvc
 2630  kubectl --context=production describe pvc redis-data-redis-redis-cluster-3
 2631  kubectl --context=production get pvc
 2632  kubectl --context=production delete pvc redis-data-redis-redis-cluster-{0,1,2,3,4,5,6,7,8,9}
 2633  cd momrah/
 2634  #helmfile -f helmfile/services/monitoring.yaml -e oracle-production apply
 2635  helmfile -f helmfile/services/monitoring.yaml -e oracle-production apply
 2636  helmfile -f helmfile/services/monitoring.yaml -e oracle-production diff | grep image
 2637  nano helmfile/values/monitoring/grafana-common.yaml 
 2638  helmfile -f helmfile/services/monitoring.yaml -e oracle-production template | grep image
 2639  nano helmfile/values/monitoring/grafana-common.yaml 
 2640  helmfile -f helmfile/services/monitoring.yaml -e oracle-production template | grep image
 2641  nano helmfile/services/monitoring.yaml 
 2642  >prom-common.yaml 
 2643  helmfile -f helmfile/services/monitoring.yaml -e oracle-production template | grep image
 2644  helmfile -f helmfile/services/monitoring.yaml -e oracle-production template | grep 50Gi
 2645  helmfile -f helmfile/services/monitoring.yaml -e oracle-production template | grep Gi
 2646  helmfile -f helmfile/services/monitoring.yaml -e oracle-production apply
 2647  helmfile -f helmfile/services/monitoring.yaml -e oracle-production -l name=urbi-prom apply
 2648  helmfile -f helmfile/services/monitoring.yaml -e oracle-staging -l name=urbi-prom apply
 2649  ls -la
 2650  cd ..
 2651  ls 
 2652  cd momrah/
 2653  helmfile -f helmfile/services/redis.yaml -e oracle-staging template | grep image
 2654  cat helmfile/services/redis.yaml
 2655  helmfile -f helmfile/services/redis.yaml -e oracle-staging template | grep image
 2656  helmfile -f helmfile/services/redis.yaml -e oracle-staging apply cd "`printf "%b" '\0057opt\0057on\0055premise\0057momrah\0057helmfile\0057values\0057redis'`"
 2657  helmfile -f helmfile/services/redis.yaml -e oracle-staging delete
 2658  helmfile -f helmfile/services/redis.yaml -e oracle-staging apply
 2659  helmfile -f helmfile/services/ingress-controller.yaml -e oracle-staging template  | gerp image:
 2660  helmfile -f helmfile/services/ingress-controller.yaml -e oracle-staging template  | gerp image
 2661  helmfile -f helmfile/services/ingress-controller.yaml -e oracle-staging template | grepimage
 2662  helmfile -f helmfile/services/ingress-controller.yaml -e oracle-staging template | grep image
 2663  helmfile -f helmfile/services/ingress-controller.yaml -e oracle-staging apply
 2664  helmfile -f helmfile/services/ingress-controller.yaml -e oracle-staging template | grep image
 2665  helmfile -f helmfile/services/ingress-controller.yaml -e oracle-staging apply
 2666  helmfile -f helmfile/services/haproxy.yaml -e oracle-staging template | grep image
 2667  helmfile -f helmfile/services/redis.yaml -e oracle-staging diff
 2668  helmfile -f helmfile/services/haproxy.yaml -e oracle-staging diff
 2669  helmfile -f helmfile/services/haproxy.yaml -e oracle-staging apply
 2670  helmfile -f helmfile/services/haproxy.yaml -e oracle-production diff
 2671  helmfile -f helmfile/services/haproxy.yaml -e oracle-production diff --context 2
 2672  helmfile -f helmfile/services/haproxy.yaml -e oracle-production apply
 2673  ls
 2674  helmfile -f helmfile/services/haproxy.yaml -e oracle-production apply
 2675  nano helmfile/services/haproxy.yaml 
 2676  helmfile -f helmfile/services/haproxy.yaml -e oracle-production apply
 2677  touch oracle-staging.yaml
 2678  cd ../snapshot-pro-ui/
 2679  touch oracle-staging.yaml
 2680  helmfile -e oracle-staging -f helmfile/services/pro-ui.yaml diff --conteext 2
 2681  helmfile -e oracle-staging -f helmfile/services/pro-ui.yaml diff --context 2
 2682  helmfile -e oracle-staging -f helmfile/services/pro-ui.yaml apply
 2683  helmfile -e oracle-staging -f helmfile/services/snapshot-pro-api.yaml apply
 2684  helmfile -e oracle-staging -f helmfile/services/snapshot-pro.yaml -l service=snapshotProApi apply
 2685  helmfile -e oracle-staging -f helmfile/services/snapshot-pro-api.yaml apply
 2686  helmfile -e oracle-staging -f helmfile/services/snapshot-pro.yaml -l service=snapshotProApi apply
 2687  kubectl --context=staging get pods | grep snap
 2688  kubectl --context=staging get pods | grep snap | wc -l
 2689  kubectl --context=staging get pods | grep snap | grep start
 2690  kubectl --context=staging get pods | grep snap | grep importer
 2691  kubectl --context=staging get pods | grep snap | grep importer | wc -l
 2692  kubectl --context=staging get pods | grep snap 
 2693  kubectl --context=staging get deployment | grep snap
 2694  kubectl --context=staging delete deployment snapshot-pro-api snapshot-pro-api-permissions
 2695  kubectl --context=staging get deployment | grep snap
 2696  kubectl --context=staging get pods | grep snap 
 2697  kubectl --context=staging get pods | grep snap | wc -l
 2698  kubectl --context=staging get pods | grep snap 
 2699  kubectl --context=staging get pods | grep snap | wc -l
 2700  helmfile -e oracle-staging -f helmfile/services/snapshot-pro.yaml -l service=snapshotProApi apply
 2701  helmfile -e oracle-staging -f helmfile/services/snapshot-pro.yaml -l service=snapshotProApi delete
 2702  helmfile -e oracle-staging -f helmfile/services/snapshot-pro.yaml -l service=snapshotProApi apply
 2703  helmfile -e oracle-staging -f helmfile/services/snapshot-pro.yaml -l service=snapshotProApi delete
 2704  helmfile -e oracle-staging -f helmfile/services/snapshot-pro.yaml -l service=snapshotProApi apply
 2705  helmfile -e oracle-staging -f helmfile/services/snapshot-pro.yaml -l service=snapshotProUi apply
 2706  helmfile -f helmfile/services/license.yaml -e oracle-production apply -i
 2707  cd helmfile/
 2708  helmfile -f helmfile/services/license.yaml -e oracle-production apply -i
 2709  cd ..
 2710  ll
 2711  mv oracle-produiction.yaml oracle-production.yaml 
 2712  helmfile -f helmfile/services/license.yaml -e oracle-production apply -i
 2713  git status
 2714  less charts/tiles-api/values.yaml 
 2715  helmfile -f helmfile/services/license.yaml -e oracle-production apply -i
 2716  ./dgctl-license.sh prod
 2717  helmfile -f helmfile/services/license.yaml -e oracle-production apply -i
 2718  cp oracle-staging.yaml oracle-production.yaml 
 2719  vim oracle-production.yaml 
 2720  helmfile -f helmfile/services/haproxy.yaml -e oracle-production apply -i
 2721  vim oracle-production.yaml 
 2722  pwd
 2723  cd ../ingress-controller/
 2724  ll
 2725  cp oracle-staging.yaml oracle-production.yaml 
 2726  vim oracle-production.yaml 
 2727  cd ..
 2728  helmfile -f helmfile/services/ingress-controller.yaml -e oracle-production apply -i
 2729  vim helmfile/values/keys/oracle-production.yaml 
 2730  less dgctl/manual/prod-manifest 
 2731  vim helmfile/values/keys/oracle-production.yaml 
 2732  cd ..
 2733  ll
 2734  cd on-premise-helm-charts/
 2735  ll
 2736  git status
 2737  git diff
 2738  git status
 2739  git add .
 2740  git status
 2741  git checkout -b add_tolerations
 2742  git add .
 2743  git status
 2744  git commit -m "Added tolerations"
 2745  git config user.email "sheikin.igor@gmail.com"
 2746  git config user.name "Igor Sheykin"
 2747  git commit -m "Added tolerations"
 2748  git checkout 1.32.0
 2749  git status
 2750  kubectl get pvc
 2751  kubectl --context=staging get pvc
 2752  kubectl --context=production get pvc
 2753  kubectl --context=production get pv
 2754  kubectl --context=staging get pv
 2755  kubectl --context=staging get storageClass
 2756  kubectl --context=production get storageClass
 2757  cd
 2758  kubectl --context=production get storageClass
 2759  kubectl --context=staging get storageClass
 2760  kubectl --context=staging get pv
 2761  kubectl --context=production get pv
 2762  cd /opt/on-premise/momrah/ansible/inventory/oracle-production/group_vars/patroni/
 2763  vim vars.yml 
 2764  cd /opt/on-premise/
 2765  grep -ril postgres_configure_users
 2766  cd ansible-on-premise/roles/postgres_configure/
 2767  ll
 2768  vim defaults/main.yml 
 2769  vim README.md 
 2770  vim tasks/main.yml 
 2771  cd ..
 2772  grep -ril postgres_configure
 2773  grep -ri postgres_configure
 2774  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/patroni-configure.yml 
 2775  source venv/bin/activate
 2776  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/patroni-configure.yml 
 2777  vim roles/postgres_configure/tasks/main.yml 
 2778  vim playbooks/patroni-configure.yml 
 2779  grep -ril psycopg2
 2780  vim roles/postgres_configure/tasks/main.yml 
 2781  grep -ri postgres_configure
 2782  vim ../momrah/ansible/inventory/oracle-production/group_vars/patroni/vars.yml 
 2783  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/patroni-configure.yml 
 2784  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/patroni-configure.yml -l prod-postgresql-1
 2785  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/patroni-configure.yml -l prod-postgresql-1 -vv
 2786  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/patroni-configure.yml
 2787  mc -b
 2788  grep -ril postgres-devel
 2789  grep -ril postgresql-devel
 2790  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-production playbooks/patroni-configure.yml
 2791  curl -v https://keys-urbi.momrah.gov.sa
 2792  curl -v https://map-urbi.momrah.gov.sa
 2793  curl -k -v https://map-urbi.momrah.gov.sa
 2794  cat helmfile/values/ingress-controller/common.gotmpl 
 2795  nano helmfile/values/ingress-controller/common.gotmpl 
 2796  helmfile -e oracle-staging --namespace urbi -f helmfile/services/ingress-controller.yaml diff --context 2
 2797  nano helmfile/values/ingress-controller/common.gotmpl 
 2798  helmfile -e oracle-staging --namespace urbi -f helmfile/services/ingress-controller.yaml diff --context 2
 2799  helmfile -e oracle-staging --namespace urbi -f helmfile/services/ingress-controller.yaml apply
 2800  helmfile -e oracle-production --namespace urbi -f helmfile/services/ingress-controller.yaml diff
 2801  helmfile -e oracle-production --namespace urbi -f helmfile/services/ingress-controller.yaml diff --context 2
 2802  helmfile -e oracle-production --namespace urbi -f helmfile/services/ingress-controller.yaml apply
 2803  curl -v https://map-urbi.momrah.gov.sa
 2804  curl -v https://map-urbi-prod.momrah.gov.sa
 2805  curl --resolve map-urbi-prod:10.246.8.88
 2806  curl --resolve map-urbi-prod:10.246.8.88  -v https://map-urbi-prod.momrah.gov.sa
 2807  curl --connect-to 10.246.8.88  -v https://map-urbi-prod.momrah.gov.sa
 2808  curl --resolve map-urbi-prod:443:10.246.8.88  -v https://map-urbi-prod.momrah.gov.sa
 2809  curl --resolve map-urbi-prod.momrah.gov.sa:443:10.246.8.88  -v https://map-urbi-prod.momrah.gov.sa
 2810  kubectl --context=staging -n urbi get pods 
 2811  helmfile -e oracle-staging --namespace urbi -f helmfile/services/ingress-controller.yaml apply
 2812  kubectl --context=staging -n urbi get ingress
 2813  nano helmfile/values/pro-api/oracle-staging.yaml 
 2814  helmfile -e oracle-staging -f helmfile/services/pro-api.yaml apply
 2815  helmfile -f helmfile/services/navi/back-custom.yaml -e oracle-staging diff
 2816  nano helmfile/values/navi/back/oracle-staging-ctx.yaml 
 2817  helmfile -f helmfile/services/navi/back-custom.yaml -e oracle-staging diff
 2818  helmfile -f helmfile/services/navi/back-custom.yaml -e oracle-staging apply
 2819  kubectl --context=staging -n urbi get pods
 2820  helmfile -e oracle-staging -f helmfile/services/ingress-controller.yaml apply
 2821  nano helmfile/values/twins-api/oracle-staging.yaml 
 2822  helmfile -f helmfile/services/twins-api.yaml -e oracle-staging d
 2823  helmfile -f helmfile/services/twins-api.yaml -e oracle-staging apply
 2824  nano helmfile/values/mapgl/oracle-
 2825  nano helmfile/values/mapgl/oracle-staging.yaml 
 2826  helmfile -f helmfile/services/mapgl.yam -e oracle-staging appl
 2827  helmfile -f helmfile/services/mapgl.yaml -e oracle-staging appl
 2828  helmfile -f helmfile/services/mapgl.yaml -e oracle-staging apply
 2829  kubectl --context=staging -n urbi get ingress
 2830  curl --resolve urbi-lifestyle-oci-stg.momrah.gov.sa:443:10.247.9.92 -v https://urbi-lifestyle-oci-stg.momrah.gov.sa/api.js
 2831  curl --resolve urbi-lifestyle-oci-stg.momrah.gov.sa:443:10.247.9.92 -v https://urbi-lifestyle-oci-stg.momrah.gov.sa/api.js -k
 2832  curl --resolve urbi-lifestyle-oci-stg.momrah.gov.sa:443:10.247.9.92 -v https://urbi-lifestyle-oci-stg.momrah.gov.sa/api.js -k | head
 2833  kubectl --context=staging -n urbi get ingress
 2834  kubectl --context=staging -n urbi get ingress be-catalog-api                    urbi    urbi-lifestyle-oci-stg.momrah.gov.sa                 10.247.9.92   80        9h
 2835  kubectl --context=staging -n urbi get ingress be-mapgl-js-api -o yamlk
 2836  kubectl --context=staging -n urbi get ingress be-mapgl-js-api -o yaml
 2837  kubectl --context=staging -n urbi увше ingress be-mapgl-js-api 
 2838  kubectl --context=staging -n urbi editingress be-mapgl-js-api 
 2839  kubectl --context=staging -n urbi edit ingress be-mapgl-js-api 
 2840  EDITOR=nano kubectl --context=staging -n urbi edit ingress be-mapgl-js-api 
 2841  kubectl --context=staging -n urbi get pods
 2842  kubectl --context=staging -n urbi logs urbi-ingress-ingress-nginx-controller-85f7d76c59-z4cpj -f
 2843  curl --resolve urbi-lifestyle-oci-stg.momrah.gov.sa:443:10.247.9.92 -v https://urbi-lifestyle-oci-stg.momrah.gov.sa/api.js -k | head
 2844  curl --resolve map-urbi.momrah.gov.sa:443:10.247.9.92 -v https://map-urbi.momrah.gov.sa/api.js -k | head
 2845  kubectl --context=staging -n urbi get ingress be-mapgl-js-api -o yaml
 2846  kubectl --context=staging -n urbi get ingress be-mapgl-js-api 
 2847  kubectl --context=staging -n urbi get ingress 
 2848  kubectl --context=staging -n urbi get ingress mapgl-js-api
 2849  kubectl --context=staging -n urbi get ingress mapgl-js-api -o yaml
 2850  kubectl --context=staging -n urbi get ingress be-mapgl-js-api -o yaml
 2851  kubectl --context=staging -n urbi edit ingress be-mapgl-js-api -o yaml
 2852  EDITOR=nano kubectl --context=staging -n urbi edit ingress be-mapgl-js-api 
 2853  curl --resolve urbi-lifestyle-oci-stg.momrah.gov.sa:443:10.247.9.92 -v https://urbi-lifestyle-oci-stg.momrah.gov.sa/api.js -k | head
 2854  EDITOR=nano kubectl --context=staging -n urbi edit ingress be-mapgl-js-api 
 2855  EDITOR=nano kubectl --context=staging -n urbi get ingress 
 2856  cd
 2857  mc
 2858  kubectl --context=staging -n urbi logs urbi-ingress-ingress-nginx-controller-85f7d76c59-z4cpj -f
 2859  curl '/3.0/items?key=ab52daf5-0c3c-4b0e-8ea6-97b7fc80fff8&type=adm_div%2Cadm_div.city%2Cadm_div.country%2Cadm_div.district%2Cadm_div.district_area%2Cadm_div.division%2Cadm_div.living_area%
 2860  2Cadm_div.place%2Cadm_div.region%2Cadm_div.settlement%2Cattraction%2Cbranch%2Cbuilding%2Ccoordinates%2Ccrossroad%2Cgate%2Ckilometer_road_sign%2Cparking%2Croad%2Croute%2Cstation%2Cstation_entrance%2Cstation.metro%2Cstation_platform%2Cstre
 2861  et&fields=items.access%2Citems.address%2Citems.address.components.fias_code%2Citems.adm_div%2Citems.attraction%2Citems.attribute_groups%2Citems.capacity%2Citems.contact_groups%2Citems.context%2Citems.description%2Citems.ev_charging_stati
 2862  on%2Citems.fias_code%2Citems.floors%2Citems.floor_id%2Citems.floor_plans%2Citems.for_trucks%2Citems.group%2Citems.is_incentive%2Citems.is_main_in_group%2Citems.is_paid%2Citems.locale%2Citems.level_count%2Citems.links%2Citems.links.databa
 2863  se_entrances.apartments_info%2Citems.marker_alt%2Citems.name_ex%2Citems.org%2Citems.paving_type%2Citems.point%2Citems.purpose%2Citems.purpose_code%2Citems.purpose_name%2Citems.region_id%2Citems.reviews%2Citems.rubrics%2Citems.schedule%2C
 2864  EDITOR=nano kubectl --context=staging -n urbi get ingress 
 2865  curl 'https://urbi-lifestyle-oci-stg.momrah.gov.sa/3.0/items?key=ab52daf5-0c3c-4b0e-8ea6-97b7fc80fff8&type=adm_div%2Cadm_div.city%2Cadm_div.country%2Cadm_div.district%2Cadm_div.district_area%2Cadm_div.division%2Cadm_div.living_area%2Cadm_div.place%2Cadm_div.region%2Cadm_div.settlement%2Cattraction%2Cbranch%2Cbuilding%2Ccoordinates%2Ccrossroad%2Cgate%2Ckilometer_road_sign%2Cparking%2Croad%2Croute%2Cstation%2Cstation_entrance%2Cstation.metro%2Cstation_platform%2Cstreet&fields=items.access%2Citems.address%2Citems.address.components.fias_code%2Citems.adm_div%2Citems.attraction%2Citems.attribute_groups%2Citems.capacity%2Citems.contact_groups%2Citems.context%2Citems.description%2Citems.ev_charging_station%2Citems.fias_code%2Citems.floors%2Citems.floor_id%2Citems.floor_plans%2Citems.for_trucks%2Citems.group%2Citems.is_incentive%2Citems.is_main_in_group%2Citems.is_paid%2Citems.locale%2Citems.level_count%2Citems.links%2Citems.links.database_entrances.apartments_info%2Citems.marker_alt%2Citems.name_ex%2Citems.org%2Citems.paving_type%2Citems.point%2Citems.purpose%2Citems.purpose_code%2Citems.purpose_name%2Citems.region_id%2Citems.reviews%2Citems.rubrics%2Citems.schedule%2Citems.search_attributes.dgis_address_details%2Citems.segment_id%2Citems.stop_factors%2Citems.timezone%2Citems.trade_license%2Crequest_type%2Csearch_attributes%2Cfilters%2Cwidgets&location=46.668593%2C24.710738&id=70000001085211197&locale=ar_SA 200","level":"INFO","method":"GET","request_id":"3e5d81f77bbb586e86dad89b52e267e2","request_uri":"/3.0/items?key=ab52daf5-0c3c-4b0e-8ea6-97b7fc80fff8&type=adm_div%2Cadm_div.city%2Cadm_div.country%2Cadm_div.district%2Cadm_div.district_area%2Cadm_div.division%2Cadm_div.living_area%2Cadm_div.place%2Cadm_div.region%2Cadm_div.settlement%2Cattraction%2Cbranch%2Cbuilding%2Ccoordinates%2Ccrossroad%2Cgate%2Ckilometer_road_sign%2Cparking%2Croad%2Croute%2Cstation%2Cstation_entrance%2Cstation.metro%2Cstation_platform%2Cstreet&fields=items.access%2Citems.address%2Citems.address.components.fias_code%2Citems.adm_div%2Citems.attraction%2Citems.attribute_groups%2Citems.capacity%2Citems.contact_groups%2Citems.context%2Citems.description%2Citems.ev_charging_station%2Citems.fias_code%2Citems.floors%2Citems.floor_id%2Citems.floor_plans%2Citems.for_trucks%2Citems.group%2Citems.is_incentive%2Citems.is_main_in_group%2Citems.is_paid%2Citems.locale%2Citems.level_count%2Citems.links%2Citems.links.database_entrances.apartments_info%2Citems.marker_alt%2Citems.name_ex%2Citems.org%2Citems.paving_type%2Citems.point%2Citems.purpose%2Citems.purpose_code%2Citems.purpose_name%2Citems.region_id%2Citems.reviews%2Citems.rubrics%2Citems.schedule%2Citems.search_attributes.dgis_address_details%2Citems.segment_id%2Citems.stop_factors%2Citems.timezone%2Citems.trade_license%2Crequest_type%2Csearch_attributes%2Cfilters%2Cwidgets&location=46.668593%2C24.710738&id=70000001085211197&locale=ar_SA'
 2866  cd /opt/on-premise/
 2867  helmfile -f helmfile/services/license.yaml -e oracle-production diff
 2868  cd momrah/
 2869  helmfile -f helmfile/services/license.yaml -e oracle-production diff
 2870  helmfile -e oracle-staging -f helmfile/services/ingress-controller.yaml diff
 2871  helmfile -e oracle-staging -f helmfile/services/monitoring.yaml diff
 2872  helmfile -e oracle-staging -f helmfile/services/monitoring.yaml apply
 2873  helmfile -e oracle-production -f helmfile/services/monitoring.yaml apply
 2874  mc
 2875  kubectl --context=production -n urbi get pods
 2876  kubectl --context=production -n logs pro-api-7db79b8d67-nx9sl
 2877  kubectl --context=staging -n urbi get secret 
 2878  kubectl --context=staging -n urbi get secret  | grep ingress
 2879  cd /opt/
 2880  ls
 2881  cd on-premise/
 2882  ls
 2883  nano ingress-cert.yaml 
 2884  kubectl --context=staging -n urbi delete -f ingress-cert.yaml
 2885  nano 
 2886  kubectl --context=staging -n urbi apply -f ingress-cert.yaml
 2887  kubectl --context=production -n urbi get pods
 2888  kubectl --context=staging -n urbi get pods | gerp nginx
 2889  kubectl --context=staging -n urbi get pods | grep nginx
 2890  kubectl --context=staging -n urbi describe pod urbi-ingress-ingress-nginx-admission-create-j76wz
 2891  kubectl --context=staging -n urbi get pods | grep nginx
 2892  kubectl --context=duction -n urbi get svc | grep nginx
 2893  kubectl --context=production -n urbi get svc | grep nginx
 2894  kubectl --context=staging -n urbi get pods | grep nginx
 2895  kubectl --context=staging -n urbi describe pod urbi-ingress-ingress-nginx-admission-create-vl5jb
 2896  kubectl --context=staging -n urbi get pods | grep nginx
 2897  kubectl --context=staging -n urbi describe pod urbi-ingress-ingress-nginx-admission-create-d7m2t
 2898  kubectl --context=staging -n urbi get pods | grep nginx
 2899  kubectl --context=staging -n urbi get ingress
 2900  kubectl --context=staging -n urbi get pods
 2901  kubectl --context=staging -n urbi get pods | grep mapg
 2902  kubectl --context=staging -n urbi describe pod mapgl-js-api-f5f6bc459-2fnq7
 2903  kubectl --context=staging -n urbi get pods | grep mapg
 2904  kubectl --context=staging -n urbi describe pod mapgl-js-api-f5f6bc459-2fnq7
 2905  kubectl --context=staging -n urbi get pods | grep mapg
 2906  kubectl --context=staging -n urbi describe pod mapgl-js-api-f5f6bc459-2fnq7
 2907  kubectl --context=staging -n urbi get pods | grep mapg
 2908  kubectl --context=staging -n urbi get ingress
 2909  kubectl --context=staging -n urbi get ingress mapgl-js-api -o yamlk
 2910  kubectl --context=staging -n urbi get ingress mapgl-js-api -o yaml
 2911  sudo nano /etc/motd.d/on-premise 
 2912  kubectl --context=staging -n urbi get pods | grep catalog
 2913  kubectl --context=staging -n urbi logs -f catalog-api-6d4bfc97f8-q8d46
 2914  kubectl --context=staging -n urbi get pods 
 2915  kubectl --context=staging -n urbi get pods | grep monito
 2916  kubectl --context=staging -n urbi get pods | grep prom
 2917  kubectl --context=production -n urbi get pods | grep prom
 2918  kubectl --context=production -n urbi get pods | grep urbi
 2919  kubectl --context=production -n urbi describe pod urbi-gf-grafana-77bc79b6dc-gd8qc
 2920  kubectl --context=production -n urbi get pvc
 2921  kubectl --context=production -n urbi get storageclasses
 2922  kubectl --context=production -n urbi get pvc
 2923  kubectl --context=production -n urbi get storageclass
 2924  kubectl --context=staging -n urbi get storageclass
 2925  kubectl --context=staging -n urbi get pwvc
 2926  kubectl --context=staging -n urbi get pvc
 2927  kubectl --context=staging -n urbi get pwv
 2928  kubectl --context=staging -n urbi get pv
 2929  kubectl --context=staging -n urbi get storageclass
 2930  kubectl --context=staging -n urbi get pvc
 2931  kubectl --context=staging -n urbi get storageclass oci-block-volume -o yaml
 2932  cat momrah/ansible/inventory/oracle-production/oracle.yml 
 2933   ssh -i ~/.ssh/keys/production.key admin@10.246.2.7
 2934  2;55;9m2;55;9M34;55;10M34;56;10M2;56;10m0;111;40M0;111;40m64;112;40M64;112;40M64;112;40M64;112;40M65;112;40M65;112;40M65;112;40M
 2935  CD
 2936  tmux a -t is
 2937  tmux ls
 2938  uptime
 2939  w
 2940  tmux ls
 2941  uptime
 2942  tmux ls
 2943  tmux a -t is
 2944   history 
 2945   tmux attach -t r9odt
 2946  history | grep tmux
 2947  history | grep kill
 2948  last | head
 2949   tmux new -s r9odt
 2950   tmux attach -t r9odt
 2951  clear
 2952  kubectl --context=staging -n urbi get deployments
 2953  history
 2954  kubectl --context=staging -n urbi get deployments
 2955  kubectl get nodes
 2956  kubectl cluster-info
 2957  kubectl config get-contexts
 2958  kubectl --context=staging
 2959  kubectl --context=staging -n urbi get deployements
 2960  kubectl --context=staging -n urbi get deployments
 2961  kubectl --context=staging
 2962  kubectl --context=staging get pods
 2963  kubectl describe pod tiles-api-6c7b9449b7-44vct
 2964  kubectl --contex=staging -n urbi get deployments
 2965  kubectl --context=staging -n urbi get deployments
 2966   tmux attach -t is
 2967  ls
 2968  ll
 2969  kubectl --context=staging -n urbi get deployments
 2970  show Kubectl get nodes -o wide
 2971  Kubectl get nodes -o wide
 2972  kubectl get nodes
 2973  clear
 2974  ls
 2975  ll
 2976  Kubectl get pods
 2977  kubeclt get pods
 2978  ls
 2979  clear
 2980  kubectl --context=staging -n urbi get deployments
 2981  Kubectl get nodes -o wide
 2982  Kubectl describe deployment search-api
 2983  kubectl --context=staging -n urbi get deployments
 2984  kubectl --context=staging -n urbi
 2985  kubectl --context=staging -n urbi cluster-info
 2986  kubectl --context=staging -n urbi get pods
 2987  ls
 2988  cd /opt/on-premise/oci-stg-ingress/
 2989  ls
 2990  nano be-navi-api.yaml 
 2991  kubectl --context=staging get ingress
 2992  kubectl --context=staging get ingress navi-back-ctx -o yaml
 2993  tmux a -t is
 2994   tmuxa ttach -t r9odt
 2995   tmux attach -t r9odt
 2996  kubectl --context=staging -n urbi get ingress 
 2997  clear
 2998  ls
 2999  kubectl --context=staging get pods | grep snapshot
 3000  kubectl --context=staging -n urbi logs -f snapshot-pro-api-5d5c87989-99qt4
 3001  clear
 3002  kubectl --context=staging -n urbi get deployments
 3003  tmux a -t is
 3004  clear
 3005  ls
 3006  clear
 3007  kubectl --context=staging -n urbi get secret
 3008  kubectl --context=staging -n urbi get secret | grep snapshot
 3009  kubectl --context=staging -n urbi get secret snapshot-pro-api-secret -o yaml
 3010  echo "YWI1MmRhZjUtMGMzYy00YjBlLThlYTYtOTdiN2ZjODBmZmY4" | base64 --decode
 3011  clear
 3012  kubectl --context=staging -n urbi get secret snapshot-pro-api-secret -o yaml
 3013  history | grep elastic
 3014  ssh -i ~/.ssh/keys/staging.key admin@stg-elasticsearch-1
 3015  clear
 3016  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-1
 3017  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-2
 3018  s3cmd -c ~/.s3cfg/me-jeddah-1-oci get s3://momrah-data/roles.sql ./
 3019  scp -i ~/.ssh/keys/production.key roles.sql  admin@prod-postgresql-2:
 3020  ssh -i ~/.ssh/keys/production.key admin@prod-postgresql-2
 3021  tmux a -y is
 3022  tmux a -t is
 3023  ls
 3024  vpn
 3025  ls
 3026  docker ps
 3027  docker images
 3028   tmux attach -t r9odt
 3029  clear
 3030  ls
 3031  kubectl --context=staging -n airflow get pods
 3032  kubectl --context=staging -n airflow get pods
 3033  kubectl --context=staging -n airflow get pods -o wide
 3034  kubectl describ pod airflow-webserver-79cbf65f76-f5sgx
 3035  kubectl describe pod airflow-webserver-79cbf65f76-f5sgx
 3036  kubectl exec -it airflow-webserver-79cbf65f76-f5sgx bash
 3037  kubectl --context=staging -n airflow exec -it airflow-webserver-79cbf65f76-f5sgx bash
 3038  clear
 3039  kubectl --context=staging -n airflow get pods
 3040  ssh -i ~/.ssh/keys/production.key admin@prod-elasticsearch-1
 3041   tmux attach -t r9odt
 3042  ls
 3043  kubectl --context=staging -n urbi get deployments
 3044  kubectl --context=staging -n airflow get pods
 3045  kubectl exec -it airflow-webserver-595b66cbc4-v4n58  -- /bin/bash
 3046  kubectl exec -it airflow-webserver-595b66cbc4-v4n58 /bin/bash
 3047  ls
 3048  kubectl --context=staging -n airflow get pods
 3049  kubectl --context=staging -n airflow describe pods
 3050  kubectl --context=staging -n airflow get pods
 3051  kubectl exec -it airflow-webserver-595b66cbc4-v4n58 /bin/bash
 3052  kubectl exec -it airflow-webserver-595b66cbc4-v4n58 --/bin/bash
 3053  kubectl exec -it airflow-webserver-595b66cbc4-v4n58 -- /bin/bash
 3054  clear
 3055  ls
 3056  history | grep elastic
 3057  ssh -i ~/.ssh/keys/staging.key admin@stg-elasticsearch-1
 3058  clear
 3059  cd omar/wedo/upliftment/upliftment-ui/chart/
 3060  ls
 3061  nano deployment.yaml 
 3062  exit
 3063  clear
 3064  ls
 3065  cd omar/
 3066  cd wedo/upliftment/upliftment-ui/chart/
 3067  ls
 3068  nano service.yaml 
 3069  ls
 3070  nano deployment.yaml 
 3071  cp ../../../dqp/dqp-ui/chart/ingress.yaml .
 3072  clear
 3073  ls
 3074  nano ingress.yaml 
 3075  cat /opt/on-premise/oci-stg-ingress/be-catalog-api.yaml 
 3076  clear
 3077  ls
 3078  cd omar/
 3079  ls
 3080  cd wedo/upliftment/upliftment-ui/chart/
 3081  ls
 3082  nano ingress.yaml 
 3083  cat /opt/on-premise/oci-stg-ingress/be-catalog-api.yaml 
 3084  nano ingress.yaml 
 3085  clear
 3086  ls
 3087  kubectl --context=staging -n urbi apply -f deployment.yaml 
 3088  kubectl --context=staging -n urbi apply -f ingress.yaml 
 3089  nano ingress.yaml 
 3090  kubectl --context=staging -n urbi apply -f ingress.yaml 
 3091  nano ../../../dqp/dqp-ui/chart/ingress.yaml 
 3092  kubectl --context=staging -n urbi apply -f  ../../../dqp/dqp-ui/chart/ingress.yaml 
 3093  ls
 3094  kubectl --context=staging -n urbi apply -f  ingress.yaml 
 3095  kubectl --context=staging -n urbi apply -f service.yaml 
 3096  clear
 3097  kubectl --context=staging -n urbi get pods | grep dqp
 3098  kubectl --context=staging -n urbi get pods | grep upliftment
 3099  ls
 3100  cd 
 3101  cd /opt/on-premise/oci-stg-ingress/
 3102  ls
 3103  cat be-balady-discrepancy.yaml 
 3104  nano be-balady-discrepancy.yaml 
 3105  ls
 3106  kubectl --context=staging apply -f be-balady-discrepancy.yaml 
 3107  clear
 3108  ls
 3109  kubectl --context=staging -n urbi get ingress
 3110  kubectl --context=staging -n urbi get ingress | grep disc
 3111  kubectl --context=staging -n urbi get ingress be-balady-discrepancy-oci -o yaml
 3112  kubectl --context=staging -n urbi get ingress be-balady-discrepancy -o yaml
 3113  clear
 3114  ls
 3115  nano be-balady-discrepancy.yaml 
 3116  ls
 3117  kubectl --context=staging -n urbi get ingress be-balady-discrepancy-oci -o yaml
 3118  kubectl --context=staging -n urbi get ingress be-balady-discrepancy -o yaml
 3119  clear
 3120  cd 
 3121  cd omar/wedo/dqp/dqp-ui/chart/
 3122  ls
 3123  nano ingress.yaml 
 3124  kubectl --context=staging apply -f ingress.yaml 
 3125  cd ..
 3126  ls
 3127  cd upliftment/
 3128  cd upliftment-ui/
 3129  ls
 3130  cd chart/
 3131  ls
 3132  nano ingress.yaml 
 3133  kubectl --context=staging apply -f ingress.yaml
 3134  clear
 3135  ls
 3136  nano service.yaml 
 3137  nano deployment.yaml 
 3138  nano ingress.yaml 
 3139  kubectl --context=staging get ingress | dqp
 3140  kubectl --context=staging get ingress | grep dqp
 3141  kubectl --context=staging get ingress be-dqp-ui -o yaml
 3142  kubectl --context=staging get ingress | gre
 3143  kubectl --context=staging get ingress be-upliftment-ui -o yaml
 3144  clear
 3145  exit
 3146  kubectl --context=staging get pods | grep dqp 
 3147  kubectl --context=staging delete pod dqp-closures-api-5745f7d86d-66vt8
 3148  kubectl --context=staging get pods | grep dqp 
 3149  ls
 3150  clear
 3151  exit
 3152   history
 3153  kubectl get po
 3154  ll
 3155  ll /home/
 3156  kubectl --context=staging -n urbi get pods
 3157  k9s
 3158  cd /opt/on-premise/momrah/helmfile/
 3159  ll
 3160  helm ls
 3161  cat ~/.kube/config 
 3162  printenv 
 3163   cat /opt/on-premise/admin-sa-kubeconfig.yml
 3164  helm --kube-context staging ls
 3165   nano ~/.ssh/authorized_keys
 3166  clear
 3167  ls
 3168  kubectl --context=staging -n urbi get ingress
 3169  kubectl --context=staging -n urbi get ingress | grep city
 3170  kubectl --context=staging -n urbi get pods | grep city
 3171  clear
 3172  ls
 3173  kubectl --context=staging -n urbi get deployments | grep citylens
 3174  k9s
 3175  ll
 3176  cd /opt/on-premise/momrah/dgctl/
 3177  ll
 3178  nano job-stage-data-apps-ksa.yaml 
 3179  kubectl --context staging apply -f job-stage-data-apps-ksa.yaml 
 3180  nano job-stage-data-apps-ksa.yaml 
 3181   k9s
 3182  nano job-stage-data-apps-ksa.yaml 
 3183  kubectl --context staging apply -f job-stage-data-apps-ksa.yaml 
 3184  k9s
 3185  nano job-stage-data-apps-ksa.yaml 
 3186  k9s
 3187  kubectl --context staging apply -f job-stage-data-apps-ksa.yaml 
 3188  k9s
 3189   history
 3190  ll
 3191  cd /opt/on-premise/momrah/dgctl/
 3192  ll
 3193  cat manual/dgctl.sh 
 3194  cd manual/
 3195   nano dgctl.sh 
 3196  ./dgctl.sh 
 3197  ./dgctl.sh stging
 3198   cat ../oracle/job-apps-only-ksa.yaml 
 3199   kubectl --context staging get cm
 3200  ll
 3201  cat dgctl-staging.yaml 
 3202   nano dgctl.sh 
 3203  ./dgctl.sh staging
 3204   history | grep dgctl
 3205  sudo ./dgctl.sh staging
 3206   git status
 3207  cd ../../helmfile/
 3208  nano values/testing-common.yaml 
 3209  nano values/oracle-staging-common.yaml 
 3210  nano values/keys/oracle-staging.yaml 
 3211  cat common.yaml 
 3212  helmfile -e oracle-staging -f services/keys.yaml --context=1 diff
 3213  cat values/keys/oracle-staging.yaml 
 3214  nano ~/.ssh/authorized_keys
 3215  exit
 3216  cd /opt/on-premise/momrah/helmfile/
 3217  helmfile -e oracle-staging -f services/keys.yaml --context=1 diff
 3218  helmfile -e oracle-staging -f services/keys.yaml --context=1 apply
 3219  k9s
 3220  nano values/platform/oracle-staging.yaml
 3221  nano values/pro-api/oracle-staging.yaml 
 3222  helmfile -e oracle-staging -f services/platform.yaml --context=1 apply
 3223   cat /etc/hosts
 3224  nano values/citylens/oracle-staging.yaml 
 3225  nano values/oracle-staging-common.yaml 
 3226  helmfile -e oracle-staging -f services/citylens.yaml --context=1 diff
 3227  nano values/oracle-staging-common.yaml 
 3228  nano values/citylens/oracle-staging.yaml 
 3229  helmfile -e oracle-staging -f services/citylens.yaml --context=1 diff
 3230  nano common.yaml 
 3231  helmfile -e oracle-staging -f services/citylens.yaml --context=1 diff
 3232  helmfile -e oracle-staging -f services/citylens.yaml --context=1 apply
 3233  k9s
 3234  nano services/citylens-routes-ui.yaml
 3235  mkdir -p values/citylens-routes-ui/oracle-staging.yaml
 3236  nano values/citylens-routes-ui/oracle-staging.yaml
 3237  rm -rf values/citylens-routes-ui/oracle-staging.yaml/
 3238  nano values/citylens-routes-ui/oracle-staging.yaml
 3239  cat services/citylens-routes-ui.yaml 
 3240  cat values/citylens-routes-ui/oracle-staging.yaml 
 3241  helmfile -e oracle-staging -f services/citylens-routes-ui.yaml --context=1 diff
 3242  helmfile -e oracle-staging -f services/citylens-routes-ui.yaml --context=1 apply
 3243  k9s
 3244  cp -r oci-stg-ingress stg-ingress
 3245  kubectl --context=staging -n urbi apply -f .
 3246  cd ../oci-stg-ingress/
 3247  kubectl --context=staging -n urbi apply -f .
 3248  kubectl --context=staging get ingress 
 3249  kubectl --context=staging -n urbi apply -f .
 3250  kubectl --context=staging get ingress 
 3251  kubectl --context=staging -n urbi logs -f urbi-ingress-ingress-nginx-controller-5c785b9ff-qkw2c 
 3252  kubectl --context=staging get ingress 
 3253  cd /opt/on-premise/momrah/helmfile/
 3254  nano values/citylens/oracle-staging.yaml 
 3255  nano values/citylens-routes-ui/oracle-staging.yaml 
 3256  cat values/citylens/oracle-staging.yaml 
 3257  sudo -i
 3258   tmux attach -t r9odt
 3259  curl https://keys-urbi.momrah.gov.sa
 3260   k9s
 3261  nano /etc/hosts 
 3262  sudo nano /etc/hosts 
 3263  curl https://keys-urbi.momrah.gov.sa
 3264  k9s
 3265  exit
 3266  helmfile -e oracle-staging -f helmfile/services/snapshot-pro.yaml -l service=snapshotProApi apply
 3267  helmfile -e oracle-staging -f helmfile/services/snapshot-pro.yaml -l service=snapshotProApi apply cd "`printf "%b" '\0057opt\0057on\0055premise\0057momrah'`"
 3268  helmfile -f helmfile/services/citylens.yaml -e oracle-staging apply
 3269  helmfile -f helmfile/services/haproxy.yaml -e oracle-production apply -i
 3270  helmfile -f helmfile/services/haproxy.yaml -e oracle-production apply 
 3271  helmfile -f helmfile/services/keys.yaml -e oracle-staging diff
 3272  kubectl --context=staging -n urbi get pods | grep discrepa
 3273  helmfile -e oracle-staging -f helmfile/services/monitoring.yaml diff
 3274  helmfile -e oracle-staging -f helmfile/services/monitoring.yaml apply
 3275   history
 3276   sudo nano /etc/hosts
 3277  cd /opt/on-premise/momrah/helmfile/
 3278  helmfile -e oracle-staging -f services/citylens.yaml --context=1 diff
 3279  helmfile -e oracle-staging -f services/citylens.yaml --context=1 apply
 3280  k9s
 3281  cat values/citylens-routes-ui/oracle-staging.yaml 
 3282   k9s
 3283  k9s
 3284  helmfile -e oracle-staging -f services/keys.yaml --context=1 diff
 3285  nano common.yaml 
 3286  helmfile -e oracle-staging -f services/keys.yaml --context=1 diff
 3287  nano values/keys/oracle-staging.yaml 
 3288  helmfile -e oracle-staging -f services/keys.yaml --context=1 diff
 3289  helmfile -e oracle-staging -f services/keys.yaml --context=1 apply
 3290  k9s
 3291  nano values/citylens-routes-ui/oracle-staging.yaml 
 3292  helmfile -e oracle-staging -f services/citylens-routes-ui.yaml --context=1 diff
 3293  nano common.yaml 
 3294  helmfile -e oracle-staging -f services/citylens-routes-ui.yaml --context=1 diff
 3295  helmfile -e oracle-staging -f services/citylens-routes-ui.yaml --context=1 apply
 3296  nano values/platform/oracle-staging.yaml 
 3297  helmfile -e oracle-staging -f services/platform.yaml --context=1 diff
 3298  helmfile -e oracle-staging -f services/platform.yaml --context=1 apply
 3299  k9s
 3300  sudo nano /etc/hosts
 3301   cat /etc/hosts
 3302  ssh stg-postgresql-1
 3303  ping 10.247.3.91
 3304  telnet 10.247.3.91 22
 3305  ssh admin@stg-postgresql-1
 3306  ll ~/.ssh/keys/
 3307  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-1
 3308  e
 3309   tmux attach -t r9odt
 3310  k9s
 3311  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-1
 3312  clear
 3313  ls
 3314  kubectl --context=staging -n urbi get ingress
 3315  kubectl --context=staging -n urbi get ingress be-balady-discrepancy -o yaml
 3316  cat /opt/on-premise/oci-stg-ingress/
 3317  cat /opt/on-premise/oci-stg-ingress/be-balady-discrepancy.yaml 
 3318  ls
 3319  cd /opt/on-premise/stg-ingress/
 3320  ls
 3321  cat be-balady-discrepancy.yaml 
 3322  nano be-balady-discrepancy-test.yaml
 3323  kubectl --context=staging -n urbi apply -f be-balady-discrepancy-test.yaml 
 3324  rm  be-balady-discrepancy-test.yaml 
 3325  rm be-balady-discrepancy.yaml 
 3326  nano be-balady-discrepancy.yaml 
 3327  ls
 3328  kubectl --context=staging -n urbi apply -f be-balady-discrepancy.yaml 
 3329  clear
 3330  ls
 3331  cat be-balady-discrepancy.yaml 
 3332  mc
 3333   tmux attach -t r9odt
 3334  clear
 3335  history | grep elastic
 3336  ssh -i ~/.ssh/keys/staging.key admin@stg-elasticsearch-1
 3337  k9s
 3338  ssh -i ~/.ssh/keys/staging.key admin@stg-elasticsearch-1
 3339  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-1
 3340  k9s
 3341  cat /etc/hosts
 3342  k9s
 3343  cd /opt/on-premise/momrah/helmfile/
 3344  cat values/citylens/oracle-staging.yaml 
 3345  nano values/pro-api/oracle-staging.yaml 
 3346  helmfile -e oracle-staging -f services/pro-api.yaml --context=1 diff
 3347  cd /opt/on-premise/momrah/helmfile/
 3348  nano common.yaml 
 3349  helmfile -e oracle-staging -f services/pro-api.yaml --context=1 diff
 3350  nano values/oracle-staging-common.yaml 
 3351  helmfile -e oracle-staging -f services/pro-api.yaml --context=1 diff
 3352  nano values/oracle-staging-common.yaml 
 3353  helmfile -e oracle-staging -f services/pro-api.yaml --context=1 diff
 3354  nano values/oracle-staging-common.yaml 
 3355  helmfile -e oracle-staging -f services/pro-api.yaml --context=1 diff
 3356  helmfile -e oracle-staging -f services/pro-api.yaml --context=1 apply
 3357  nano values/pro-api/oracle-staging.yaml 
 3358  helmfile -e oracle-staging -f services/pro-api.yaml --context=1 apply
 3359  k9s
 3360  cat values/pro-api/oracle-staging.yaml 
 3361  cat values/citylens/oracle-staging.yaml 
 3362  nano values/citylens/oracle-staging.yaml 
 3363  nano common.yaml 
 3364  k9s
 3365  nano common.yaml 
 3366  nano values/oracle-staging-common.yaml 
 3367  helmfile -e oracle-staging -f services/citylens.yaml --context=1 diff
 3368  helmfile -e oracle-staging -f services/citylens.yaml --context=1 apply
 3369  k9s
 3370  cd /opt/on-premise/momrah/helmfile/
 3371  cat values/haproxy/oracle-staging.yaml 
 3372   k9s
 3373  cd /opt/on-premise/momrah/helmfile/
 3374  helmfile -e oracle-staging -f services/citylens.yaml --context=1 apply
 3375  k9s
 3376  exit
 3377  k9s
 3378  cd /opt/on-premise/momrah/helmfile/
 3379  nano values/citylens/oracle-staging.yaml 
 3380  helmfile -e oracle-staging -f services/citylens.yaml --context=1 apply
 3381  k9s
 3382  helmfile -e oracle-staging -f services/citylens.yaml --context=1 apply
 3383  helmfile -e oracle-staging -f services/citylens.yaml sync
 3384  k9s
 3385  exit
 3386  history | grep elastic
 3387  ssh -i ~/.ssh/keys/staging.key admin@stg-elasticsearch-1
 3388  kubectl --context=staging -n urbi get deployments
 3389  cat balady-admin
 3390  kubectl --context=staging -n urbi get pods
 3391  kubectl exec -it balady-admin-f788bc4-jq999
 3392  kubectl exec -it balady-discrepancy-7978c9774d-j7rlz bash
 3393  clear
 3394  kubectl --context=staging -n urbi get deployment
 3395  kubectl --context=staging -n urbi get pod
 3396  clear
 3397  kubectl --context=staging get namespace
 3398  ls
 3399  kubectl --context=staging -n urbi get deployment
 3400  kubectl --context=staging -n urbi get pods
 3401  clear
 3402  history | grep elastic
 3403  ssh -i ~/.ssh/keys/staging.key admin@stg-elasticsearch-1
 3404  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-1
 3405  ls
 3406  kubectl --context=staging -n urbi get deployments
 3407  kubectl --context=staging -n urbi get pods
 3408  kubectl --context=staging -n uri describe pod balady-migration-87l6h
 3409  kubectl --context=staging -n urbi describe pod balady-migration-87l6h
 3410  kubectl --context=staging -n urbi describe pod pro-ui-799778b846-hb5fw
 3411  kubectl --context=staging -n urbi get pods
 3412  kubectl --context=staging -n urbi network ls
 3413  clear
 3414  ls
 3415  ll
 3416  docker ps
 3417  docker images
 3418  docker ps -a
 3419  ls
 3420  cd omar/
 3421  ls
 3422  cd airflow/
 3423  ls
 3424  cd ..
 3425  cd wedo/
 3426  ls
 3427  cd ..
 3428  ls
 3429  clear
 3430  exit
 3431  clear
 3432  kubectl --context=staging -n urbi get ingress | grep platform
 3433  kubectl --context=staging -n urbi get ingress platform -o yaml
 3434  kubectl --context=staging -n urbi get configmaps
 3435  kubectl --context=staging -n urbi get deployments
 3436  kubectl --context=staging -n urbi get deployments | grep platform
 3437  kubectl --context=staging -n urbi get deployment platform -o yaml
 3438  clear
 3439  ls
 3440  cd omar/wedo/
 3441  ls
 3442  mkdir maplocator-snapshot
 3443  cd maplocator-snapshot/
 3444  mkdir upload-service
 3445  cd upload-service/
 3446  mkdir chart
 3447  cd chart/
 3448  ls
 3449  nano deployment.yaml
 3450  kubectl --context=staging get pvc
 3451  kubectl --context=staging get pvc urbi-gf-grafana -o yaml
 3452  kubectl --context=staging get pv
 3453  kubectl --context=staging get sc
 3454  kubectl --context=staging get sc oci-block-volume -o yaml
 3455  ls
 3456  nano deployment.yaml
 3457  clear
 3458  cat deployment.yaml 
 3459  clear
 3460  kubectl --context=staging get sc oci-block-volume -o yaml
 3461  kubectl --context=staging get pvc 
 3462  kubectl --context=staging get pvc redis-data-redis-redis-cluster-0 -o yaml
 3463  rm deployment.yaml 
 3464  nano pvc.yaml
 3465  nano deployment.yaml
 3466  clear
 3467  ls
 3468  nano service.yaml
 3469  clear
 3470  ls
 3471  kubectl --context=staging get ingress be-catalog-api
 3472  kubectl --context=staging get ingress be-catalog-api -o yaml
 3473  nano ingress.yaml
 3474  clear
 3475  ls
 3476  nano deployment.yaml 
 3477  kubectl --context=staging -n urbi apply -f pvc.yaml 
 3478  kubectl --context=staging -n urbi get pvc
 3479  kubectl --context=staging -n urbi get pv
 3480  kubectl --context=staging -n urbi get pvc
 3481  kubectl --context=staging -n urbi describe pvc maplocator-storage-pvc
 3482  clear
 3483  kubectl --context=staging -n urbi apply -f pvc.yaml 
 3484  kubectl --context=staging -n urbi apply -f deployment.yaml 
 3485  nano service.yaml 
 3486  kubectl --context=staging -n urbi apply -f service.yaml 
 3487  kubectl --context=staging -n urbi apply -f ingress.yaml 
 3488  clear
 3489  kubectl --context=staging -n urbi get pvc
 3490  kubectl --context=staging -n urbi get pv
 3491  clear
 3492  ls
 3493  kubectl --context=staging -n urbi get ingress maplocator-snapshot
 3494  kubectl --context=staging -n urbi get ingress
 3495  cat ingress.yaml 
 3496  nano ingress.yaml 
 3497  clear
 3498  ls
 3499  kubectl --context=staging -n urbi get ingress
 3500  kubectl --context=staging -n urbi delete ingress maplocator-snapshot-ingress
 3501  kubectl --context=staging -n urbi apply -f ingress.yaml 
 3502  clear
 3503  ls
 3504  cat service.yaml 
 3505  kubectl --context=staging -n urbi get deployments maplocator-snapshot
 3506  kubectl --context=staging -n urbi get pods | grep  maplocator-snapshot
 3507  nano deployment.yaml 
 3508  clear
 3509  ls
 3510  kubectl --context=staging -n urbi get endpoints 
 3511  kubectl --context=staging -n urbi get endpoints | grep search
 3512  kubectl --context=staging -n urbi get ingress | grep search
 3513  kubectl --context=staging -n urbi get ingress | grep keys
 3514  ls
 3515  cat deployment.yaml 
 3516  ls
 3517  nano deployment.yaml 
 3518  kubectl --context=staging -n urbi apply -f deployment.yaml
 3519  kubectl --context=staging -n urbi get pods | grep maplocator
 3520  kubectl --context=staging -n urbi delete pod maplocator-snapshot-688464cf7b-6wr2j
 3521  kubectl --context=staging -n urbi delete pod maplocator-snapshot-6b87dd9d4f-rg6nl
 3522  ls
 3523  kubectl --context=staging -n urbi get pods | grep maplocator
 3524  kubectl --context=staging -n urbi delete deployment maplocator-snapshot
 3525  clear
 3526  kubectl --context=staging -n urbi get pods | grep maplocator
 3527  nano deployment.yaml 
 3528  kubectl --context=staging -n urbi apply -f deployment.yaml 
 3529  kubectl --context=staging -n urbi get pods | grep maplocator
 3530  kubectl --context=staging -n urbi describe pod maplocator-snapshot-6b87dd9d4f-sc6m8
 3531  kubectl --context=staging -n urbi get pods | grep maplocator
 3532  kubectl --context=staging -n urbi logs maplocator-snapshot-6b87dd9d4f-sc6m8
 3533  kubectl --context=staging -n urbi rollout restart deployment/maplocator-snapshot
 3534  kubectl --context=staging -n urbi get pods | grep maplocator
 3535  kubectl --context=staging -n urbi rollout restart deployment/maplocator-snapshot
 3536  kubectl --context=staging -n urbi delete deployment maplocator-snapshot
 3537  clear
 3538  kubectl --context=staging -n urbi get pods | grep maplocator
 3539  kubectl --context=staging -n urbi apply -f deployment.yaml 
 3540  ls
 3541  kubectl --context=staging -n urbi get pods | grep maplocator
 3542  kubectl --context=staging -n urbi describe pod maplocator-snapshot-6b87dd9d4f-5kmrs
 3543  kubectl --context=staging -n urbi delete deployment maplocator-snapshot
 3544  kubectl --context=staging -n urbi get pods | grep maplocator
 3545  nano deployment.yaml 
 3546  kubectl --context=staging -n urbi apply -f deployment.yaml 
 3547  kubectl --context=stagi
 3548  kubectl --context=staging -n urbi apply -f deployment.yaml 
 3549  kubectl --context=staging -n urbi get pods | grep maplocator
 3550  kubectl --context=staging -n urbi get cj
 3551  kubectl --context=staging -n urbi get job
 3552  kubectl --context=staging -n urbi create job --from=cronjob/pro-api-asset-importer pro-api-asset-importer-manual-$(date +%s)
 3553  kubectl --context=staging -n urbi get job
 3554  kubectl --context=staging -n urbi get ingress | citylens
 3555  kubectl --context=staging -n urbi get ingress | grep citylens
 3556  cat /opt/on-premise/stg-ingress/
 3557  cd /opt/on-premise/stg-ingress
 3558  ls
 3559  cd ..
 3560  ls
 3561  cd momrah/helmfile/values/citylens/
 3562  ls
 3563  cat oracle-staging.yaml 
 3564  cd ..
 3565  ls
 3566  cd pro-ui
 3567  ls
 3568  cat oracle-staging.yaml 
 3569  clear
 3570  ls
 3571  cd ../pro-api/
 3572  cat oracle-staging.yaml 
 3573  ls
 3574  cd 
 3575  clear
 3576  kubectl --context=staging -n urbi get svc
 3577  clear
 3578  kubectl --context=staging -n urbi get svc | haproxy
 3579  kubectl --context=staging -n urbi get svc | grep  haproxy
 3580  kubectl --context=staging -n urbi get svc haproxy -o yaml
 3581  clear
 3582  cd omar/wedo/
 3583  mkdir dev
 3584  cd dev/
 3585  ls
 3586  mkdir haproxy-ro
 3587  cd haproxy-ro/
 3588  nano service.yaml
 3589  kubectl --context=staging -n urbi apply -f service.yaml 
 3590  kubectl --context=staging -n urbi get svc | grep haproxy
 3591  nano service.yaml
 3592  kubectl --context=staging -n urbi apply -f service.yaml 
 3593  cd ..
 3594  ls
 3595  cd haproxy-ro/
 3596  cp ../../dqp/dqp-ui/chart/ingress.yaml .
 3597  clear
 3598  ls
 3599  cat ingress.yaml 
 3600  history | grep helm
 3601  clear
 3602  cd /opt/on-premise/momrah/helmfile/values/ingress-controller/
 3603  ls
 3604  cat common.gotmpl 
 3605  cat oracle-staging.yaml 
 3606  cat common.gotmpl 
 3607  kubectl --context=staging -n urbi get configmaps
 3608  kubectl --context=staging -n urbi get ingress urbi-ingress-nginx-controller -o yaml
 3609  kubectl --context=staging -n urbi get configmap urbi-ingress-nginx-controller -o yaml
 3610  kubectl --context=staging -n urbi get deployments
 3611  kubectl --context=staging -n urbi get deployment urbi-ingress-ingress-nginx-controller -o yaml
 3612  clear
 3613  cd ..
 3614  ls
 3615  cd ..
 3616  ls
 3617  cd services/
 3618  ls
 3619  nano ingress-controller.yaml 
 3620  ls
 3621  cd ..
 3622  ls
 3623  cd values/
 3624  ls
 3625  cd haproxy/
 3626  ls
 3627  cat _common.gotmpl 
 3628  clear
 3629  ls
 3630  cat oracle-staging.yaml 
 3631  clear
 3632  ls
 3633  cd ..
 3634  ls
 3635  cd ..
 3636  ls
 3637  cat common.yaml 
 3638  clear
 3639  ls
 3640  cd services/
 3641  ls
 3642  cat ingress-controller.yaml 
 3643  cat ../values/ingress-controller/common.gotmpl 
 3644  ls
 3645  cd ..
 3646  clear
 3647  ls
 3648  cd tests/
 3649  ls
 3650  cd ..
 3651  ls
 3652  cd charts/
 3653  ls
 3654  cd tfs-momrah/
 3655  ls
 3656  cd ..
 3657  ls
 3658  cd bitnami-common/
 3659  ls
 3660  cat Chart.yaml 
 3661  cd templates/
 3662  ls
 3663  clear
 3664  cd ../../
 3665  ls
 3666  cd ..
 3667  ls
 3668  clear
 3669  kubectl --context=staging get ingress
 3670  kubectl --context=staging get ingress | grep minio
 3671  ls
 3672  clear
 3673  ls
 3674  cd /opt/on-premise/momrah/helmfile/values/citylens-routes-ui/
 3675  ls
 3676  cat oracle-staging.yaml 
 3677  ls
 3678  cd ..
 3679  ls
 3680  cd citylens
 3681  ls
 3682  cat oracle-staging.yaml 
 3683  cd ..
 3684  kubectl --context=staging get deployments
 3685  kubectl --context=staging get deployment citylens-routes-ui -o yaml
 3686  kubectl --context=staging get deployment citylens-routes-ui -o yaml | grep minio
 3687  kubectl --context=staging get ingres
 3688  kubectl --context=staging get ingress
 3689  kubectl --context=staging get configmap
 3690  kubectl --context=staging get configmap citylens-routes-ui -o yaml | grep minio
 3691  kubectl --context=staging get configmap citylens-routes-ui -o yaml
 3692  clear
 3693  kubectl --context=staging get secret 
 3694  kubectl --context=staging get secret | grep city
 3695  kubectl --context=staging get secret citylens-routes-ui-secret -o yaml
 3696  kubectl --context=staging get secret citylens-routes-ui-secret -o yaml | grep minio
 3697  kubectl --context=staging get secret citylens-routes-api--secret -o yaml | grep minio
 3698  kubectl --context=staging get secret citylens-routes-api-secret -o yaml | grep minio
 3699  kubectl --context=staging get configmap citylens-routes-api -o yaml
 3700  kubectl --context=staging get configmap
 3701  kubectl --context=staging get deployment
 3702  kubectl --context=staging get deployment citylens-routes-api -o yaml
 3703  kubectl --context=staging get deployment citylens-routes-api -o yaml | grep minio
 3704  clear
 3705  ls
 3706  cd 
 3707  cd /opt/on-premise/momrah/helmfile/values/ingress-controller/
 3708  ls
 3709  nano common.gotmpl 
 3710  kubectl --context=staging get svc 
 3711  history | grep ingress
 3712  clear
 3713  ls
 3714  cd ..
 3715  ls
 3716  cd ..
 3717  helmfile -f helmfile/services/ingress-controller.yaml -e oracle-staging apply
 3718  clear
 3719  ls
 3720  cd 
 3721  cd omar/wedo/dev/haproxy-ro/
 3722  ls
 3723  rm ingress.yaml 
 3724  nano ingress.yaml
 3725  kubectl --context=staging -n urbi apply -f ingress.yaml 
 3726  cat ingress.yaml 
 3727  cat /opt/on-premise/momrah/helmfile/values/ingress-controller/common.gotmpl 
 3728  clear
 3729  ls
 3730  cat ingress.yaml 
 3731  ls
 3732  kubectl --context=staging -n urbi get ingress | grep ha
 3733  clear
 3734  history
 3735  clear
 3736  cd 
 3737  cd /opt/on-premise/momrah/
 3738  helmfile -f helmfile/services/ingress-controller.yaml -e oracle-staging apply
 3739  ls
 3740  cd helmfile/services/
 3741  cat ingress-controller.yaml 
 3742  ls
 3743  cd ..
 3744  cat values/ingress-controller/oracle-staging.yaml 
 3745  cat values/ingress-controller/common.gotmpl 
 3746  clear
 3747  ls
 3748  kubectl --context=staging -n urbi get configmap | grep ingress
 3749  kubectl --context=staging -n urbi get configmap urbi-ingress-ingress-nginx-controller -o yaml
 3750  kubectl --context=staging -n urbi get configmap urbi-ingress-nginx-controller -o yaml
 3751  kubectl --context=staging -n urbi get deployments
 3752  kubectl --context=staging -n urbi get pods | grep ingress
 3753  kubectl -urbi ingress-nginx exec urbi-ingress-ingress-nginx-controller-5c785b9ff-qkw2c -- cat /etc/nginx/nginx.conf | grep "5001"
 3754  kubectl --context=staging -n urbi  exec urbi-ingress-ingress-nginx-controller-5c785b9ff-qkw2c -- cat /etc/nginx/nginx.conf | grep "5001"
 3755  kubectl --context=staging -n urbi  exec urbi-ingress-ingress-nginx-controller-5c785b9ff-qkw2c -- cat /etc/nginx/nginx.conf
 3756  clear
 3757  kubectl --context=staging -n urbi  exec urbi-ingress-ingress-nginx-controller-5c785b9ff-qkw2c -- cat /etc/nginx/nginx.conf | grep tcp
 3758  kubectl --context=staging -n urbi  exec urbi-ingress-ingress-nginx-controller-5c785b9ff-qkw2c -- cat /etc/nginx/nginx.conf | grep 5002
 3759  kubectl --context=staging -n urbi  exec urbi-ingress-ingress-nginx-controller-5c785b9ff-qkw2c -- cat /etc/nginx/nginx.conf | grep 5001
 3760  kubectl --context=staging -n urbi get configmap urbi-ingress-ingress-nginx-controller -o yaml
 3761  kubectl --context=staging -n ingress-nginx get configmap tcp-services
 3762  kubectl --context=staging -n ingress-nginx get configmap
 3763  kubectl --context=staging -n ingress-nginx get deployments
 3764  kubectl --context=staging -n ingress-nginx get all
 3765  kubectl --context=staging -n urbi get pods | ingress
 3766  kubectl --context=staging -n urbi get pods | grep ingress
 3767  kubectl --context=staging -n urbi lo
 3768  kubectl --context=staging -n urbi get pods | grep ingress
 3769  clear
 3770  kubectl --context=staging -n urbi logs urbi-ingress-ingress-nginx-controller-5c785b9ff-qkw2c | grep 5001
 3771  kubectl --context=staging -n urbi logs urbi-ingress-ingress-nginx-controller-5c785b9ff-qkw2c | grep haproxy-ro
 3772  kubectl --context=staging -n urbi delete ingress haproxy-ro
 3773  nano /opt/on-premise/momrah/helmfile/values/ingress-controller/common.gotmpl 
 3774  history | grep common.gotmpl
 3775  history | grep ingress-controller.yaml
 3776  clear
 3777  cd /opt/on-premise/momrah/
 3778  helmfile -f helmfile/services/ingress-controller.yaml -e oracle-stagin
 3779  kubectl --context=staging -n urbi get pods | grep controller
 3780  kubectl --context=staging -n urbi exec urbi-ingress-ingress-nginx-controller-xxxx -- \
 3781  clear
 3782  kubectl --context=staging -n urbi get pods | grep controller
 3783  kubectl --context=staging -n urbi exec urbi-ingress-ingress-nginx-controller-5c785b9ff-qkw2c --   cat /etc/nginx/nginx.conf | grep -A 5 "stream"
 3784  kubectl --context=staging -n urbi exec urbi-ingress-ingress-nginx-controller-5c785b9ff-qkw2c --   cat /etc/nginx/nginx.conf | grep haproxy
 3785  kubectl --context=staging -n urbi exec urbi-ingress-ingress-nginx-controller-5c785b9ff-qkw2c --   cat /etc/nginx/nginx.conf | grep 5001
 3786  kubectl --context=staging -n urbi exec urbi-ingress-ingress-nginx-controller-5c785b9ff-qkw2c --   cat /etc/nginx/nginx.conf | grep 5002
 3787  kubectl --context=staging -n urbi exec urbi-ingress-ingress-nginx-controller-5c785b9ff-qkw2c --   cat /etc/nginx/nginx.conf | grep tcp
 3788  k9s
 3789  cat /etc/hosts
 3790  sudo nano /etc/hosts
 3791  k9s
 3792  kubectl --context staging logs pro-api-bd8769fdc-5gh9q
 3793  k9s
 3794  kubectl --context staging logs pro-api-bd8769fdc-5gh9q
 3795   tmux attach -t r9odt
 3796  k9s
 3797  cat /opt/on-premise/momrah/helmfile/values/citylens/oracle-staging.yaml 
 3798  helm --kube-context staging ls -a
 3799  tmux new -s citylens
 3800  kubectl --context=staging -n urbi get nodes
 3801  kubectl --context=staging -n urbi get ingresss
 3802  kubectl --context=staging -n urbi get ingress
 3803  kubectl --context=staging -n urbi get deployments
 3804  kubectl --context=staging -n urbi get pods
 3805  kubectl exec -it balady-discrepancy-7978c9774d-j7rlz -- /bin/sh
 3806  clear
 3807  ls
 3808  clear
 3809  docker ps
 3810  docker images
 3811  tmux attach -t citylens
 3812  clear
 3813  kubectl --context=staging get ns
 3814  history | grep elastic
 3815  ssh -i ~/.ssh/keys/staging.key admin@stg-elasticsearch-1
 3816  clear
 3817  kubectl --context=staging -n urbi get deployment pro-api -o yaml
 3818  kubectl --context=staging -n urbi get svc haproxy
 3819  clear
 3820  kubectl --context=staging -n urbi get svc haproxy
 3821  kubectl --context=staging -n urbi get pvc | grep maplocator
 3822  kubectl --context=staging -n urbi get pvc maplocator-storage-pvc
 3823  kubectl --context=staging -n urbi get pvc maplocator-storage-pvc -o yaml
 3824  kubectl --context=staging -n urbi get deployments
 3825  kubectl --context=production -n urbi get deployments
 3826  clear
 3827  kubectl --context=production -n urbi get deployments
 3828  cd /opt/on-premise/momrah/helmfile/
 3829   history | grep helmfile
 3830  helmfile -e oracle-staging -f services/citylens.yaml --context=1 diff
 3831  helmfile -e oracle-staging -f services/citylens.yaml sync
 3832  helm --kube-context staging rollback citylens
 3833  helm --kube-context staging ls -a
 3834  helmfile -e oracle-staging -f services/citylens.yaml sync
 3835  cat values/citylens/oracle-staging.yaml 
 3836  cat values/navi/router/oracle-staging.yaml 
 3837  nano values/pro-api/oracle-staging.yaml 
 3838  helmfile -e oracle-staging -f services/pro-api.yaml --context=1 diff
 3839  nano common.yaml 
 3840  helmfile -e oracle-staging -f services/pro-api.yaml --context=1 diff
 3841  nano values/oracle-staging-common.yaml 
 3842  helmfile -e oracle-staging -f services/pro-api.yaml --context=1 diff
 3843  helmfile -e oracle-staging -f services/pro-api.yaml --context=1 apply
 3844  nano values/oracle-staging-common.yaml 
 3845  nano values/pro-api/oracle-staging.yaml 
 3846  helmfile -e oracle-staging -f services/pro-api.yaml --context=1 apply
 3847  nano values/oracle-staging-common.yaml 
 3848  nano common.yaml 
 3849  nano values/citylens/oracle-staging.yaml 
 3850  helmfile -e oracle-staging -f services/citylens.yaml --context=1 diff
 3851  helmfile -e oracle-staging -f services/citylens.yaml --context=1 apply
 3852   tmux attach -t r9odt
 3853  k9s
 3854  tmux attach -t citylens
 3855  kubectl --context staging describe cm haproxy
 3856  tmux a -t is
 3857  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-1
 3858  tmux attach -t citylens
 3859  cat ~/.s3cfg/me-jeddah-1-oci 
 3860  k9s
 3861  tmux attach -t citylens
 3862   tmux attach -t r9odt
 3863  kubectl --context staging get cm kafka-ui-fromvalues -o yaml
 3864  k9s
 3865  tmux attach -t citylens
 3866  clear
 3867  cd omar/wedo/
 3868  cd maplocator-snapshot/
 3869  ls
 3870  mkdir generate-map-service
 3871  mkdir chart
 3872  cd chart/
 3873  ls
 3874  nano deployment.yaml
 3875  cat ../../maplocator-snapshot/upload-service/chart/deployment.yaml 
 3876  clear
 3877  ls
 3878  kubectl --context=staging -n urbi get configmap
 3879  kubectl --context=staging -n urbi get configmap haproxy -o yaml
 3880  clear
 3881  kubectl --context=staging get ingress -n urbi 
 3882  kubectl --context=staging get ingress -n urbi | grep keys
 3883  cd omar/wedo/maplocator-snapshot/
 3884  clear
 3885  ls
 3886  cd chart/
 3887  ls
 3888  cat ../upload-service/chart/deployment.yaml 
 3889  cat ../upload-service/chart/pvc.yaml 
 3890  cat ../upload-service/chart/ingress.yaml 
 3891  cat ../upload-service/chart/service.yaml 
 3892  clear
 3893  nano deployment.yaml
 3894  nano service.yaml
 3895  nano ingress.yaml
 3896  nano pvc.yaml
 3897  clear
 3898  cat /opt/on-premise/momrah/helmfile/services/redis.yaml
 3899  cat /opt/on-premise/momrah/helmfile/values/redis/common.yaml 
 3900  cat /opt/on-premise/momrah/helmfile/values/redis/oracle-staging.yaml 
 3901  clear
 3902  ls
 3903  kubectl --context=staging -n urbi get ingress | grep citylens-routes
 3904  kubectl --context=staging -n urbi get ingress citylens-routes-api -o yaml
 3905  clea
 3906  kubectl --context=staging -n urbi get pods | grep citylens-routes-api
 3907  kubectl --context=staging -n urbi logs citylens-routes-api-78cf779d77-g5r5k | grep o.qahtani@urbi.ae
 3908  kubectl --context=staging -n urbi logs -f citylens-routes-api-78cf779d77-g5r5k
 3909  clear
 3910  kubectl --context=staging -n urbi exec -it citylens-routes-api-78cf779d77-g5r5k --bash
 3911  kubectl --context=staging -n urbi exec -it citylens-routes-api-78cf779d77-g5r5k --bash
 3912  kubectl --context=staging -n urbi exec -it citylens-routes-api-78cf779d77-g5r5k -- bash
 3913  clear
 3914  ls
 3915  kubectl --context=staging -n urbi get ingress
 3916  kubectl --context=staging -n urbi get ingress be-navi-api-oci -o yaml
 3917  kubectl --context=staging -n urbi get secret
 3918  kubectl --context=staging -n urbi get secret pro-ui-secret -o yaml
 3919  kubectl --context=staging -n urbi get secret pro-api-secret -o yaml
 3920  kubectl --context=staging -n urbi get secret pro-api-secret -o jsonpath='{.data.routingApi2gisKey}' | base64 --decode
 3921  clear
 3922  cd omar/wedo/dqp/dqp-closures-service/chart/
 3923  ls
 3924  nano ingress.yaml 
 3925  ls
 3926  nano deployment.yaml 
 3927  nano ingress.yaml
 3928  nano deployment.yaml 
 3929  kubectl --context=staging -n urbi apply -f deployment.yaml 
 3930  kubectl --context=staging -n urbi apply -f ingress.yaml 
 3931  k9s
 3932  kubectl --context staging logs citylens-map-matcher-c7fb66975-f9nmr
 3933  k9s
 3934  cat /opt/on-premise/momrah/helmfile/values/citylens/oracle-staging.yaml 
 3935  kubectl --context staging get cm
 3936  kubectl --context staging get cm citylens-web-configmap
 3937  kubectl --context staging get cm citylens-web-configmap -o yaml
 3938  kubectl --context staging describe cm citylens-web-configmap
 3939  k9s
 3940  cd /opt/on-premise/momrah/helmfile/
 3941  helmfile -e oracle-staging -f services/navi/front.yaml --context=1 diff
 3942  helm --kube-context staging ls -a
 3943  nano common.yaml 
 3944  helmfile -e oracle-staging -f services/navi/front.yaml --context=1 diff
 3945  helmfile -e oracle-staging -f services/navi/front.yaml --context=1 apply
 3946  nano common.yaml 
 3947  helmfile -e oracle-staging -f services/navi/front.yaml --context=1 apply
 3948  nano tests/navi-custom-all.sh 
 3949  helmfile -e oracle-staging -f services/navi/front.yaml --context=1 apply
 3950  nano tests/navi/riyadh_map_matching.json
 3951  helmfile -e oracle-staging -f services/navi/front.yaml --context=1 apply
 3952  helmfile -e oracle-staging -f services/navi/router.yaml --context=1 diff
 3953  helmfile -e oracle-staging -f services/navi/router.yaml sync
 3954  k9s
 3955  helmfile -e oracle-staging -f services/navi/back-custom.yaml --context=1 diff
 3956  helmfile -e oracle-staging -f services/navi/back-custom.yaml sync
 3957  nano tests/navi-custom-all.sh 
 3958  helmfile -e oracle-staging -f services/navi/back-custom.yaml sync
 3959  nano tests/navi-custom-all.sh 
 3960  helmfile -e oracle-staging -f services/navi/back-custom.yaml -l service=map-matching sync
 3961  nano /etc/hosts
 3962  sudo nano /etc/hosts
 3963  helmfile -e oracle-staging -f services/navi/back-custom.yaml -l service=map-matching sync
 3964  helmfile -e oracle-staging -f services/navi/router.yaml sync
 3965  helmfile -e oracle-staging -f services/navi/front.yaml sync
 3966  k9s
 3967  nano tests/navi/riyadh_map_matching.json 
 3968  helmfile -e oracle-staging -f services/navi/back-custom.yaml -l service=map-matching sync
 3969  helmfile -e oracle-staging -f services/navi/front.yaml sync
 3970  k9s
 3971  nano tests/navi/riyadh_map_matching.json 
 3972  helmfile -e oracle-staging -f services/navi/front.yaml sync
 3973  k9s
 3974  kubectl --context staging logs navi-front | grep 404
 3975  kubectl --context staging logs navi-front-c87899c9b-qkdp6 | grep 404
 3976  kubectl --context staging logs navi-front-c87899c9b-qkdp6 | grep error
 3977  kubectl --context staging logs navi-front-c87899c9b-qkdp6 
 3978  cd /opt/on-premise/momrah/helmfile/
 3979  nano tests/navi/riyadh_map_matching.json 
 3980  cat tests/navi-custom-all.sh 
 3981  kubectl --context=staging -n urbi get deployments
 3982  kubectl --context=staging -n urbi get pods
 3983  k9s
 3984  tmux attach -t citylens
 3985  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-1
 3986  kubectl --context staging describe cm haproxy -o yaml
 3987  kubectl --context staging describe cm haproxy
 3988  ll
 3989  cd ..
 3990  ll
 3991  helmfile -e oracle-staging -f services/citylens.yaml --context=1 diff
 3992  helm --kube-context staging ls -a
 3993  helm --kube-context staging delete citylens
 3994  helmfile -e oracle-staging -f services/citylens.yaml --context=1 apply
 3995  s3cmd ls
 3996  s3cmd -c ~/.s3cfg/me-jeddah-1-oci ls
 3997  s3cmd -c ~/.s3cfg/me-jeddah-1-oci ls s3://staging-pro-userassets
 3998  s3cmd -c ~/.s3cfg/me-jeddah-1-oci ls s3://staging-pro-assets
 3999  cat ~/.s3cfg/me-jeddah-1-oci 
 4000  nano values/citylens/oracle-staging.yaml 
 4001  helmfile -e oracle-staging -f services/citylens.yaml --context=1 apply
 4002  k9s
 4003  k9s
 4004  tmux attach -t citylens
 4005  k9s
 4006  clear
 4007  ls
 4008  cd /opt/on-premise/stg-ingress/
 4009  ls
 4010  cat be-catalog-api.yaml 
 4011  ls
 4012  nano be-balady-geostreaming.yaml
 4013  kubectl --context=staging -n urbi apply -f be-balady-geostreaming.yaml 
 4014  cat be-balady-geostreaming.yaml 
 4015  kubectl --context=staging -n urbi apply get pods | grep geo
 4016  kubectl --context=staging -n urbi get pods | grep geo
 4017  kubectl --context=staging -n urbi get svc | grep geo
 4018  ls
 4019  rm be-balady-geostreaming.yaml 
 4020  nano be-balady-geostreaming.yaml
 4021  kubectl --context=staging -n urbi apply -f be-balady-geostreaming.yaml 
 4022  ls
 4023  cat ../momrah/helmfile/services/ingress-controller.yaml 
 4024  ls
 4025  tmux a -t is
 4026  clear
 4027  ls
 4028  cat /opt/on-premise/stg-ingress/be-balady-public.yaml 
 4029  cat /opt/on-premise/stg-ingress/be-balady-discrepancy.yaml 
 4030   tmux attach -t r9odt
 4031  ll
 4032  cat dgctl.sh 
 4033  sudo ./dgctl.sh staging
 4034  nano ~/.bashrc
 4035  cd ..
 4036  ls
 4037  cd ..
 4038  ls
 4039  cat ansible/inventory/oracle-production/oracle.yml 
 4040  ssh prod-elasticsearch-1 -l admin -i ~/.ssh/keys/production.key
 4041  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-staging playbooks/patroni-configure.yml -CD
 4042  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-staging playbooks/patroni-configure.yml 
 4043  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-staging playbooks/patroni.yml --list-tags
 4044  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-staging playbooks/patroni.yml -t patroni
 4045  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-2
 4046  ansible-playbook -i /opt/on-premise/momrah/ansible/inventory/oracle-staging playbooks/patroni.yml -t patroni
 4047  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-2
 4048  touch t.txt
 4049  reset
 4050  cd
 4051  cd /opt
 4052  mc
 4053  cd ..
 4054  ls
 4055  cd ..
 4056  ls
 4057  cat ansible/inventory/oracle-production/oracle.yml 
 4058  ssh prod-elasticsearch-2 -l admin -i ~/.ssh/keys/production.key
 4059  tmux a -t is
 4060  k9s
 4061  tmux a -t is
 4062  k9s
 4063  cd /opt/on-premise/momrah/helmfile/
 4064  nano values/platform/oracle-staging.yaml 
 4065  helmfile -e oracle-staging -f services/platform.yaml --context=1 diff
 4066  helmfile -e oracle-staging -f services/platform.yaml --context=1 apply
 4067   tmux attach -t r9odt
 4068  kubectl --context=staging -n urbi get pods | grep catalog-api
 4069  kubectl --context=staging -n urbi get pods catalog-api-6d4bfc97f8-httjf | grep 70030076609571646
 4070  kubectl --context=staging -n urbi get pods catalog-api-6d4bfc97f8-q8d46 | grep 70030076609571646
 4071  kubectl --context=staging -n urbi get pods catalog-api-6d4bfc97f8-q8d46 | gre
 4072  kubectl --context=staging -n urbi logs catalog-api-6d4bfc97f8-q8d46 | grep 70030076609571646
 4073  tmux a -t is
 4074  ssh prod-postgresql-2 -l admin -i ~/.ssh/keys/production.key
 4075  tmux a -t is
 4076  k9s
 4077  exit
 4078  clear
 4079  cd omar/wedo/dqp/dqp-ui/chart/
 4080  ls
 4081  nano deployment.yaml 
 4082  kubectl cluster-info
 4083  ls
 4084  kubectl get pods
 4085  kubectl --context=staging -n urbi get deployments
 4086  kubectl get pods --context=staging
 4087  touch oracle-{staging,production}.yaml
 4088  cd ../pgadmin/
 4089  touch oracle-{staging,production}.yaml
 4090  helmfile helmfile/services/pgadmin.yaml -e oracle-staging template
 4091  helmfile -f helmfile/services/pgadmin.yaml -e oracle-staging template
 4092  helmfile -f helmfile/services/pgadmin.yaml -e oracle-staging template cd "`printf "%b" '\0057opt\0057on\0055premise\0057momrah\0057helmfile\0057values\0057kafka\0055ui'`"
 4093  helmfile -f helmfile/services/pgadmin.yaml -e oracle-staging template
 4094  helmfile -f helmfile/services/pgadmin.yaml -e oracle-staging apply
 4095  helmfile -f helmfile/services/pgadmin.yaml -e oracle-production apply
 4096  helmfile -f helmfile/services/kafka-ui.yaml -e oracle-production apply
 4097  helmfile -f helmfile/services/kafka-ui.yaml -e oracle-staging apply
 4098  helmfile -f helmfile/services/kafka-ui.yaml -e oracle-production  apply
 4099  helmfile -f helmfile/services/kafka-ui.yaml -e oracle-staging apply
 4100  helmfile -f helmfile/services/pgadmin.yaml -e oracle-production apply
 4101  helmfile -f helmfile/services/pgadmin.yaml -e oracle-staging apply
 4102  helmfile -e oracle-staging -f helmfile/services/monitoring.yaml apply
 4103  helmfile -e oracle-production -f helmfile/services/monitoring.yaml apply
 4104  helmfile -f helmfile/services/kafka-ui.yaml -e oracle-staging apply
 4105  helmfile -f helmfile/services/kafka-ui.yaml -e oracle-production apply
 4106  helmfile -f helmfile/services/mapgl.yaml -e oracle-staging apply
 4107  touch oracle-production.yaml
 4108  helmfile -f helmfile/services/balady.yaml -e oracle-staging diff --context 2
 4109  helmfile -f helmfile/services/balady.yaml -e oracle-staging apply
 4110  helmfile -f helmfile/services/balady.yaml -e oracle-staging production
 4111  helmfile -f helmfile/services/balady.yaml -e oracle-production apply
 4112  helmfile -f helmfile/services/balady-discrepancy.yaml -e oracle-production apply
 4113  helmfile -f helmfile/services/balady-geostreaming.yaml -e oracle-production apply
 4114  helmfile -f helmfile/services/balady-geostreaming-web.yaml -e oracle-production apply
 4115  kubectl --context=staging -n urbi apply -f .
 4116  pwd
 4117  kubectl --context=staging -n urbi apply -f .
 4118  touch oracle-staging.yaml
 4119  >oracle-staging.yaml
 4120  cd /opt
 4121  scd on-premise/stg-ingress/
 4122  mc
 4123  kubectl --context=staging -n urbi get ingress 
 4124  kubectl --context=staging -n urbi logs urbi-ingress-ingress-nginx-controller-5c785b9ff-qkw2c 
 4125  kubectl --context=staging -n urbi logs urbi-ingress-ingress-nginx-controller-5c785b9ff-qkw2c  -f
 4126  kubectl --context staging get pods | grep routin
 4127  kubectl --context staging get pods | grep cityle
 4128  kubectl --context staging get pods 
 4129  kubectl --context staging get pods  | grep citylens
 4130  kubectl --context staging get pods  logs citylens-map-matcher-5dd59bcd47-zgmvc
 4131  kubectl --context staging logs citylens-map-matcher-5dd59bcd47-zgmvc
 4132  kubectl --context staging get pods  logs citylens-map-matcher-5dd59bcd47-zgmvc
 4133  kubectl --context staging get pods  | grep citylens
 4134  kubectl --context staging logs citylens-routes-worker-f9dc5c49b-zlh6d
 4135  kubectl --context staging get pods  | grep citylens
 4136  kubectl --context staging logs citylens-routes-worker-55698cf84d-74c79
 4137  kubectl --context staging get pods  | grep citylens
 4138  kubectl --context staging get ingress
 4139  host urbi-lifestyle-stg.momrah.gov.sa
 4140  nslookup urbi-lifestyle-stg.momrah.gov.sa 1.1.1.1.
 4141  nslookup urbi-lifestyle-stg.momrah.gov.sa 1.1.1.1
 4142  nslookup map-urbi.momrah.gov.sa 1.1.1.1
 4143  kubectl --context staging get pods
 4144  kubectl --context staging describe pod urbi-gf-grafana-54cff5d97d-v5xqs
 4145  kubectl --context staging get pod
 4146  kubectl --context staging delete pod urbi-gf-grafana-54cff5d97d-v5xq
 4147  kubectl --context staging get pods
 4148  kubectl --context staging delete pod urbi-gf-grafana-54cff5d97d-dl5d5 
 4149  kubectl --context staging describe pod urbi-gf-grafana-54cff5d97d-jxz8p 
 4150  kubectl --context staging logs urbi-gf-grafana-54cff5d97d-jxz8p 
 4151  kubectl --context staging logs urbi-gf-grafana-5bbd7d74c5-mshbm -f
 4152  kubectl --context staging get pods
 4153  kubectl --context staging logs snapshot-pro-api-5d5c87989-np7xh
 4154  kubectl --context production get pods
 4155  kubectl --context production get pods | grep pgadmin
 4156  kubectl --context production describe pod pgadmin-pgadmin4-55d95bf5c-v7kvc
 4157  kubectl --context production get pods | grep pgadmin
 4158  kubectl --context production describe pod pgadmin-pgadmin4-55d95bf5c-v7kvc
 4159  kubectl --context production get pods | grep pgadmin
 4160  kubectl --context production delete pod pgadmin-pgadmin4-55d95bf5c-v7kvc 
 4161  kubectl --context production get pods | grep pgadmin
 4162  kubectl --context production describe pod pgadmin-pgadmin4-55d95bf5c-plgr2 pro-
 4163  kubectl --context production describe pod pgadmin-pgadmin4-55d95bf5c-plgr2 
 4164  kubectl --context production logs  pgadmin-pgadmin4-55d95bf5c-plgr2 
 4165  kubectl --context production get pods | grep pgadmin
 4166  kubectl --context production get pods 
 4167  kubectl --context production get pods | grep pgadmin
 4168  kubectl --context production get pods | grep kafka
 4169  kubectl --context staging get pods | grep kafka
 4170  kubectl --context staging get ingress
 4171  kubectl --context production get ingress
 4172  kubectl get secret --context staging --namespace urbi urbi-gf-grafana -o jsonpath="{.data.admin-password}" | base64 --decode
 4173  kubectl --context production get ingress
 4174  kubectl --context staging get ingress
 4175  kubectl --context staging get pods | grep kafka
 4176  kubectl --context production get pods | grep balady
 4177  kubectl --context production get ingress
 4178  kubectl --context production get pods | grep catalog
 4179  kubectl --context staging get pods | grep catalog
 4180  kubectl --context staging logs catalog-api-6d4bfc97f8-httjf | grep 70030076609571646
 4181  kubectl --context staging logs catalog-api-6d4bfc97f8-httjf | grep geocoding
 4182  kubectl --context staging logs catalog-api-6d4bfc97f8-q8d46 | grep geocoding
 4183  kubectl --context staging logs catalog-api-6d4bfc97f8-httjf | grep geocoding
 4184  kubectl --context staging logs catalog-api-6d4bfc97f8-httjf | grep geocode
 4185  kubectl --context staging logs catalog-api-6d4bfc97f8-q8d46 | grep geocode
 4186  kubectl --context=staging -n urbi get ingerss
 4187  kubectl --context=staging -n urbi get ingress
 4188  kubectl --context=staging -n urbi get ingress -o yaml be-catalog-api
 4189  kubectl --context staging logs catalog-api-6d4bfc97f8-q8d46 | grep geocode
 4190  kubectl --context=staging -n urbi get ingress -o yaml be-catalog-api
 4191  curl https://charts.bitnami.com/bitnami
 4192  kubectl --context production get ingress
 4193  curl https://10.246.8.88
 4194  curl -k https://10.246.8.88
 4195  curl -v -k https://10.246.8.88
 4196  cd /home
 4197  ls
 4198  ls -la
 4199  cd
 4200  kubectl --context production logs -f urbi-ingress-ingress-nginx-controller-64f9b895f4-lm7xk 
 4201  kubectl --context production get ingress
 4202  kubectl --context staging get ingress
 4203  kubectl --context staging logs urbi-ingress-ingress-nginx-controller-5c785b9ff-qkw2c | grep geolocation
 4204   host geostream-urbi-lifestyle-stg.momrah.gov.sa
 4205  nslookup geostream-urbi-lifestyle-stg.momrah.gov.sa 8.8.8.8
 4206  nslookup urbi-lifestyle-stg.momrah.gov.sa 8.8.8.8
 4207  kubectl --context staging get ingress -o yaml be-balady-geostreaming-ws
 4208  kubectl --context staging logs -f urbi-ingress-ingress-nginx-controller-5c785b9ff-qkw2c | grep geolocation
 4209  tmux a -t is
 4210   tmux attach -t r9odt
 4211  ls -lh /opt/on-premise/momrah/helmfile/
 4212  ls -lh /opt/on-premise/momrah/helmfile/values/snapshot-pro-api/
 4213  less /opt/on-premise/momrah/helmfile/values/snapshot-pro-api/oracle-staging.yaml 
 4214  docker images
 4215  less /opt/on-premise/admin-sa-kubeconfig.yml
 4216  kubectl --context=staging -n urbi get pods
 4217  less ~/.kube/config 
 4218  less /opt/on-premise/admin-sa-kubeconfig.yml
 4219  less ~/.s3cfg/me-jeddah-1-oci 
 4220  less /opt/on-premise/momrah/helmfile/values/snapshot-pro-api/oracle-staging.yaml 
 4221  less /opt/on-premise/momrah/helmfile/values/snapshot-pro-ui/oracle-staging.yaml 
 4222  clear
 4223  ls
 4224  clear
 4225  ls
 4226  cd /opt/on-premise/stg-ingress/
 4227  ls
 4228  nano be-balady-geostreaming.yaml 
 4229  kubectl --context=staging -n urbi apply -f be-balady-geostreaming.yaml 
 4230  cat be-balady-geostreaming.yaml 
 4231  clear
 4232  kubectl --context=production -n urbi get deployments
 4233  nslookup map-urbi-prod.momrah.gov.sa
 4234  nslookup map-urbi.momrah.gov.sa
 4235  nslookup map-urbi-prod.momrah.gov.sa
 4236  wget map-urbi-prod.momrah.gov.sa
 4237  curl map-urbi-prod.momrah.gov.sa
 4238  curl https://map-urbi-prod.momrah.gov.sa
 4239  curl https://map-urbi-prod.momrah.gov.sa/api.js
 4240  clear
 4241  ls
 4242  cd ..
 4243  clear
 4244  ls
 4245  mkdir prod-ingress
 4246  cd stg-ingress/
 4247  rm index.html 
 4248  clear
 4249  ls
 4250  cat be-mapgl-js-api.yaml 
 4251  clear
 4252  ls
 4253  cd ..
 4254  ls
 4255  mkdir oci-prod-ingress
 4256  cp oci-stg-ingress/be-mapgl-js-api.yaml oci-prod-ingress/
 4257  cd oci-prod-ingress/
 4258  ls
 4259  cat be-mapgl-js-api.yaml 
 4260  clear
 4261  nano be-mapgl-js-api.yaml 
 4262  kubectl --context=production -n urbi apply -f be-mapgl-js-api.yaml 
 4263  curl -v https://urbi-lifestyle-oci.momrah.gov.sa
 4264  clear
 4265  ls
 4266  cat /opt/on-premise/momrah/helmfile/services/ingress-controller.yaml 
 4267  cat /opt/on-premise/momrah/helmfile/values/ingress-controller/oracle-production.yaml 
 4268  cat /opt/on-premise/momrah/helmfile/values/ingress-controller/common.gotmpl 
 4269   tmux attach -t r9odt
 4270  ssh prod-postgresql-1 -l admin -i ~/.ssh/keys/production.key
 4271  ssh -i ~/.ssh/keys/staging.key admin@stg-postgresql-2
 4272  ssh prod-postgresql-2 -l admin -i ~/.ssh/keys/production.key
 4273  cd ..
 4274  cd dgctl/manual/
 4275  ls
 4276  sudo ./dgctl.sh staging
 4277  telnet 192.168.240.13 22
 4278  telnet 192.168.240.13 443
 4279  telnet 10.80.64.20 22
 4280  telnet 10.80.64.20 5432
 4281  ssh prod-elasticsearch-2 -l admin -i ~/.ssh/keys/production.key
 4282  ssh prod-elasticsearch-1 -l admin -i ~/.ssh/keys/production.key
 4283  kubectl --context=production -n urbi get pods
 4284  kubectl --context=production -n urbi logs twins-api-66db49c79b-grxlg
 4285  kubectl --context=production -n urbi logs twins-api-66db49c79b-vj8df
 4286  kubectl --context=production -n urbi get ingress
 4287  kubectl --context=production -n urbi edit ingress citylens-api
 4288  kubectl --context=production -n urbi edit ingress citylens-web
 4289  mc
 4290  kubectl --context=production -n urbi edit ingress navi-back-ctx
 4291  clear
 4292  kubectl --context=production -n urbi get secret
 4293  kubectl --context=production -n urbi get secret | grep pro
 4294  kubectl --context=production -n urbi get secret pro-api-secret -o yaml
 4295  kubectl --context=production -n urbi get secret pro-api-secret -o jsonpath='{.data.apiKey}' | base64 --decode
 4296  clear
 4297  c
 4298  cd /opt/on-premise/oci-prod-ingress/
 4299  clear
 4300  ls
 4301  cat ../oci-stg-ingress/
 4302  ls ../oci-stg-ingress/
 4303  cp ../oci-stg-ingress/be-balady-discrepancy.yaml .
 4304  cp ../oci-stg-ingress/be-balady-public.yaml .
 4305  cp ../oci-stg-ingress/be-catalog-api.yaml .
 4306  cp ../oci-stg-ingress/be-navi-api.yaml .
 4307  cp ../oci-stg-ingress/be-tiles-api.yaml .
 4308  ls
 4309  nano be-balady-discrepancy.yaml 
 4310  nano be-balady-public.yaml 
 4311  nano be-catalog-api.yaml 
 4312  nano be-mapgl-js-api.yaml 
 4313  nano be-navi-api.yaml 
 4314  nano be-tiles-api.yaml 
 4315  kubectl --context=production -n urbi apply -f be-balady-discrepancy.yaml 
 4316  kubectl --context=production -n urbi apply -f be-balady-public.yaml 
 4317  kubectl --context=production -n urbi apply -f be-catalog-api.yaml 
 4318  kubectl --context=production -n urbi apply -f be-mapgl-js-api.yaml 
 4319  kubectl --context=production -n urbi apply -f be-navi-api.yaml 
 4320  kubectl --context=production -n urbi apply -f be-tiles-api.yaml 
 4321  clear
 4322  ls
 4323  history
 4324  history | grep helm
 4325  history > omar/wedo/history.txt
